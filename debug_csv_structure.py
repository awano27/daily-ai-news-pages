#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSVã®æ§‹é€ ã‚’è©³ç´°åˆ†æã—ã¦XæŠ•ç¨¿ã®URLæŠ½å‡ºæ–¹æ³•ã‚’ç‰¹å®š
"""
import requests
import csv
import io
import re

def analyze_csv_structure():
    """CSVã®æ§‹é€ ã‚’è©³ç´°åˆ†æ"""
    csv_url = 'https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0'
    
    print("ğŸ” CSVæ§‹é€ ã®è©³ç´°åˆ†æ")
    print("=" * 50)
    
    try:
        response = requests.get(csv_url, timeout=30)
        if response.status_code != 200:
            print(f"âŒ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
            return
        
        content = response.text
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(content)} æ–‡å­—")
        
        # CSVã‚’ãƒ‘ãƒ¼ã‚¹
        reader = csv.reader(io.StringIO(content))
        rows = list(reader)
        
        print(f"ç·è¡Œæ•°: {len(rows)}")
        
        if rows:
            # ãƒ˜ãƒƒãƒ€ãƒ¼åˆ†æ
            headers = rows[0]
            print(f"\nãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ{len(headers)}åˆ—ï¼‰:")
            for i, header in enumerate(headers):
                print(f"  [{i}]: '{header}'")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿è¡Œã‚’åˆ†æ
            print(f"\næœ€åˆã®3ã¤ã®ãƒ‡ãƒ¼ã‚¿è¡Œ:")
            for row_num, row in enumerate(rows[1:4], 1):
                print(f"\n--- è¡Œ {row_num} ({len(row)}åˆ—) ---")
                for i, cell in enumerate(row):
                    if i < len(headers):
                        header = headers[i]
                    else:
                        header = f"Column_{i}"
                    
                    # URLã‚’æ¤œå‡º
                    if re.search(r'https?://[^\s]+', cell):
                        print(f"  [{i}] {header}: '{cell}' â† URLç™ºè¦‹!")
                    elif len(cell) > 50:
                        print(f"  [{i}] {header}: '{cell[:50]}...'")
                    else:
                        print(f"  [{i}] {header}: '{cell}'")
            
            # URL ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ†æ
            print(f"\nğŸ“Š URL ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ:")
            url_patterns = {}
            
            for row_num, row in enumerate(rows[1:10], 1):  # æœ€åˆã®10è¡Œã‚’ãƒã‚§ãƒƒã‚¯
                for col_num, cell in enumerate(row):
                    urls = re.findall(r'https?://[^\s,;"\']+', cell)
                    for url in urls:
                        if 'x.com' in url or 'twitter.com' in url:
                            col_name = headers[col_num] if col_num < len(headers) else f"Column_{col_num}"
                            if col_name not in url_patterns:
                                url_patterns[col_name] = []
                            url_patterns[col_name].append({
                                'row': row_num,
                                'url': url,
                                'cell_content': cell[:100] + '...' if len(cell) > 100 else cell
                            })
            
            print("X/Twitter URL ãŒå«ã¾ã‚Œã‚‹åˆ—:")
            for col_name, urls in url_patterns.items():
                print(f"\nğŸ“ {col_name}:")
                for info in urls[:3]:  # æœ€åˆã®3ã¤ã‚’è¡¨ç¤º
                    print(f"    è¡Œ{info['row']}: {info['url']}")
                    print(f"         å†…å®¹: {info['cell_content']}")
                    
        print("\nâœ… CSVæ§‹é€ åˆ†æå®Œäº†")
        
        # æ¨å¥¨ã•ã‚Œã‚‹ä¿®æ­£ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
        print("\nğŸ”§ æ¨å¥¨ã•ã‚Œã‚‹ä¿®æ­£:")
        if url_patterns:
            main_col = max(url_patterns.keys(), key=lambda k: len(url_patterns[k]))
            main_col_index = headers.index(main_col) if main_col in headers else -1
            print(f"   ãƒ¡ã‚¤ãƒ³ã®URLåˆ—: '{main_col}' (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {main_col_index})")
            print(f"   ä¿®æ­£ã‚³ãƒ¼ãƒ‰: tweet_url = row[{main_col_index}].strip() if len(row) > {main_col_index} else \"\"")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    analyze_csv_structure()