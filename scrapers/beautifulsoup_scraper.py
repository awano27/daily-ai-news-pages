#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BeautifulSoup ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
é«˜é€Ÿã§è»½é‡ãªHTMLè§£æã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
"""

import requests
from bs4 import BeautifulSoup
import time
from typing import Dict, Any, List
import re

class BeautifulSoupScraper:
    """BeautifulSoupãƒ™ãƒ¼ã‚¹ã®ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, timeout: int = 10, delay: float = 1.0):
        """
        åˆæœŸåŒ–
        
        Args:
            timeout: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (ç§’)
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš” (ç§’)
        """
        self.timeout = timeout
        self.delay = delay
        self.session = requests.Session()
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
    
    def scrape(self, url: str) -> Dict[str, Any]:
        """
        URLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ãƒ»è§£æ
        
        Args:
            url: å¯¾è±¡URL
            
        Returns:
            è§£æçµæœè¾æ›¸
        """
        print(f"ğŸ” ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
        
        try:
            # HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            # HTML è§£æ
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
            result = {
                'success': True,
                'url': url,
                'status_code': response.status_code,
                'title': self._extract_title(soup),
                'content': self._extract_content(soup),
                'links': self._extract_links(soup, url),
                'images': self._extract_images(soup, url),
                'meta': self._extract_meta(soup),
                'raw_html': str(soup)[:2000] + '...' if len(str(soup)) > 2000 else str(soup)
            }
            
            print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(result['content'])}æ–‡å­—")
            
            # é…å»¶
            time.sleep(self.delay)
            
            return result
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'url': url,
                'error': str(e),
                'error_type': 'request_error'
            }
        
        except Exception as e:
            print(f"âŒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'url': url,
                'error': str(e),
                'error_type': 'parsing_error'
            }
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
        # <title>ã‚¿ã‚°
        if soup.title:
            return soup.title.get_text(strip=True)
        
        # h1ã‚¿ã‚°
        h1 = soup.find('h1')
        if h1:
            return h1.get_text(strip=True)
        
        return "ã‚¿ã‚¤ãƒˆãƒ«æœªå–å¾—"
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º"""
        # ä¸è¦è¦ç´ ã‚’å‰Šé™¤
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å€™è£œè¦ç´ 
        content_selectors = [
            'article',
            'main',
            '.content',
            '.post-content',
            '.entry-content',
            '.article-content',
            '#content',
            '#main'
        ]
        
        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼é †ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œç´¢
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                text = element.get_text(separator=' ', strip=True)
                if len(text) > 100:  # ååˆ†ãªé•·ã•ãŒã‚ã‚‹å ´åˆ
                    return text
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: bodyå…¨ä½“
        body = soup.find('body')
        if body:
            text = body.get_text(separator=' ', strip=True)
            # çŸ­ã™ãã‚‹å ´åˆã¯æœ€åˆã®1000æ–‡å­—
            return text[:1000] if len(text) > 1000 else text
        
        return soup.get_text(separator=' ', strip=True)
    
    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """ãƒªãƒ³ã‚¯æŠ½å‡º"""
        links = []
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            text = a_tag.get_text(strip=True)
            
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            if href.startswith('/'):
                href = base_url.rstrip('/') + href
            elif not href.startswith(('http://', 'https://')):
                continue
            
            if text and len(text) > 0:
                links.append({
                    'url': href,
                    'text': text
                })
        
        return links[:20]  # æœ€å¤§20ãƒªãƒ³ã‚¯
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """ç”»åƒURLæŠ½å‡º"""
        images = []
        
        for img_tag in soup.find_all('img', src=True):
            src = img_tag['src']
            
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            if src.startswith('/'):
                src = base_url.rstrip('/') + src
            elif not src.startswith(('http://', 'https://')):
                continue
            
            images.append(src)
        
        return images[:10]  # æœ€å¤§10ç”»åƒ
    
    def _extract_meta(self, soup: BeautifulSoup) -> Dict[str, str]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        meta = {}
        
        # description
        desc_tag = soup.find('meta', attrs={'name': 'description'})
        if desc_tag:
            meta['description'] = desc_tag.get('content', '')
        
        # keywords
        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_tag:
            meta['keywords'] = keywords_tag.get('content', '')
        
        # Open Graph
        og_tags = soup.find_all('meta', attrs={'property': re.compile(r'^og:')})
        for tag in og_tags:
            property_name = tag.get('property', '').replace('og:', '')
            content = tag.get('content', '')
            if property_name and content:
                meta[f'og_{property_name}'] = content
        
        return meta
    
    def scrape_multiple(self, urls: List[str]) -> List[Dict[str, Any]]:
        """è¤‡æ•°URLä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        results = []
        
        print(f"ğŸ” ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {len(urls)}ä»¶")
        
        for i, url in enumerate(urls, 1):
            print(f"é€²è¡ŒçŠ¶æ³: {i}/{len(urls)}")
            result = self.scrape(url)
            results.append(result)
        
        print(f"âœ… ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†")
        return results
    
    def close(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†"""
        self.session.close()

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    scraper = BeautifulSoupScraper()
    
    # ãƒ†ã‚¹ãƒˆURL
    test_url = "https://httpbin.org/get"
    result = scraper.scrape(test_url)
    
    print("çµæœ:", result)
    scraper.close()