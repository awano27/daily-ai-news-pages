#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced News Collector - Gemini URL contextã‚’ä½¿ã£ãŸæ”¹è‰¯ç‰ˆãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚·ã‚¹ãƒ†ãƒ 
"""
import os
import json
import logging
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional
import feedparser
import requests
from pathlib import Path

# Gemini URL context client
try:
    from gemini_url_context import GeminiURLContextClient, get_client
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸ Gemini URL contextæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    import build
    BUILD_AVAILABLE = True
except ImportError:
    BUILD_AVAILABLE = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedNewsCollector:
    """Gemini URL contextã‚’æ´»ç”¨ã—ãŸå¼·åŒ–ç‰ˆãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.gemini_client = get_client() if GEMINI_AVAILABLE else None
        self.cache_dir = Path("_cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.ai_keywords = [
            "AI", "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "neural network", 
            "transformer", "LLM", "GPT", "Claude", "Gemini", "ChatGPT",
            "ç”ŸæˆAI", "generative", "diffusion", "stable diffusion",
            "computer vision", "natural language", "reinforcement learning",
            "MLOps", "AutoML", "federated learning"
        ]
        
        logger.info(f"ğŸ¤– Enhanced News Collector initialized (Gemini: {'âœ…' if GEMINI_AVAILABLE else 'âŒ'})")
    
    def collect_and_analyze_feeds(self, feeds_config: str = "feeds.yml") -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰åé›†ã¨åˆ†æã®çµ±åˆå‡¦ç†"""
        
        # 1. å¾“æ¥ã®ãƒ•ã‚£ãƒ¼ãƒ‰åé›†
        logger.info("ğŸ“¡ RSS ãƒ•ã‚£ãƒ¼ãƒ‰åé›†é–‹å§‹...")
        raw_articles = self._collect_rss_feeds(feeds_config)
        
        # 2. AIé–¢é€£è¨˜äº‹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        logger.info("ğŸ” AIé–¢é€£è¨˜äº‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°...")
        ai_articles = self._filter_ai_articles(raw_articles)
        
        # 3. Gemini URL contextã«ã‚ˆã‚‹æ·±ã„åˆ†æ
        if GEMINI_AVAILABLE and ai_articles:
            logger.info("ğŸ§  Gemini URL contextåˆ†æé–‹å§‹...")
            enhanced_articles = self._analyze_with_gemini(ai_articles)
        else:
            logger.warning("âš ï¸ Geminiåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—")
            enhanced_articles = ai_articles
        
        # 4. ã‚«ãƒ†ã‚´ãƒªåˆ¥æ•´ç†
        categorized = self._categorize_articles(enhanced_articles)
        
        # 5. é‡è¤‡é™¤å»ã¨å“è³ªãƒ•ã‚£ãƒ«ã‚¿
        final_articles = self._deduplicate_and_filter(categorized)
        
        return {
            "articles": final_articles,
            "statistics": self._calculate_statistics(final_articles),
            "timestamp": datetime.now().isoformat()
        }
    
    def _collect_rss_feeds(self, feeds_config: str) -> List[Dict[str, Any]]:
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰åé›†ï¼ˆå¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯æ´»ç”¨ï¼‰"""
        articles = []
        
        # feeds.ymlã®èª­ã¿è¾¼ã¿
        import yaml
        try:
            with open(feeds_config, 'r', encoding='utf-8') as f:
                feeds = yaml.safe_load(f)
        except FileNotFoundError:
            logger.error(f"âŒ {feeds_config} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return articles
        
        # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=24)
        
        for category, feed_list in feeds.items():
            if not isinstance(feed_list, list):
                continue
                
            logger.info(f"ğŸ“° {category}ã‚«ãƒ†ã‚´ãƒª: {len(feed_list)}ãƒ•ã‚£ãƒ¼ãƒ‰")
            
            for feed_info in feed_list:
                if isinstance(feed_info, dict):
                    url = feed_info.get('url')
                    source = feed_info.get('name', 'Unknown')
                    is_general = feed_info.get('general', False)
                else:
                    url = feed_info
                    source = 'Unknown'
                    is_general = False
                
                if not url:
                    continue
                
                try:
                    # RSSè§£æ
                    feed = feedparser.parse(url)
                    
                    for entry in feed.entries[:10]:  # æœ€æ–°10ä»¶
                        # æ—¥æ™‚ãƒã‚§ãƒƒã‚¯
                        pub_time = None
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            pub_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                        elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                            pub_time = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                        
                        if pub_time and pub_time < cutoff_time:
                            continue  # å¤ã„è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        
                        # è¨˜äº‹æƒ…å ±ã‚’æ§‹ç¯‰
                        article = {
                            'title': entry.get('title', '').strip(),
                            'link': entry.get('link', ''),
                            'summary': entry.get('summary', ''),
                            'source': source,
                            'category': category,
                            'published': pub_time.isoformat() if pub_time else '',
                            'is_general_feed': is_general,
                            'raw_entry': entry
                        }
                        
                        articles.append(article)
                        
                except Exception as e:
                    logger.error(f"âŒ ãƒ•ã‚£ãƒ¼ãƒ‰è§£æã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                    continue
        
        logger.info(f"ğŸ“Š åé›†å®Œäº†: {len(articles)}ä»¶ã®è¨˜äº‹")
        return articles
    
    def _filter_ai_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """AIé–¢é€£è¨˜äº‹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        ai_articles = []
        
        for article in articles:
            # ä¸€èˆ¬ãƒ•ã‚£ãƒ¼ãƒ‰ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if article.get('is_general_feed', False):
                title = article.get('title', '').lower()
                summary = article.get('summary', '').lower()
                content = f"{title} {summary}"
                
                # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                if any(keyword.lower() in content for keyword in self.ai_keywords):
                    ai_articles.append(article)
            else:
                # AIå°‚é–€ãƒ•ã‚£ãƒ¼ãƒ‰ã¯ãã®ã¾ã¾æ¡ç”¨
                ai_articles.append(article)
        
        logger.info(f"ğŸ¯ AIé–¢é€£è¨˜äº‹: {len(ai_articles)}ä»¶")
        return ai_articles
    
    def _analyze_with_gemini(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Gemini URL contextã«ã‚ˆã‚‹è¨˜äº‹åˆ†æ"""
        if not self.gemini_client:
            return articles
        
        enhanced_articles = []
        batch_size = 5  # ä¸€åº¦ã«åˆ†æã™ã‚‹URLæ•°
        
        # ãƒãƒƒãƒå‡¦ç†
        for i in range(0, len(articles), batch_size):
            batch = articles[i:i + batch_size]
            batch_urls = [article['link'] for article in batch if article.get('link')]
            
            if not batch_urls:
                enhanced_articles.extend(batch)
                continue
            
            try:
                logger.info(f"ğŸ§  ãƒãƒƒãƒ{i//batch_size + 1}: {len(batch_urls)}è¨˜äº‹ã‚’åˆ†æä¸­...")
                
                # Geminiåˆ†æå®Ÿè¡Œ
                analysis_result = self.gemini_client.summarize_news_articles(
                    article_urls=batch_urls,
                    focus_topics=["æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰", "ä¼æ¥­å‹•å‘", "å¸‚å ´ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ", "æ—¥æœ¬ã¸ã®å½±éŸ¿"]
                )
                
                # çµæœã‚’ãƒãƒƒãƒè¨˜äº‹ã«çµ±åˆ
                for j, article in enumerate(batch):
                    enhanced_article = article.copy()
                    
                    # Geminiåˆ†æçµæœã‚’è¿½åŠ 
                    enhanced_article.update({
                        'gemini_analysis': analysis_result.get('text', ''),
                        'analysis_metadata': {
                            'url_context': analysis_result.get('url_context_metadata'),
                            'usage': analysis_result.get('usage_metadata'),
                            'timestamp': analysis_result.get('timestamp')
                        },
                        'enhanced': True
                    })
                    
                    enhanced_articles.append(enhanced_article)
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                import time
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"âŒ Geminiåˆ†æã‚¨ãƒ©ãƒ¼ (batch {i//batch_size + 1}): {e}")
                # åˆ†æå¤±æ•—æ™‚ã¯å…ƒè¨˜äº‹ã‚’ãã®ã¾ã¾è¿½åŠ 
                for article in batch:
                    article['enhanced'] = False
                enhanced_articles.extend(batch)
        
        logger.info(f"âœ… Geminiåˆ†æå®Œäº†: {len(enhanced_articles)}ä»¶")
        return enhanced_articles
    
    def _categorize_articles(self, articles: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªåˆ¥æ•´ç†"""
        categories = {
            'business': [],
            'technology': [],
            'research': [],
            'tools': [],
            'other': []
        }
        
        # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°
        category_mapping = {
            'business': 'business',
            'tech': 'technology',
            'posts': 'research',
            'tools': 'tools'
        }
        
        for article in articles:
            original_category = article.get('category', 'other')
            mapped_category = category_mapping.get(original_category, 'other')
            categories[mapped_category].append(article)
        
        logger.info(f"ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥è¨˜äº‹æ•°: {[(k, len(v)) for k, v in categories.items()]}")
        return categories
    
    def _deduplicate_and_filter(self, categorized: Dict[str, List[Dict[str, Any]]]) -> Dict[str, List[Dict[str, Any]]]:
        """é‡è¤‡é™¤å»ã¨å“è³ªãƒ•ã‚£ãƒ«ã‚¿"""
        filtered_categories = {}
        
        for category, articles in categorized.items():
            # URLé‡è¤‡é™¤å»
            seen_urls = set()
            unique_articles = []
            
            for article in articles:
                url = article.get('link', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    unique_articles.append(article)
            
            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            scored_articles = []
            for article in unique_articles:
                score = self._calculate_quality_score(article)
                article['quality_score'] = score
                scored_articles.append(article)
            
            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆã€ä¸Šä½è¨˜äº‹ã‚’é¸æŠ
            scored_articles.sort(key=lambda x: x['quality_score'], reverse=True)
            filtered_categories[category] = scored_articles[:15]  # å„ã‚«ãƒ†ã‚´ãƒªæœ€å¤§15ä»¶
        
        return filtered_categories
    
    def _calculate_quality_score(self, article: Dict[str, Any]) -> float:
        """è¨˜äº‹å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0.0
        
        # ã‚¿ã‚¤ãƒˆãƒ«é•·ï¼ˆé©åº¦ãªé•·ã•ã‚’è©•ä¾¡ï¼‰
        title_len = len(article.get('title', ''))
        if 20 <= title_len <= 100:
            score += 1.0
        
        # è¦ç´„ã®æœ‰ç„¡
        if article.get('summary') and len(article.get('summary', '')) > 50:
            score += 1.0
        
        # Geminiåˆ†ææ¸ˆã¿
        if article.get('enhanced', False):
            score += 2.0
        
        # ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹
        source = article.get('source', '').lower()
        trusted_sources = ['techcrunch', 'venturebeat', 'wired', 'arxiv', 'google', 'microsoft', 'openai']
        if any(trusted in source for trusted in trusted_sources):
            score += 1.0
        
        # æœ€æ–°æ€§ï¼ˆ24æ™‚é–“ä»¥å†…ï¼‰
        if article.get('published'):
            try:
                pub_time = datetime.fromisoformat(article['published'].replace('Z', '+00:00'))
                hours_ago = (datetime.now(timezone.utc) - pub_time).total_seconds() / 3600
                if hours_ago <= 24:
                    score += 1.0
            except:
                pass
        
        return score
    
    def _calculate_statistics(self, articles: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±è¨ˆç®—"""
        stats = {
            'total_articles': sum(len(v) for v in articles.values()),
            'by_category': {k: len(v) for k, v in articles.items()},
            'enhanced_count': 0,
            'sources': set(),
            'avg_quality_score': 0.0
        }
        
        all_articles = []
        for article_list in articles.values():
            all_articles.extend(article_list)
        
        if all_articles:
            stats['enhanced_count'] = sum(1 for a in all_articles if a.get('enhanced', False))
            stats['sources'] = list(set(a.get('source', '') for a in all_articles))
            quality_scores = [a.get('quality_score', 0) for a in all_articles]
            stats['avg_quality_score'] = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        return stats
    
    def save_results(self, results: Dict[str, Any], filename: str = None) -> str:
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"enhanced_news_{timestamp}.json"
        
        output_path = self.cache_dir / filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ çµæœä¿å­˜å®Œäº†: {output_path}")
        return str(output_path)

def run_enhanced_collection():
    """å¼·åŒ–ç‰ˆãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã®å®Ÿè¡Œ"""
    print("ğŸš€ Enhanced News Collection with Gemini URL Context")
    
    try:
        collector = EnhancedNewsCollector()
        
        # ãƒ¡ã‚¤ãƒ³åé›†å‡¦ç†
        results = collector.collect_and_analyze_feeds()
        
        # çµæœä¿å­˜
        output_file = collector.save_results(results)
        
        # çµ±è¨ˆè¡¨ç¤º
        stats = results['statistics']
        print(f"\\nğŸ“Š åé›†çµæœ:")
        print(f"   ç·è¨˜äº‹æ•°: {stats['total_articles']}ä»¶")
        print(f"   Geminiåˆ†ææ¸ˆã¿: {stats['enhanced_count']}ä»¶")
        print(f"   æƒ…å ±æº: {len(stats['sources'])}å€‹")
        print(f"   å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {stats['avg_quality_score']:.2f}")
        
        print(f"\\nğŸ“‚ çµæœãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
        return results
        
    except Exception as e:
        logger.error(f"âŒ åé›†å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    run_enhanced_collection()