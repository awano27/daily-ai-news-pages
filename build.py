# -*- coding: utf-8 -*-
"""
Daily AI News - static site generator (JST)
- Summaries are translated to Japanese (no API key) using deep-translator.
- Primary engine: GoogleTranslator (unofficial). Fallback: MyMemory (ja-JP).
- Caches translations to _cache/translations.json to avoid repeated calls.
- Reads RSS list from feeds.yml with categories: Business, Tools, Posts.
- Injects X posts from a CSV file into the 'Posts' category.

Env (optional):
  HOURS_LOOKBACK=24        # Fetch window in hours
  MAX_ITEMS_PER_CATEGORY=8 # Max cards per tab
  TRANSLATE_TO_JA=1        # 1=enable JA summaries, 0=disable
  TRANSLATE_ENGINE=google  # google|mymemory
  X_POSTS_CSV=_sources/x_favorites.csv # Path to X posts CSV
  TZ=Asia/Tokyo            # for timestamps
"""
import os, re, sys, json, time, html, csv, io
from datetime import datetime, timezone, timedelta
from pathlib import Path
from urllib.parse import urlparse
from urllib.request import urlopen

import yaml
import feedparser

# ---------- config ----------
HOURS_LOOKBACK = int(os.getenv("HOURS_LOOKBACK", "24"))
MAX_ITEMS_PER_CATEGORY = int(os.getenv("MAX_ITEMS_PER_CATEGORY", "8"))
TRANSLATE_TO_JA = os.getenv("TRANSLATE_TO_JA", "1") == "1"
TRANSLATE_ENGINE = os.getenv("TRANSLATE_ENGINE", "google").lower()
# Google Sheets CSV URL for live X posts data
GOOGLE_SHEETS_URL = "https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0"
X_POSTS_CSV = os.getenv("X_POSTS_CSV", GOOGLE_SHEETS_URL)

JST = timezone(timedelta(hours=9))
NOW = datetime.now(JST)

CACHE_DIR = Path("_cache")
CACHE_DIR.mkdir(exist_ok=True)
CACHE_FILE = CACHE_DIR / "translations.json"

def load_cache():
    try:
        if CACHE_FILE.exists():
            return json.loads(CACHE_FILE.read_text(encoding="utf-8"))
    except Exception:
        pass
    return {}

def save_cache(cache):
    try:
        CACHE_DIR.mkdir(exist_ok=True)
        CACHE_FILE.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception:
        pass

# ---------- translation ----------
def looks_japanese(s: str) -> bool:
    if not s:
        return False
    # Hiragana, Katakana, CJK
    return re.search(r"[\u3040-\u30ff\u3400-\u9fff]", s) is not None

class JaTranslator:
    def __init__(self, engine="google"):
        self.engine = engine
        self._gt = None
        self._mm = None
        self.warned = False

    def _google(self, text: str) -> str:
        if self._gt is None:
            from deep_translator import GoogleTranslator
            self._gt = GoogleTranslator(source="auto", target="ja")
        return self._gt.translate(text)

    def _mymemory(self, text: str) -> str:
        # deep-translator >=1.11.0 expects region code 'ja-JP' for MyMemory
        if self._mm is None:
            from deep_translator import MyMemoryTranslator
            self._mm = MyMemoryTranslator(source="en-GB", target="ja-JP")
        return self._mm.translate(text)

    def translate(self, text: str) -> str:
        if not text or looks_japanese(text):
            return text
        try:
            if self.engine == "google":
                try:
                    return self._google(text)
                except Exception:
                    return self._mymemory(text)
            elif self.engine == "mymemory":
                try:
                    return self._mymemory(text)
                except Exception:
                    return self._google(text)
            else:
                # unknown -> google then mymemory
                try:
                    return self._google(text)
                except Exception:
                    return self._mymemory(text)
        except Exception as e:
            if not self.warned:
                print(f"[WARN] translation disabled due to error: {e.__class__.__name__}: {e}")
                self.warned = True
            return text

# ---------- X (Twitter) post injection ----------
def _read_csv_bytes(path_or_url: str) -> bytes:
    if re.match(r'^https?://', path_or_url, re.I):
        with urlopen(path_or_url) as r:
            return r.read()
    with open(path_or_url, 'rb') as f:
        return f.read()

def _extract_x_data_from_csv(raw: bytes) -> list[dict]:
    # æ–‡å­—ã‚³ãƒ¼ãƒ‰è‡ªå‹•åˆ¤å®š(UTF-8 BOMå„ªå…ˆâ†’UTF-8â†’CP932)
    for enc in ('utf-8-sig','utf-8','cp932'):
        try:
            txt = raw.decode(enc)
            break
        except Exception:
            continue
    else:
        txt = raw.decode('utf-8', errors='ignore')

    # CSVå½¢å¼: "æ—¥ä»˜", "@ãƒ¦ãƒ¼ã‚¶ãƒ¼", "ãƒ†ã‚­ã‚¹ãƒˆ", "ç”»åƒURL", "ãƒ„ã‚¤ãƒ¼ãƒˆURL"
    data = []
    try:
        rdr = csv.reader(io.StringIO(txt))
        for row in rdr:
            if len(row) >= 5:
                date_str = row[0].strip('"')
                username = row[1].strip('"')
                text = row[2].strip('"')
                tweet_url = row[4].strip('"')
                
                # URLãŒX/Twitterã®ã‚‚ã®ã‹ãƒã‚§ãƒƒã‚¯
                if re.match(r'https?://(?:x|twitter)\.com/', tweet_url):
                    # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆè¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œï¼‰
                    dt = None
                    # è¤‡æ•°ã®æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è©¦ã™
                    date_formats = [
                        "%B %d, %Y at %I:%M%p",  # "August 10, 2025 at 02:41AM"
                        "%B %d, %Y"               # "August 13, 2025"
                    ]
                    for fmt in date_formats:
                        try:
                            dt = datetime.strptime(date_str, fmt)
                            dt = dt.replace(tzinfo=JST)  # JSTã¨ã—ã¦æ‰±ã†
                            break
                        except:
                            continue
                    
                    # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç¾åœ¨æ™‚åˆ»ã«ã—ãªã„ï¼‰
                    if dt is None:
                        continue
                    
                    if dt is not None:  # æ—¥ä»˜ãŒæ­£ã—ããƒ‘ãƒ¼ã‚¹ã§ããŸå ´åˆã®ã¿è¿½åŠ 
                        data.append({
                            'url': tweet_url,
                            'username': username,
                            'text': text,
                            'datetime': dt
                        })
    except Exception as e:
        print(f"[WARN] CSV parsing error: {e}")
        pass

    # å¤ã„å½¢å¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆURLæŠ½å‡ºã®ã¿ï¼‰
    if not data:
        urls = re.findall(r'https?://(?:x|twitter)\.com/[^\s,"]+', txt)
        for url in urls:
            data.append({
                'url': url,
                'username': '',
                'text': '',
                'datetime': NOW
            })
    
    return data

def _extract_x_urls_from_csv(raw: bytes) -> list[str]:
    # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
    data = _extract_x_data_from_csv(raw)
    
    # æ­£è¦åŒ– & é‡è¤‡é™¤å»ï¼ˆé †åºç¶­æŒï¼‰
    seen, out = set(), []
    for item in data:
        u = item['url'].replace('x.com/','twitter.com/')  # æ­£è¦åŒ–
        if u not in seen:
            seen.add(u); out.append(u)
    return out

def _author_from_url(u: str) -> str:
    try:
        p = urlparse(u)
        parts = p.path.strip('/').split('/')
        return '@'+parts[0] if parts and parts[0] else 'X'
    except Exception:
        return 'X'

def gather_x_posts(csv_path: str) -> list[dict]:
    # Check if it's a URL or local file
    is_url = csv_path.startswith('http')
    
    if not is_url and not Path(csv_path).exists():
        print(f"[INFO] X posts CSV not found at: {csv_path}")
        return []
    
    if is_url:
        print(f"[INFO] Loading X posts from Google Sheets: {csv_path}")
    else:
        print(f"[INFO] Loading X posts from local file: {csv_path}")
    items = []
    try:
        raw = _read_csv_bytes(csv_path)
        x_data = _extract_x_data_from_csv(raw)
        print(f"[INFO] Extracted {len(x_data)} X posts from CSV.")
        
        for data in x_data:
            url = data['url']
            username = data['username'] or _author_from_url(url)
            post_date = data['datetime']
            text_preview = data['text'][:50] + '...' if len(data['text']) > 50 else data['text']
            
                        # 8/14ä»¥é™ã®æŠ•ç¨¿ã®ã¿å«ã‚ã‚‹
            aug14_jst = datetime(2025, 8, 14, 0, 0, 0, tzinfo=JST)
            if post_date >= aug14_jst and (NOW - post_date) <= timedelta(hours=HOURS_LOOKBACK):
                items.append({
                    "title": f"Xãƒã‚¹ãƒˆ {username}",
                    "link": url,
                    "_summary": text_preview or "æ‰‹å‹•ã§ã€Œã„ã„ã­ã€ã—ãŸãƒã‚¹ãƒˆã‹ã‚‰è‡ªå‹•æŠ½å‡ºï¼ˆè¦ç´„ãªã—ï¼‰",
                    "_source": "X / SNS", 
                    "_dt": post_date,  # å®Ÿéš›ã®æŠ•ç¨¿æ—¥æ™‚ã‚’ä½¿ç”¨
                })
        
        print(f"[INFO] Created {len(items)} X post items (filtered to last {HOURS_LOOKBACK} hours).")
    except Exception as e:
        print(f"[WARN] Failed to process X posts CSV: {e}")
    return items

# ---------- HTML template ----------
PAGE_TMPL = """<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Daily AI News â€” {updated_title}</title>
  <link rel="stylesheet" href="style.css"/>
</head>
<body>
  <header class="site-header">
    <div class="brand">ğŸ“° Daily AI News</div>
    <nav class="nav-links">
      <a href="ai_news_dashboard.html" class="nav-link">ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</a>
    </nav>
    <div class="updated">æœ€çµ‚æ›´æ–°ï¼š{updated_full}</div>
  </header>

  <main class="container">
    <h1 class="page-title">ä»Šæ—¥ã®æœ€æ–°AIæƒ…å ±</h1>
    <p class="lead">ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãƒ„ãƒ¼ãƒ«æƒ…å ±ãƒ»SNS/è«–æ–‡ãƒã‚¹ãƒˆã«åˆ†ã‘ã€ç›´è¿‘{lookback}æ™‚é–“ã®æ›´æ–°ã‚’é…ä¿¡ã—ã¾ã™ã€‚ã‚½ãƒ¼ã‚¹ã¯åŸæ–‡ã€è¦ç´„ã®ã¿æ—¥æœ¬èªåŒ–ã€‚</p>

    <section class="kpi-grid">
      <div class="kpi-card">
        <div class="kpi-value">{cnt_business}ä»¶</div>
        <div class="kpi-label">ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹</div>
        <div class="kpi-note">é‡è¦åº¦é«˜ã‚</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{cnt_tools}ä»¶</div>
        <div class="kpi-label">ãƒ„ãƒ¼ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹</div>
        <div class="kpi-note">é–‹ç™ºè€…å‘ã‘</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{cnt_posts}ä»¶</div>
        <div class="kpi-label">SNS/è«–æ–‡ãƒã‚¹ãƒˆ</div>
        <div class="kpi-note">æ¤œè¨¼ç³»</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{updated_full}</div>
        <div class="kpi-label">æœ€çµ‚æ›´æ–°</div>
        <div class="kpi-note">JST</div>
      </div>
    </section>

    <nav class="tabs" role="tablist">
      <button class="tab active" data-target="#business" aria-selected="true">ğŸ¢ ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹</button>
      <button class="tab" data-target="#tools" aria-selected="false">âš¡ ãƒ„ãƒ¼ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹</button>
      <button class="tab" data-target="#posts" aria-selected="false">ğŸ§ª SNS/è«–æ–‡ãƒã‚¹ãƒˆ</button>
    </nav>

    <!-- æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹: ã‚¿ã‚¤ãƒˆãƒ«ã‚„è¦ç´„ã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™ -->
    <div class="search-container">
      <input id="searchBox" type="text" placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è¨˜äº‹ã‚’æ¤œç´¢..." aria-label="æ¤œç´¢" />
    </div>

    {sections}
    <section class="note">
      <p>æ–¹é‡ï¼šä¸€æ¬¡æƒ…å ±ï¼ˆå…¬å¼ãƒ–ãƒ­ã‚°/ãƒ—ãƒ¬ã‚¹/è«–æ–‡ï¼‰ã‚’å„ªå…ˆã€‚ä¸€èˆ¬ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ AI ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æŠ½å‡ºã€‚è¦ç´„ã¯æ—¥æœ¬èªåŒ–ã—ã€<strong>å‡ºå…¸ãƒªãƒ³ã‚¯ã¯åŸæ–‡</strong>ã®ã¾ã¾ã€‚</p>
    </section>
  </main>

  <footer class="site-footer">
    <div>Generated by <code>build.py</code> Â· Timezone: JST</div>
    <div><a href="https://github.com/">Hosted on GitHub Pages</a></div>
  </footer>

  <script>
    const tabs = document.querySelectorAll('.tab');
    tabs.forEach(btn => btn.addEventListener('click', () => {{
      tabs.forEach(b => {{ b.classList.remove('active'); b.setAttribute('aria-selected','false'); }});
      btn.classList.add('active'); btn.setAttribute('aria-selected','true');
      document.querySelectorAll('.tab-panel').forEach(p => p.classList.add('hidden'));
      const target = document.querySelector(btn.dataset.target);
      if (target) target.classList.remove('hidden');
    }}));

    // æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã®å…¥åŠ›ã«å¿œã˜ã¦ã‚«ãƒ¼ãƒ‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
    const searchBox = document.getElementById('searchBox');
    if (searchBox) {{
      searchBox.addEventListener('input', () => {{
        const query = searchBox.value.toLowerCase();
        // ã™ã¹ã¦ã®ã‚«ãƒ¼ãƒ‰ã‚’å¯¾è±¡ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
        document.querySelectorAll('.card').forEach(card => {{
          const titleEl = card.querySelector('.card-title');
          const summaryEl = card.querySelector('.card-summary');
          const title = titleEl ? titleEl.textContent.toLowerCase() : '';
          const summary = summaryEl ? summaryEl.textContent.toLowerCase() : '';
          if (!query || title.includes(query) || summary.includes(query)) {{
            card.style.display = '';
          }} else {{
            card.style.display = 'none';
          }}
        }});
      }});
    }}
  </script>
</body>
</html>
"""

SECTION_TMPL = """
<section id="{sec_id}" class="tab-panel {extra_class}">
{cards}
</section>
"""

CARD_TMPL = """
<article class="card">
  <div class="card-header">
    <a class="card-title" href="{link}" target="_blank" rel="noopener">{title}</a>
  </div>
  <div class="card-body">
    <p class="card-summary">{summary}</p>
    <div class="chips">
      <span class="chip">{source_name}</span>
      <span class="chip ghost">{summary_lang}</span>
      <span class="chip ghost">{ago}</span>
    </div>
  </div>
  <div class="card-footer">
    å‡ºå…¸: <a href="{link}" target="_blank" rel="noopener">{link}</a>
  </div>
</article>
"""

EMPTY_TMPL = '<div class="empty">æ–°ç€ãªã—ï¼ˆæœŸé–“ã‚’åºƒã’ã‚‹ã‹ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼‰</div>'

def ago_str(dt: datetime) -> str:
    delta = NOW - dt
    secs = int(delta.total_seconds())
    if secs < 60: return f"{secs}ç§’å‰"
    mins = secs // 60
    if mins < 60: return f"{mins}åˆ†å‰"
    hrs = mins // 60
    if hrs < 24: return f"{hrs}æ™‚é–“å‰"
    days = hrs // 24
    return f"{days}æ—¥å‰"

def clean_html(s: str) -> str:
    if not s: return ""
    # strip tags very lightly
    s = re.sub(r"<.*?>", "", s)
    s = s.replace("&nbsp;", " ").strip()
    return html.escape(s, quote=False)

def pick_summary(entry) -> str:
    for key in ("summary", "subtitle", "description"):
        if key in entry and entry[key]:
            return clean_html(entry[key])
    return clean_html(entry.get("title", ""))

def parse_feeds():
    raw = yaml.safe_load(Path("feeds.yml").read_text(encoding="utf-8"))
    return raw or {}

def get_category(conf, category_name):
    """Case-insensitive category lookup"""
    # Try exact match first
    if category_name in conf:
        return conf[category_name]
    # Try case-insensitive match
    for key, value in conf.items():
        if key.lower() == category_name.lower():
            return value
    return []

def within_window(published_parsed):
    if not published_parsed: 
        return True, NOW  # keep if unknown, use current time
    dt = datetime.fromtimestamp(time.mktime(published_parsed), tz=timezone.utc).astimezone(JST)
    return (NOW - dt) <= timedelta(hours=HOURS_LOOKBACK), dt

def is_ai_relevant(title: str, summary: str) -> bool:
    """
    AIã«é–¢é€£æ€§ã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    ã‚ˆã‚Šå³æ ¼ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§è³ªã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’é¸åˆ¥
    """
    content = f"{title} {summary}".lower()
    
    # é«˜é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã‚‰ãŒã‚ã‚Œã°å¿…ãšå«ã‚ã‚‹ï¼‰
    high_relevance = [
        'artificial intelligence', 'machine learning', 'deep learning', 'neural network',
        'gpt', 'llm', 'large language model', 'transformer', 'bert', 'claude',
        'chatgpt', 'gemini', 'copilot', 'anthropic', 'openai', 'deepmind',
        'computer vision', 'natural language processing', 'nlp', 'reinforcement learning',
        'generative ai', 'ai model', 'ai research', 'ai breakthrough',
        'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ',
        'ï¼¡ï¼©', 'AI', 'ML', 'DL'
    ]
    
    # ä¸­é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ã‚ã‚Œã°å«ã‚ã‚‹ï¼‰
    medium_relevance = [
        'algorithm', 'automation', 'robot', 'autonomous', 'prediction',
        'data science', 'analytics', 'intelligent', 'smart system',
        'cognitive', 'inference', 'classification', 'recognition',
        'generation', 'synthesis', 'optimization', 'recommendation',
        'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ', 'è‡ªå‹•åŒ–', 'ãƒ­ãƒœãƒƒãƒˆ', 'è‡ªå¾‹', 'äºˆæ¸¬',
        'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹', 'åˆ†æ', 'ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ', 'ã‚¹ãƒãƒ¼ãƒˆ',
        'èªè­˜', 'ç”Ÿæˆ', 'æœ€é©åŒ–', 'ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰'
    ]
    
    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã‚‰ãŒã‚ã‚Œã°é™¤å¤–ï¼‰
    exclude_keywords = [
        'cryptocurrency', 'crypto', 'blockchain', 'bitcoin', 'nft',
        'gaming', 'game', 'sports', 'entertainment', 'music', 'movie',
        'politics', 'political', 'election', 'government policy',
        'weather', 'climate change', 'environmental',
        'æš—å·é€šè²¨', 'ã‚²ãƒ¼ãƒ ', 'ã‚¹ãƒãƒ¼ãƒ„', 'å¨¯æ¥½', 'éŸ³æ¥½', 'æ˜ ç”»',
        'æ”¿æ²»', 'é¸æŒ™', 'å¤©æ°—', 'æ°—å€™å¤‰å‹•', 'ç’°å¢ƒ'
    ]
    
    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    for keyword in exclude_keywords:
        if keyword in content:
            return False
    
    # é«˜é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    high_score = sum(1 for keyword in high_relevance if keyword in content)
    if high_score >= 1:
        return True
    
    # ä¸­é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆ2ã¤ä»¥ä¸Šã§æ¡ç”¨ï¼‰
    medium_score = sum(1 for keyword in medium_relevance if keyword in content)
    if medium_score >= 2:
        return True
    
    return False

def build_cards(items, translator):
    cards = []
    for it in items[:MAX_ITEMS_PER_CATEGORY]:
        title = it.get("title") or "(no title)"
        link = it.get("link") or "#"
        src  = it.get("_source") or ""
        dt   = it.get("_dt") or NOW
        raw_summary = it.get("_summary") or ""
        ja_summary = raw_summary
        did_translate = False

        if TRANSLATE_TO_JA and translator and raw_summary and not looks_japanese(raw_summary):
            # cache key: stable on link+hash(summary)
            cache_key = f"{link}::{hash(raw_summary)}"
            cached = TRANSLATION_CACHE.get(cache_key)
            if cached:
                ja_summary = cached
                did_translate = True
            else:
                try:
                    ja = translator.translate(raw_summary)
                    if ja and ja != raw_summary:
                        ja_summary = ja
                        TRANSLATION_CACHE[cache_key] = ja_summary
                        did_translate = True
                except Exception as e:
                    print(f"[WARN] Translation failed for {link[:50]}: {e}")

        cards.append(CARD_TMPL.format(
            link=html.escape(link, quote=True),
            title=html.escape(title, quote=False),
            summary=(ja_summary if did_translate else raw_summary),
            source_name=html.escape(src, quote=False),
            summary_lang=("è¦ç´„: æ—¥æœ¬èª" if did_translate else "è¦ç´„: è‹±èª"),
            ago=ago_str(dt),
        ))
    return "\n".join(cards) if cards else EMPTY_TMPL

def gather_items(feeds, category_name):
    items = []
    print(f"[INFO] Processing {len(feeds)} feeds for {category_name}")
    for f in feeds:
        url = f.get("url")
        name = f.get("name", url)
        if not url: 
            print(f"[WARN] No URL for feed: {name}")
            continue
        try:
            print(f"[INFO] Fetching: {name}")
            d = feedparser.parse(url)
            if d.bozo:
                print(f"[WARN] Feed parse warning for {name}: {getattr(d, 'bozo_exception', 'unknown')}")
        except Exception as e:
            print(f"[ERROR] feed parse error: {name}: {e}")
            continue
        entry_count = 0
        filtered_count = 0
        for e in d.entries:
            ok, dt = within_window(e.get("published_parsed") or e.get("updated_parsed"))
            if not ok:
                continue
            
            title = e.get("title", "")
            summary = pick_summary(e)
            
            # generalãƒ•ã‚£ãƒ¼ãƒ‰ã®å ´åˆã¯AIé–¢é€£åº¦ã‚’ãƒã‚§ãƒƒã‚¯
            is_general_feed = f.get("general", False)
            if is_general_feed:
                if not is_ai_relevant(title, summary):
                    filtered_count += 1
                    continue
            
            entry_count += 1
            items.append({
                "title": title,
                "link": e.get("link", ""),
                "_summary": summary,
                "_source": name,
                "_dt": dt,
            })
        if entry_count > 0:
            if filtered_count > 0:
                print(f"[INFO] Found {entry_count} recent items from {name} (filtered out {filtered_count} non-AI items)")
            else:
                print(f"[INFO] Found {entry_count} recent items from {name}")
    # sort by time desc
    items.sort(key=lambda x: x["_dt"], reverse=True)
    print(f"[INFO] {category_name}: Total {len(items)} items found")
    return items

def main():
    print(f"[INFO] Starting build at {NOW.strftime('%Y-%m-%d %H:%M JST')}")
    print(f"[INFO] HOURS_LOOKBACK={HOURS_LOOKBACK}, MAX_ITEMS_PER_CATEGORY={MAX_ITEMS_PER_CATEGORY}")
    print(f"[INFO] TRANSLATE_TO_JA={TRANSLATE_TO_JA}, TRANSLATE_ENGINE={TRANSLATE_ENGINE}")
    
    global TRANSLATION_CACHE
    TRANSLATION_CACHE = load_cache()
    print(f"[INFO] Loaded {len(TRANSLATION_CACHE)} cached translations")

    try:
        feeds_conf = parse_feeds()
        print(f"[INFO] Loaded {sum(len(v) for v in feeds_conf.values())} feeds from feeds.yml")
    except Exception as e:
        print(f"[ERROR] Failed to parse feeds.yml: {e}")
        feeds_conf = {}
    
    # Gather items with error handling
    try:
        business = gather_items(get_category(feeds_conf, "Business"), "Business")
        print(f"[INFO] Gathered {len(business)} Business items")
    except Exception as e:
        print(f"[ERROR] Failed to gather Business items: {e}")
        business = []
    
    try:
        tools = gather_items(get_category(feeds_conf, "Tools"), "Tools")
        print(f"[INFO] Gathered {len(tools)} Tools items")
    except Exception as e:
        print(f"[ERROR] Failed to gather Tools items: {e}")
        tools = []
    
    try:
        posts = gather_items(get_category(feeds_conf, "Posts"), "Posts")
        print(f"[INFO] Gathered {len(posts)} Posts items")
    except Exception as e:
        print(f"[ERROR] Failed to gather Posts items: {e}")
        posts = []
    
    # Inject X posts
    if X_POSTS_CSV:
        try:
            x_posts = gather_x_posts(X_POSTS_CSV)
            if x_posts:
                print(f"[INFO] Adding {len(x_posts)} X posts")
                posts.extend(x_posts)
            else:
                print(f"[INFO] No X posts to add")
            # Remove duplicates and sort again
            seen_links = set()
            unique_posts = []
            for post in posts:
                if post.get('link') and post['link'] not in seen_links:
                    unique_posts.append(post)
                    seen_links.add(post['link'])
            posts = sorted(unique_posts, key=lambda x: x.get('_dt', NOW), reverse=True)
        except Exception as e:
            print(f"[WARN] Failed to process X posts: {e}")


    try:
        translator = JaTranslator(engine=TRANSLATE_ENGINE)
    except Exception as e:
        print(f"[WARN] Failed to initialize translator: {e}")
        translator = None

    sections_html = []
    try:
        sections_html.append(SECTION_TMPL.format(
            sec_id="business",
            extra_class="",
            cards=build_cards(business[:MAX_ITEMS_PER_CATEGORY], translator)
        ))
    except Exception as e:
        print(f"[ERROR] Failed to build Business section: {e}")
        sections_html.append(SECTION_TMPL.format(sec_id="business", extra_class="", cards=EMPTY_TMPL))
    
    try:
        sections_html.append(SECTION_TMPL.format(
            sec_id="tools",
            extra_class="hidden",
            cards=build_cards(tools[:MAX_ITEMS_PER_CATEGORY], translator)
        ))
    except Exception as e:
        print(f"[ERROR] Failed to build Tools section: {e}")
        sections_html.append(SECTION_TMPL.format(sec_id="tools", extra_class="hidden", cards=EMPTY_TMPL))
    
    try:
        sections_html.append(SECTION_TMPL.format(
            sec_id="posts",
            extra_class="hidden",
            cards=build_cards(posts[:MAX_ITEMS_PER_CATEGORY], translator)
        ))
    except Exception as e:
        print(f"[ERROR] Failed to build Posts section: {e}")
        sections_html.append(SECTION_TMPL.format(sec_id="posts", extra_class="hidden", cards=EMPTY_TMPL))

    print(f"[INFO] Final counts after limiting to {MAX_ITEMS_PER_CATEGORY} per category:")
    print(f"  Business: {len(business[:MAX_ITEMS_PER_CATEGORY])} items")
    print(f"  Tools: {len(tools[:MAX_ITEMS_PER_CATEGORY])} items")
    print(f"  Posts: {len(posts[:MAX_ITEMS_PER_CATEGORY])} items")
    print(f"[INFO] Total items to display: {len(business[:MAX_ITEMS_PER_CATEGORY]) + len(tools[:MAX_ITEMS_PER_CATEGORY]) + len(posts[:MAX_ITEMS_PER_CATEGORY])}")
    html_out = PAGE_TMPL.format(
        updated_title=NOW.strftime("%Y-%m-%d %H:%M JST"),
        updated_full=NOW.strftime("%Y-%m-%d %H:%M JST"),
        lookback=HOURS_LOOKBACK,
        cnt_business=len(business[:MAX_ITEMS_PER_CATEGORY]),
        cnt_tools=len(tools[:MAX_ITEMS_PER_CATEGORY]),
        cnt_posts=len(posts[:MAX_ITEMS_PER_CATEGORY]),
        sections="".join(sections_html)
    )

    try:
        Path("index.html").write_text(html_out, encoding="utf-8")
        print(f"[SUCCESS] Wrote index.html ({len(html_out)} bytes)")
    except Exception as e:
        print(f"[ERROR] Failed to write index.html: {e}")
        raise
    
    try:
        save_cache(TRANSLATION_CACHE)
        print(f"[SUCCESS] Saved {len(TRANSLATION_CACHE)} translations to cache")
    except Exception as e:
        print(f"[WARN] Failed to save cache: {e}")

if __name__ == "__main__":
    main()
