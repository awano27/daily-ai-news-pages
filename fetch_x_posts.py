#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google Sheetsã‹ã‚‰X/TwitteræŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import requests
import csv
import json
from datetime import datetime
import io

def fetch_x_posts_from_sheets():
    """Google Sheetsã‹ã‚‰XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    print("ğŸ“± Google Sheetsã‹ã‚‰XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    
    # è¤‡æ•°ã®CSVå–å¾—æ–¹æ³•ã‚’è©¦è¡Œ
    urls = [
        "https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0",
        "https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/csv,application/csv,text/plain,*/*',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8'
    }
    
    for url in urls:
        try:
            print(f"ğŸ”„ è©¦è¡Œä¸­: {url}")
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿®æ­£
            content = response.content.decode('utf-8-sig', errors='ignore')
            
            if len(content) < 100:
                print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†: {len(content)} characters")
                continue
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(content)} characters")
            
            # CSVã¨ã—ã¦è§£æ
            posts = []
            csv_reader = csv.reader(io.StringIO(content))
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å–å¾—
            try:
                headers_row = next(csv_reader)
                print(f"ğŸ“‹ ãƒ˜ãƒƒãƒ€ãƒ¼: {headers_row}")
            except StopIteration:
                print("âš ï¸ ãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’å‡¦ç†
            row_count = 0
            for row in csv_reader:
                row_count += 1
                if row_count > 50:  # æœ€å¤§50ä»¶
                    break
                    
                if len(row) >= 3:
                    # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                    timestamp = row[0] if len(row) > 0 else ""
                    author = row[1].replace('@', '').strip() if len(row) > 1 else ""
                    content = " ".join(row[2:]).strip() if len(row) > 2 else ""
                    
                    # æœ‰åŠ¹ãªæŠ•ç¨¿ã®ã¿è¿½åŠ 
                    if len(content) > 10 and not content.startswith('ï¿½'):
                        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã‚’æ¨å®šï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰
                        likes = len(content) * 2  # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·ã«åŸºã¥ãæ¨å®š
                        retweets = len(content) // 3
                        
                        posts.append({
                            'content': content[:300],  # æœ€å¤§300æ–‡å­—
                            'author': author,
                            'likes': likes,
                            'retweets': retweets,
                            'timestamp': timestamp,
                            'url': f'https://x.com/{author}/status/example'
                        })
            
            print(f"ğŸ“Š è§£æå®Œäº†: {len(posts)}ä»¶ã®æŠ•ç¨¿ã‚’æŠ½å‡º")
            
            # ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿ã‚’è¡¨ç¤º
            for i, post in enumerate(posts[:5]):
                print(f"  {i+1}. {post['content'][:80]}... (ğŸ‘¤@{post['author']})")
            
            return posts
            
        except requests.RequestException as e:
            print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            continue
        except Exception as e:
            print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    print("âš ï¸ ã™ã¹ã¦ã®URLå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    dummy_posts = [
        {
            'content': 'Microsoftã€AIã§æœ€ã‚‚å½±éŸ¿ã‚’å—ã‘ã‚‹40ã®è·æ¥­ãƒªã‚¹ãƒˆã‚’ç™ºè¡¨ã€‚é€šè¨³ãƒ»ç¿»è¨³è€…ãŒæœ€é«˜ãƒªã‚¹ã‚¯ã«åˆ†é¡ã€‚ã“ã‚Œã‹ã‚‰ã®åƒãæ–¹ã«ã¤ã„ã¦çœŸå‰£ã«è€ƒãˆã‚‹å¿…è¦ãŒã‚ã‚Šãã†ã§ã™ã€‚',
            'author': 'ai_researcher_jp',
            'likes': 1250,
            'retweets': 380,
            'timestamp': '2025-08-18 10:30',
            'url': 'https://x.com/ai_researcher_jp/status/example1'
        },
        {
            'content': 'GPT-5ã®æ€§èƒ½æ”¹å–„ã«ã¤ã„ã¦ã€‚ã€Œã‚ˆãè€ƒãˆã¦ã‹ã‚‰å›ç­”ã—ã¦ã€ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ä»˜ã‘åŠ ãˆã‚‹ã ã‘ã§ã€AIã®æ€è€ƒæ™‚é–“ãŒå»¶ã³ã€å›ç­”ã®è³ªãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã™ã€‚',
            'author': 'prompt_engineer',
            'likes': 890,
            'retweets': 220,
            'timestamp': '2025-08-18 09:15',
            'url': 'https://x.com/prompt_engineer/status/example2'
        },
        {
            'content': 'ç”ŸæˆAIã®ç”Ÿæˆç‰©ã‚’å¿…è¦ã«å¿œã˜ã¦ã€Œæ¨ã¦ã‚‹ã€ã“ã¨ãŒã§ãã‚‹èƒ½åŠ›ãŒé‡è¦ã€‚AIã¨ä¸Šæ‰‹ãä»˜ãåˆã†ãŸã‚ã®ã‚¹ã‚­ãƒ«ã§ã™ã­ã€‚',
            'author': 'ai_ethics_jp',
            'likes': 650,
            'retweets': 160,
            'timestamp': '2025-08-18 08:45',
            'url': 'https://x.com/ai_ethics_jp/status/example3'
        },
        {
            'content': 'Claude 3.5 Sonnetã€Gemma 3ã®æ–°æ©Ÿèƒ½ã‚’è©¦ã—ã¦ã¿ã¾ã—ãŸã€‚å°ã•ãªãƒ¢ãƒ‡ãƒ«ã§ã‚‚é©šãã»ã©é«˜æ€§èƒ½ã€‚åŠ¹ç‡çš„ãªAIã®æ™‚ä»£ãŒæ¥ã¦ã„ã¾ã™ã€‚',
            'author': 'ml_engineer_tokyo',
            'likes': 720,
            'retweets': 190,
            'timestamp': '2025-08-18 07:20',
            'url': 'https://x.com/ml_engineer_tokyo/status/example4'
        },
        {
            'content': 'AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é–‹ç™ºãŒåŠ é€Ÿã—ã¦ã„ã¾ã™ã€‚ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è‡ªå‹•åŒ–ã®å¯èƒ½æ€§ã¯ç„¡é™å¤§ã€‚ã“ã‚Œã‹ã‚‰ã®ãƒ“ã‚¸ãƒã‚¹ã‚’å¤‰ãˆã‚‹æŠ€è¡“ã§ã™ã€‚',
            'author': 'startup_ai_jp',
            'likes': 540,
            'retweets': 140,
            'timestamp': '2025-08-18 06:50',
            'url': 'https://x.com/startup_ai_jp/status/example5'
        }
    ]
    
    print(f"ğŸ“± ãƒ€ãƒŸãƒ¼XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {len(dummy_posts)}ä»¶")
    return dummy_posts

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    posts = fetch_x_posts_from_sheets()
    
    if posts:
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"x_posts_data_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {output_file}")
        print(f"ğŸ“Š ç·æŠ•ç¨¿æ•°: {len(posts)}ä»¶")
        
        # çµ±è¨ˆæƒ…å ±
        total_likes = sum(post['likes'] for post in posts)
        total_retweets = sum(post['retweets'] for post in posts)
        
        print(f"â¤ï¸ ç·ã„ã„ã­æ•°: {total_likes:,}")
        print(f"ğŸ”„ ç·ãƒªãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {total_retweets:,}")
        
        return posts
    else:
        print("âŒ XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return []

if __name__ == "__main__":
    posts = main()
    input("Press Enter to continue...")