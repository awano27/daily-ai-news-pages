# -*- coding: utf-8 -*-
"""
Simple Enhanced Daily AI News - ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
å…ƒã® build.py ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æƒ…å ±é‡ã‚’ç¶­æŒã—ã¤ã¤ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ã‚’è¿½åŠ 

HTML Structure Fix Applied: 2025-08-23
- Enhanced card template with priority system
- Proper HTML tag structure and closure
- CSS generation included for styling
- Tab functionality JavaScript fixed (hidden class logic)
- Force GitHub Actions to use this updated version
"""

import os, re, sys, json, time, html, csv, io
from datetime import datetime, timezone, timedelta
from pathlib import Path
from urllib.parse import urlparse
from urllib.request import urlopen
import yaml
import feedparser
import requests
import random
from bs4 import BeautifulSoup

# åŸºæœ¬è¨­å®š
HOURS_LOOKBACK = int(os.getenv('HOURS_LOOKBACK', '24'))
MAX_ITEMS_PER_CATEGORY = int(os.getenv('MAX_ITEMS_PER_CATEGORY', '25'))
TOP_PICKS_COUNT = int(os.getenv('TOP_PICKS_COUNT', '10'))
TRANSLATE_TO_JA = os.getenv('TRANSLATE_TO_JA', '1') == '1'
TRANSLATE_ENGINE = os.getenv('TRANSLATE_ENGINE', 'google')
X_POSTS_CSV = os.getenv('X_POSTS_CSV', 'https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0')

# ç¿»è¨³æ©Ÿèƒ½
try:
    from deep_translator import GoogleTranslator, MyMemoryTranslator
    TRANSLATE_AVAILABLE = True
    print("âœ… ç¿»è¨³æ©Ÿèƒ½: åˆ©ç”¨å¯èƒ½")
except ImportError:
    print("âš ï¸ ç¿»è¨³æ©Ÿèƒ½: deep-translatoræœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    TRANSLATE_AVAILABLE = False

class SimpleEngineerRanking:
    """AIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢/æ¥­å‹™åŠ¹ç‡åŒ– æœ‰ç”¨åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
    
    # ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé‡ã¿ä»˜ãï¼‰
    TECH_KEYWORDS = {
        # é«˜å„ªå…ˆåº¦ (3.0å€)
        'code': 3.0, 'api': 3.0, 'sdk': 3.0, 'github': 3.0, 'implementation': 3.0,
        'tutorial': 3.0, 'framework': 3.0, 'library': 3.0, 'sample': 2.8,
        
        # AI/ML (2.5å€)
        'pytorch': 2.5, 'tensorflow': 2.5, 'huggingface': 2.5, 'gpt': 2.5, 
        'llm': 2.5, 'openai': 2.5, 'anthropic': 2.5, 'model': 2.5, 'ai': 2.5,
        
        # ã‚¤ãƒ³ãƒ•ãƒ© (2.0å€)
        'docker': 2.0, 'kubernetes': 2.0, 'aws': 2.0, 'azure': 2.0, 'gcp': 2.0,
        'deployment': 2.0, 'production': 2.0, 'mlops': 2.0,
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ (1.8å€)
        'performance': 1.8, 'benchmark': 1.8, 'optimization': 1.8, 'speed': 1.8,
        'memory': 1.8, 'gpu': 1.8, 'cuda': 1.8,
        
        # ç ”ç©¶ (1.5å€) 
        'research': 1.5, 'paper': 1.5, 'arxiv': 1.5, 'algorithm': 1.5,
        'method': 1.5, 'evaluation': 1.5
    }

    # æ¥­å‹™åŠ¹ç‡åŒ–ãƒ»å®Ÿå‹™æ´»ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé‡ã¿ä»˜ãï¼‰
    EFFICIENCY_KEYWORDS = {
        # å¼·ã„æ„å›³ï¼ˆ3.0å€ï¼‰
        'automation': 3.0, 'automate': 3.0, 'workflow': 3.0, 'rpa': 3.0,
        'copilot': 3.0, 'prompt': 2.6, 'prompt engineering': 2.8,
        'zapier': 2.8, 'make.com': 2.4, 'notion': 2.2, 'slack': 2.0,
        'excel': 2.4, 'spreadsheet': 2.2, 'power automate': 2.6,
        'powerapps': 2.2, 'power bi': 2.2, 'apps script': 2.4, 'gas': 2.4,

        # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ2.0-3.0å€ï¼‰
        'è‡ªå‹•åŒ–': 3.0, 'åŠ¹ç‡åŒ–': 2.8, 'æ¥­å‹™åŠ¹ç‡': 2.6, 'çœåŠ›åŒ–': 2.4,
        'ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼': 2.6, 'æ‰‹é †': 2.0, 'ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ': 2.0, 'å°å…¥äº‹ä¾‹': 2.4,
        'æ´»ç”¨äº‹ä¾‹': 2.4, 'ã‚³ãƒ„': 2.0, 'ä½¿ã„æ–¹': 2.2, 'æ™‚çŸ­': 2.2,
        'ã‚¹ã‚¯ãƒªãƒ—ãƒˆ': 2.2, 'ãƒã‚¯ãƒ­': 2.2,
    }
    
    # ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹
    TRUSTED_DOMAINS = [
        'arxiv.org', 'github.com', 'pytorch.org', 'tensorflow.org', 
        'huggingface.co', 'openai.com', 'anthropic.com', 'deepmind.com',
        'ai.googleblog.com', 'research.facebook.com', 'cloud.google.com',
        'learn.microsoft.com', 'devblogs.microsoft.com', 'powerautomate.microsoft.com',
        'zapier.com', 'notion.so', 'workspaceupdates.googleblog.com',
        'salesforce.com', 'atlassian.com', 'ibm.com'
    ]
    
    @classmethod
    def calculate_score(cls, item):
        """AIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢/æ¥­å‹™åŠ¹ç‡åŒ–ã®æœ‰ç”¨åº¦ã‚¹ã‚³ã‚¢ (0-10)"""
        title = item.get('title', '').lower()
        summary = item.get('summary', '').lower()
        url = item.get('url', '').lower()
        
        content = f"{title} {summary}"
        score = 0.0
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        for keyword, weight in cls.TECH_KEYWORDS.items():
            if keyword in content:
                score += weight
                if keyword in title:
                    score += weight * 0.5
        for keyword, weight in cls.EFFICIENCY_KEYWORDS.items():
            if keyword in content:
                score += weight
                if keyword in title:
                    score += weight * 0.6
        
        # ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹ãƒœãƒ¼ãƒŠã‚¹
        domain = urlparse(url).netloc.lower()
        for trusted in cls.TRUSTED_DOMAINS:
            if trusted in domain:
                score *= 1.25
                break
        
        # å®Ÿè£…/ãƒã‚¦ãƒ„ãƒ¼/ã‚³ãƒ¼ãƒ‰ã®ç‰¹åˆ¥ãƒœãƒ¼ãƒŠã‚¹
        howto_indicators = [
            'how to', 'step-by-step', 'guide', 'tutorial', 'best practices',
            'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«', 'æ‰‹é †', 'å…¥é–€', 'ä½¿ã„æ–¹', 'å°å…¥äº‹ä¾‹', 'æ´»ç”¨äº‹ä¾‹'
        ]
        code_indicators = ['```', 'code example', 'implementation', 'github.com', 'gist.github.com']
        if any(x in content for x in howto_indicators + code_indicators):
            score *= 1.15
        
        # 10ç‚¹æº€ç‚¹ã«æ­£è¦åŒ–
        return min(score, 10.0)

def load_translation_cache():
    """ç¿»è¨³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿"""
    cache_file = Path('_cache/translations.json')
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {}

def save_translation_cache(cache):
    """ç¿»è¨³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
    cache_dir = Path('_cache')
    cache_dir.mkdir(exist_ok=True)
    cache_file = cache_dir / 'translations.json'
    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def translate_text(text, target_lang='ja', cache=None):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰"""
    if not TRANSLATE_AVAILABLE or not TRANSLATE_TO_JA:
        return text
    
    if cache is None:
        cache = {}
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    cache_key = f"{text[:100]}_{target_lang}"
    if cache_key in cache:
        return cache[cache_key]
    
    try:
        if TRANSLATE_ENGINE == 'google':
            translator = GoogleTranslator(source='auto', target=target_lang)
        else:
            translator = MyMemoryTranslator(source='auto', target=target_lang)
        
        result = translator.translate(text[:500])  # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ‡ã‚Šè©°ã‚
        cache[cache_key] = result
        return result
    except:
        return text

def load_feeds_config():
    """ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    feeds_file = Path('feeds.yml')
    if feeds_file.exists():
        with open(feeds_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    return {
        'business': [
            {'url': 'https://techcrunch.com/feed/', 'name': 'TechCrunch'},
            {'url': 'https://aws.amazon.com/blogs/machine-learning/feed/', 'name': 'AWS ML Blog'},
            {'url': 'https://ai.googleblog.com/feeds/posts/default', 'name': 'Google AI Blog'}
        ],
        'tools': [
            {'url': 'https://huggingface.co/blog/feed.xml', 'name': 'Hugging Face'},
            {'url': 'https://pytorch.org/blog/feed.xml', 'name': 'PyTorch Blog'},
            {'url': 'https://blog.openai.com/rss/', 'name': 'OpenAI Blog'}
        ],
        'posts': [
            {'url': 'https://www.reddit.com/r/MachineLearning/.rss', 'name': 'Reddit ML'},
            {'url': 'https://arxiv.org/rss/cs.AI', 'name': 'ArXiv AI'},
            {'url': 'https://arxiv.org/rss/cs.LG', 'name': 'ArXiv Learning'}
        ]
    }

def is_recent(published_date, hours_back=24):
    """æŒ‡å®šæ™‚é–“å†…ã®è¨˜äº‹ã‹ãƒã‚§ãƒƒã‚¯"""
    if not published_date:
        return True
    
    try:
        if isinstance(published_date, str):
            # ISOå½¢å¼ã‚„ä¸€èˆ¬çš„ãªå½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
            from dateutil import parser
            pub_time = parser.parse(published_date)
        else:
            # struct_time ã®å ´åˆ
            pub_time = datetime(*published_date[:6], tzinfo=timezone.utc)
        
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_back)
        return pub_time > cutoff_time
    except:
        return True

def fetch_feed_items(url, source_name, max_items=25):
    """ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
    try:
        print(f"ğŸ“¡ å–å¾—ä¸­: {source_name} ({url})")
        
        # User-Agentã‚’è¨­å®š
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; AI-News-Bot/1.0)'
        }
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å–å¾—
        import urllib.request
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=30) as response:
            feed_data = response.read()
        
        # feedparserã§è§£æ
        feed = feedparser.parse(feed_data)
        items = []
        
        for entry in feed.entries[:max_items]:
            # æœ€è¿‘ã®è¨˜äº‹ã®ã¿
            if not is_recent(entry.get('published_parsed'), HOURS_LOOKBACK):
                continue
            
            item = {
                'title': entry.get('title', '').strip(),
                'url': entry.get('link', ''),
                'summary': entry.get('summary', entry.get('description', '')),
                'published': entry.get('published', ''),
                'source': source_name,
                'engineer_score': 0.0
            }
            
            # HTMLã‚¿ã‚°ã‚’é™¤å»
            item['summary'] = re.sub(r'<[^>]+>', '', item['summary'])
            item['summary'] = html.unescape(item['summary']).strip()
            
            # ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢é–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            item['engineer_score'] = SimpleEngineerRanking.calculate_score(item)
            
            items.append(item)
        
        print(f"âœ… {source_name}: {len(items)}ä»¶å–å¾—")
        return items
        
    except Exception as e:
        print(f"âŒ {source_name} ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def _clean_tweet_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r"https?://\S+", "", text)  # URLså‰Šé™¤
    text = re.sub(r"\s+", " ", text).strip()  # ä½™åˆ†ãªç©ºç™½ã‚’åœ§ç¸®
    text = re.sub(r"(#[\wä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³ãƒ¼]+\s*)+$", "", text)  # æœ«å°¾ã®ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ç¾¤ã‚’å‰Šé™¤
    text = re.sub(r"(@[\w_]+\s*)+$", "", text)  # æœ«å°¾ã®ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ç¾¤ã‚’å‰Šé™¤
    return text.strip()

def _extract_external_url(text: str) -> str | None:
    if not text:
        return None
    urls = re.findall(r"https?://\S+", text)
    for u in urls:
        try:
            host = urlparse(u).netloc.lower()
            if any(x in host for x in ["x.com", "twitter.com", "t.co"]):
                continue
            return u
        except Exception:
            continue
    return None

def _fetch_og_title(url: str, timeout: int = 8) -> str | None:
    if not url:
        return None
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (compatible; AI-News-Bot/1.0)'}
        resp = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        if resp.status_code != 200:
            return None
        soup = BeautifulSoup(resp.text, 'html.parser')
        tag = soup.find('meta', attrs={'property': 'og:title'})
        if tag and tag.get('content'):
            return tag.get('content').strip()
        if soup.title and soup.title.string:
            return soup.title.string.strip()
    except Exception:
        return None
    return None

def _username_from_status_url(x_status_url: str) -> str | None:
    try:
        p = urlparse(x_status_url)
        parts = [seg for seg in p.path.split('/') if seg]
        if len(parts) >= 2 and parts[0].lower() not in ("i",):
            return parts[0]
    except Exception:
        return None
    return None

def _guess_tag(text: str) -> str | None:
    t = (text or '').lower()
    jp = (text or '')
    # å®Ÿè£…/ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ç³»
    if any(k in t for k in ['how to', 'tutorial', 'guide', 'step-by-step']) or any(k in jp for k in ['ä½¿ã„æ–¹', 'æ‰‹é †', 'å…¥é–€', 'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«']):
        return 'å®Ÿè£…'
    # æ¥­å‹™åŠ¹ç‡åŒ–/è‡ªå‹•åŒ–ç³»
    if any(k in t for k in ['workflow', 'automation', 'automate', 'copilot', 'zapier', 'notion', 'excel', 'apps script', 'power automate', 'prompt']) or any(k in jp for k in ['åŠ¹ç‡åŒ–', 'è‡ªå‹•åŒ–', 'æ™‚çŸ­']):
        return 'åŠ¹ç‡åŒ–'
    # ç ”ç©¶/è«–æ–‡ç³»
    if any(k in t for k in ['arxiv', 'paper', 'research']) or any(k in jp for k in ['è«–æ–‡', 'ç ”ç©¶']):
        return 'ç ”ç©¶'
    # ãƒªãƒªãƒ¼ã‚¹/ç™ºè¡¨
    if any(k in t for k in ['release', 'launch', 'announce', 'announced']) or any(k in jp for k in ['ç™ºè¡¨', 'ãƒªãƒªãƒ¼ã‚¹']):
        return 'ç™ºè¡¨'
    return None

def _build_readable_summary(cleaned: str, og_title: str | None, domain: str | None) -> str:
    tag = _guess_tag((og_title or '') + ' ' + (cleaned or ''))
    parts = []
    if tag:
        parts.append(f"[{tag}]")
    if og_title:
        parts.append(og_title.strip())
    # æŠ•ç¨¿è¦ç´„ã¯é‡è¤‡ã—ãªã„ã¨ãã®ã¿æ·»ãˆã‚‹
    if cleaned:
        if not og_title or og_title.lower() not in cleaned.lower():
            # çŸ­ãæ•´å½¢
            brief = cleaned.strip()
            if len(brief) > 140:
                brief = brief[:140] + '...'
            parts.append(f"æŠ•ç¨¿è¦ç´„: {brief}")
    if domain:
        parts.append(f"å‡ºå…¸: {domain}")
    # ä»•ä¸Šã’ï¼ˆå…¨è§’åŒºåˆ‡ã‚Šã§è¦–èªæ€§å‘ä¸Šï¼‰
    summary = ' ï½œ '.join(p for p in parts if p)
    # æœ€çµ‚é•·ã•ä¸Šé™
    if len(summary) > 280:
        summary = summary[:277] + '...'
    return summary

def fetch_x_posts():
    """X(Twitter)æŠ•ç¨¿ã‚’å–å¾—"""
    try:
        print(f"ğŸ“± XæŠ•ç¨¿å–å¾—ä¸­: {X_POSTS_CSV}")
        
        response = requests.get(X_POSTS_CSV, timeout=30)
        print(f"ğŸŒ HTTP Response: {response.status_code}")
        if response.status_code != 200:
            print(f"âŒ HTTP Status: {response.status_code}")
            return []
        
        content = response.text.strip()
        print(f"ğŸ“„ å—ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(content)} æ–‡å­—")
        print(f"ğŸ“„ ãƒ‡ãƒ¼ã‚¿å…ˆé ­100æ–‡å­—: {content[:100]}")
        
        # CSVã‹ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚’åˆ¤å®š
        if content.startswith('"Timestamp"') or ',' in content[:200]:
            print("ğŸ“‹ CSVå½¢å¼ã¨ã—ã¦å‡¦ç†ä¸­...")
            return fetch_x_posts_from_csv(content)
        else:
            print("ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã¨ã—ã¦å‡¦ç†ä¸­...")
            return fetch_x_posts_from_text(content)
            
    except Exception as e:
        print(f"âŒ XæŠ•ç¨¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []

def fetch_x_posts_from_csv(csv_content):
    """CSVå½¢å¼ã®Xãƒã‚¹ãƒˆã‚’å‡¦ç†"""
    try:
        reader = csv.DictReader(io.StringIO(csv_content))
        
        posts = []
        og_cache: dict[str, str] = {}
        for row in reader:
            tweet_content = (row.get('Tweet Content', '') or '').strip()
            username = (row.get('Username', '') or '').strip()
            timestamp_str = (row.get('Timestamp', '') or '').strip()
            if not tweet_content:
                continue
            try:
                from dateutil import parser
                post_date = parser.parse(timestamp_str)
                if not is_recent(post_date.strftime('%Y-%m-%d %H:%M:%S'), HOURS_LOOKBACK):
                    continue
            except Exception as e:
                print(f"âš ï¸ æ—¥ä»˜è§£æã‚¨ãƒ©ãƒ¼: {timestamp_str} - {e}")
                continue

            cleaned = _clean_tweet_text(tweet_content)
            ext_url = row.get('Source Link 1', '').strip() or row.get('Source Link 2', '').strip()
            if not ext_url:
                ext_url = _extract_external_url(tweet_content)

            domain = urlparse(ext_url).netloc if ext_url else ''
            og_title = None
            if ext_url:
                og_title = og_cache.get(ext_url)
                if og_title is None:
                    og_title = _fetch_og_title(ext_url)
                    if og_title:
                        og_cache[ext_url] = og_title

            if og_title:
                title = f"{og_title}ï¼ˆ{domain}ï¼‰"
            else:
                title = cleaned if len(cleaned) <= 100 else (cleaned[:100] + '...')

            summary = _build_readable_summary(cleaned, og_title, domain)

            source_label = f"X @{username}" if username else "X (Twitter)"
            score_payload = {'title': title, 'summary': summary or cleaned, 'url': ext_url or ''}

            posts.append({
                'title': title,
                'url': ext_url or '',
                'summary': summary or cleaned,
                'published': timestamp_str,
                'source': source_label,
                'engineer_score': SimpleEngineerRanking.calculate_score(score_payload)
            })
        
        print(f"âœ… CSVå½¢å¼XæŠ•ç¨¿: {len(posts)}ä»¶å–å¾—")
        return posts[:MAX_ITEMS_PER_CATEGORY]
        
    except Exception as e:
        print(f"âŒ CSVå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def fetch_x_posts_from_text(text_content):
    """ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®Xãƒã‚¹ãƒˆã‚’å‡¦ç†"""
    try:
        import re
        
        # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒã‚¹ãƒˆã‚’åˆ†å‰²
        posts = []
        
        # August XX, 2025 å½¢å¼ã®æ—¥ä»˜ã‚’æ¤œç´¢
        date_pattern = r'(August \d{1,2}, 2025 at \d{1,2}:\d{2}[AP]M)'
        username_pattern = r'@([a-zA-Z0-9_]+)'
        url_pattern = r'https?://[^\s,"]+'
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡Œã§åˆ†å‰²ã—ã¦å‡¦ç†
        lines = text_content.split('\n')
        current_post = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ—¥ä»˜ã‚’æ¤œå‡º
            date_match = re.search(date_pattern, line)
            if date_match:
                # å‰ã®ãƒã‚¹ãƒˆã‚’ä¿å­˜
                if current_post.get('content'):
                    posts.append(current_post.copy())
                
                # æ–°ã—ã„ãƒã‚¹ãƒˆã‚’é–‹å§‹
                current_post = {
                    'timestamp': date_match.group(1),
                    'content': '',
                    'urls': [],
                    'username': ''
                }
                continue
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æ¤œå‡º
            username_match = re.search(username_pattern, line)
            if username_match and not current_post.get('username'):
                current_post['username'] = username_match.group(1)
            
            # URLã‚’æ¤œå‡º
            url_matches = re.findall(url_pattern, line)
            for url in url_matches:
                if url not in current_post['urls']:
                    current_post['urls'].append(url)
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è“„ç©
            if not re.search(date_pattern, line):  # æ—¥ä»˜è¡Œä»¥å¤–
                if current_post.get('content'):
                    current_post['content'] += ' ' + line
                else:
                    current_post['content'] = line
        
        # æœ€å¾Œã®ãƒã‚¹ãƒˆã‚’è¿½åŠ 
        if current_post.get('content'):
            posts.append(current_post)
        
        # ãƒã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        converted_posts = []
        og_cache: dict[str, str] = {}
        for post_data in posts[:MAX_ITEMS_PER_CATEGORY]:
            if not post_data.get('content'):
                continue
            
            # æ—¥ä»˜ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€è¿‘48æ™‚é–“ä»¥å†…ï¼‰
            try:
                from dateutil import parser
                post_date = parser.parse(post_data['timestamp'])
                if not is_recent(post_date.strftime('%Y-%m-%d %H:%M:%S'), HOURS_LOOKBACK):
                    continue
            except:
                continue
            
            cleaned = _clean_tweet_text(post_data['content'])
            ext_url = None
            for u in post_data.get('urls', []):
                host = urlparse(u).netloc.lower()
                if not any(x in host for x in ["x.com", "twitter.com", "t.co"]):
                    ext_url = u
                    break
            domain = urlparse(ext_url).netloc if ext_url else ''
            og_title = None
            if ext_url:
                og_title = og_cache.get(ext_url)
                if og_title is None:
                    og_title = _fetch_og_title(ext_url)
                    if og_title:
                        og_cache[ext_url] = og_title

            if og_title:
                title = f"{og_title}ï¼ˆ{domain}ï¼‰"
            else:
                title = cleaned if len(cleaned) <= 100 else (cleaned[:100] + '...')

            summary = _build_readable_summary(cleaned, og_title, domain)

            source_label = f"X @{post_data.get('username', 'unknown')}"
            score_payload = {'title': title, 'summary': summary or cleaned, 'url': ext_url or ''}
            converted_posts.append({
                'title': title,
                'url': ext_url or '',
                'summary': summary or cleaned,
                'published': post_data['timestamp'],
                'source': source_label,
                'engineer_score': SimpleEngineerRanking.calculate_score(score_payload)
            })
        
        print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼XæŠ•ç¨¿: {len(converted_posts)}ä»¶å–å¾—")
        return converted_posts
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []

def format_time_ago(published_str):
    """çµŒéæ™‚é–“ã‚’æ—¥æœ¬èªã§è¡¨ç¤º"""
    if not published_str:
        return ""
    
    try:
        from dateutil import parser
        pub_time = parser.parse(published_str)
        now = datetime.now(timezone.utc)
        
        if pub_time.tzinfo is None:
            pub_time = pub_time.replace(tzinfo=timezone.utc)
        
        diff = now - pub_time
        hours = diff.total_seconds() / 3600
        
        if hours < 1:
            return "1æ™‚é–“æœªæº€"
        elif hours < 24:
            return f"{int(hours)}æ™‚é–“å‰"
        else:
            return f"{int(hours // 24)}æ—¥å‰"
    except:
        return ""

def generate_css():
    """CSSãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
    css_content = '''/* Digital.gov compliance deployed at 2025-08-23 */
:root{
  /* Digital.govæº–æ‹ : WCAG AAAå¯¾å¿œã‚«ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ  */
  --fg:#0f172a; --bg:#ffffff; --muted:#475569; --line:#e2e8f0;
  --brand:#1e40af; --brand-weak:#f1f5f9; --chip:#f8fafc;
  --brand-hover:#1e3a8a; --brand-light:#bfdbfe; --brand-dark:#1e3a8a;
  --success:#15803d; --warning:#ca8a04; --danger:#dc2626;
  --info:#0369a1; --neutral:#64748b;
  
  /* æ®µéšçš„èƒŒæ™¯è‰²ï¼ˆå½©åº¦ã‚’ä¸‹ã’ãŸèƒŒæ™¯ï¼‰ */
  --bg-subtle:#f8fafc; --bg-muted:#f1f5f9; --bg-emphasis:#e2e8f0;
  
  /* ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå½©åº¦èª¿æ•´æ¸ˆã¿ï¼‰ */
  --gradient:linear-gradient(135deg, #1e40af 0%, #1e3a8a 100%);
  --gradient-subtle:linear-gradient(135deg, #f1f5f9 0%, #e2e8f0 100%);
  
  /* ã‚·ãƒ£ãƒ‰ã‚¦ */
  --shadow:0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
  --shadow-lg:0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
  
  /* ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå¤‰æ•° */
  --border-radius: 12px;
  --spacing-xs: 4px; --spacing-sm: 8px; --spacing-md: 16px; 
  --spacing-lg: 24px; --spacing-xl: 32px; --spacing-2xl: 48px;
  
  /* ã‚¿ã‚¤ãƒã‚°ãƒ©ãƒ•ã‚£ */
  --font-size-xs: 12px; --font-size-sm: 14px; --font-size-base: 16px;
  --font-size-lg: 18px; --font-size-xl: 20px; --font-size-2xl: 24px;
  --font-size-3xl: 32px; --font-size-4xl: 36px;
  
  /* ãƒ•ã‚©ãƒ¼ã‚«ã‚¹è¡¨ç¤º */
  --focus-ring: 0 0 0 3px rgba(59, 130, 246, 0.5);
  --focus-ring-offset: 2px;
}
*{box-sizing:border-box}
body{
  margin:0;
  background:var(--bg);
  color:var(--fg);
  font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,'Noto Sans JP',sans-serif;
  font-size:var(--font-size-base);
  line-height:1.6;
  scroll-behavior:smooth;
}
.container{
  max-width:1000px;
  margin:0 auto;
  padding:var(--spacing-lg) var(--spacing-md);
  display:flex;
  flex-direction:column;
  gap:var(--spacing-lg);
}

/* ãƒ˜ãƒƒãƒ€ãƒ¼ */
.site-header{
  display:flex;
  justify-content:space-between;
  align-items:center;
  padding:var(--spacing-md);
  background:var(--bg-subtle);
  border-bottom:1px solid var(--line);
  margin-bottom:var(--spacing-lg);
}
.brand{
  font-size:var(--font-size-xl);
  font-weight:700;
  color:var(--brand);
}
.updated{
  color:var(--muted);
  font-size:var(--font-size-sm);
}

/* ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ */
.page-title{
  font-size:var(--font-size-3xl);
  font-weight:800;
  text-align:center;
  margin:0 0 var(--spacing-md);
  background:var(--gradient);
  -webkit-background-clip:text;
  -webkit-text-fill-color:transparent;
  background-clip:text;
}
.lead{
  text-align:center;
  color:var(--muted);
  font-size:var(--font-size-lg);
  margin:0 0 var(--spacing-xl);
  max-width:600px;
  margin-left:auto;
  margin-right:auto;
}

/* KPIã‚°ãƒªãƒƒãƒ‰ */
.kpi-grid{
  display:grid;
  grid-template-columns:repeat(auto-fit,minmax(150px,1fr));
  gap:var(--spacing-md);
  margin-bottom:var(--spacing-xl);
}
.kpi-card{
  background:var(--bg-subtle);
  padding:var(--spacing-md);
  border-radius:var(--border-radius);
  text-align:center;
  border:1px solid var(--line);
}
.kpi-value{
  font-size:var(--font-size-2xl);
  font-weight:800;
  color:var(--brand);
}
.kpi-label{
  font-size:var(--font-size-sm);
  color:var(--muted);
  margin-top:var(--spacing-xs);
}
.kpi-note{
  font-size:var(--font-size-xs);
  color:var(--success);
  margin-top:var(--spacing-xs);
}

/* ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ« */
.filters{
  display:flex;
  flex-wrap:wrap;
  gap:var(--spacing-sm);
  align-items:center;
  padding:var(--spacing-md);
  background:var(--bg-subtle);
  border-radius:var(--border-radius);
  margin-bottom:var(--spacing-lg);
}
.filter-group{
  display:flex;
  align-items:center;
  gap:var(--spacing-sm);
}
.filter-label{
  font-size:var(--font-size-sm);
  color:var(--muted);
  font-weight:600;
}
.filter-select, .filter-input{
  padding:var(--spacing-xs) var(--spacing-sm);
  border:1px solid var(--line);
  border-radius:calc(var(--border-radius) / 2);
  font-size:var(--font-size-sm);
  background:var(--bg);
}
.filter-select:focus, .filter-input:focus{
  outline:none;
  border-color:var(--brand);
  box-shadow:var(--focus-ring);
}

/* ã‚¿ãƒ–ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ */
.tabs{
  display:flex;
  border-bottom:2px solid var(--line);
  margin-bottom:var(--spacing-lg);
  overflow-x:auto;
}
.tab-button{
  background:none;
  border:none;
  padding:var(--spacing-md) var(--spacing-lg);
  font-size:var(--font-size-base);
  font-weight:600;
  color:var(--muted);
  cursor:pointer;
  border-bottom:3px solid transparent;
  white-space:nowrap;
  transition:all 0.2s;
}
.tab-button:hover{
  color:var(--fg);
  background:var(--bg-subtle);
}
.tab-button.active{
  color:var(--brand);
  border-bottom-color:var(--brand);
  background:var(--brand-weak);
}
.tab-button:focus{
  outline:none;
  box-shadow:var(--focus-ring);
}

/* ã‚¿ãƒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ */
.tab-content{
  display:grid;
  grid-template-columns:repeat(auto-fit,minmax(300px,1fr));
  gap:var(--spacing-md);
}
.tab-panel{
  transition:opacity 0.3s ease;
}
.tab-panel.hidden{
  display:none;
}

/* ã‚«ãƒ¼ãƒ‰ */
.enhanced-card{
  background:var(--bg);
  border:1px solid var(--line);
  border-radius:var(--border-radius);
  padding:var(--spacing-md);
  box-shadow:var(--shadow);
  transition:all 0.3s ease;
  position:relative;
}
.enhanced-card:hover{
  transform:translateY(-2px);
  box-shadow:var(--shadow-lg);
  border-color:var(--brand-light);
}
.card-priority{
  position:absolute;
  top:var(--spacing-sm);
  right:var(--spacing-sm);
  background:var(--brand);
  color:white;
  padding:var(--spacing-xs) var(--spacing-sm);
  border-radius:calc(var(--border-radius) / 2);
  font-size:var(--font-size-xs);
  font-weight:600;
}
.card-priority.high{background:var(--success)}
.card-priority.medium{background:var(--warning)}
.card-priority.low{background:var(--neutral)}
.card-header{
  display:flex;
  justify-content:space-between;
  align-items:flex-start;
  margin-bottom:var(--spacing-sm);
}
.card-title{
  font-size:var(--font-size-lg);
  font-weight:700;
  line-height:1.3;
  margin:0;
}
.card-title a{
  color:var(--fg);
  text-decoration:none;
}
.card-title a:hover{
  color:var(--brand);
  text-decoration:underline;
}
.card-meta{
  display:flex;
  gap:var(--spacing-sm);
  font-size:var(--font-size-xs);
  color:var(--muted);
  margin-bottom:var(--spacing-sm);
}
.card-source{
  background:var(--chip);
  padding:var(--spacing-xs) var(--spacing-sm);
  border-radius:calc(var(--border-radius) / 3);
  font-weight:600;
}
.card-summary{
  color:var(--fg);
  line-height:1.5;
  margin:var(--spacing-sm) 0;
}
.card-footer{
  display:flex;
  justify-content:space-between;
  align-items:center;
  margin-top:var(--spacing-sm);
  padding-top:var(--spacing-sm);
  border-top:1px solid var(--line);
}
.card-score{
  font-size:var(--font-size-sm);
  font-weight:600;
  color:var(--info);
}
.card-time{
  font-size:var(--font-size-xs);
  color:var(--muted);
}

/* ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ– */
@media (max-width: 768px) {
  .container{
    padding:var(--spacing-md) var(--spacing-sm);
  }
  .page-title{
    font-size:var(--font-size-2xl);
  }
  .lead{
    font-size:var(--font-size-base);
  }
  .filters{
    flex-direction:column;
    align-items:stretch;
  }
  .filter-group{
    justify-content:space-between;
  }
  .tabs{
    justify-content:space-around;
  }
  .tab-button{
    flex:1;
    padding:var(--spacing-sm);
    font-size:var(--font-size-sm);
  }
  .tab-content{
    grid-template-columns:1fr;
  }
}

/* ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ */
@media (prefers-reduced-motion: reduce) {
  *, *::before, *::after {
    animation-duration: 0.01ms !important;
    animation-iteration-count: 1 !important;
    transition-duration: 0.01ms !important;
    scroll-behavior: auto !important;
  }
}

/* ãƒã‚¤ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å¯¾å¿œ */
@media (prefers-contrast: high) {
  :root {
    --line: #000000;
    --muted: #000000;
  }
  .enhanced-card {
    border-width: 2px;
  }
}

/* ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œ */
@media (prefers-color-scheme: dark) {
  :root {
    --fg: #f1f5f9;
    --bg: #0f172a;
    --muted: #94a3b8;
    --line: #334155;
    --bg-subtle: #1e293b;
    --bg-muted: #334155;
    --chip: #1e293b;
  }
}
'''
    return css_content

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ Simple Enhanced Daily AI News Builder")
    print("=" * 50)
    
    # JSTæ™‚é–“ã‚’å–å¾—
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst).strftime('%Y-%m-%d %H:%M JST')
    
    print(f"ğŸ“… ç”Ÿæˆæ—¥æ™‚: {now}")
    print(f"â° éå» {HOURS_LOOKBACK} æ™‚é–“ã®è¨˜äº‹ã‚’åé›†")
    print(f"ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥æœ€å¤§ {MAX_ITEMS_PER_CATEGORY} ä»¶")
    print(f"ğŸŒ ç¿»è¨³: {'æœ‰åŠ¹' if TRANSLATE_TO_JA else 'ç„¡åŠ¹'}")
    
    # ç¿»è¨³ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
    translation_cache = load_translation_cache()
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šèª­ã¿è¾¼ã¿
    feeds_config = load_feeds_config()
    
    # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿åé›†
    all_categories = {}
    
    for category, feeds in feeds_config.items():
        print(f"\nğŸ“‚ {category.upper()} ã‚«ãƒ†ã‚´ãƒªå‡¦ç†ä¸­...")
        
        category_items = []
        for feed_config in feeds:
            items = fetch_feed_items(
                feed_config['url'], 
                feed_config['name'], 
                MAX_ITEMS_PER_CATEGORY
            )
            category_items.extend(items)
        
        # XæŠ•ç¨¿ã‚‚è¿½åŠ ï¼ˆpostsã‚«ãƒ†ã‚´ãƒªã®ã¿ï¼‰
        if category == 'posts':
            print(f"ğŸ” DEBUG: postsã‚«ãƒ†ã‚´ãƒªã§XæŠ•ç¨¿å–å¾—é–‹å§‹...")
            print(f"ğŸ” DEBUG: X_POSTS_CSVç’°å¢ƒå¤‰æ•° = {X_POSTS_CSV}")
            print(f"ğŸ” DEBUG: HOURS_LOOKBACK = {HOURS_LOOKBACK}")
            
            x_items = fetch_x_posts()
            print(f"ğŸ” DEBUG: XæŠ•ç¨¿å–å¾—å®Œäº† - {len(x_items)}ä»¶")
            
            if x_items:
                # Xãƒã‚¹ãƒˆã®ã‚¹ã‚³ã‚¢ã‚’å¼·åˆ¶çš„ã«é«˜ãã—ã¦å„ªå…ˆè¡¨ç¤º
                for i, item in enumerate(x_items):
                    item['engineer_score'] = 10.0  # æœ€é«˜ã‚¹ã‚³ã‚¢è¨­å®š
                    print(f"ğŸ” DEBUG: Xãƒã‚¹ãƒˆ[{i+1}] - ã‚¿ã‚¤ãƒˆãƒ«: {item['title'][:50]}... (ã‚¹ã‚³ã‚¢: {item['engineer_score']})")
                    print(f"ğŸ” DEBUG: Xãƒã‚¹ãƒˆ[{i+1}] - URL: {item.get('url', 'N/A')}")
                
                # Xãƒã‚¹ãƒˆã‚’ category_items ã«è¿½åŠ 
                category_items.extend(x_items)
                print(f"ğŸ” DEBUG: Xãƒã‚¹ãƒˆçµ±åˆå¾Œã®ç·è¨˜äº‹æ•°: {len(category_items)}ä»¶")
            else:
                print(f"âš ï¸ DEBUG: XæŠ•ç¨¿ãŒå–å¾—ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ - åŸå› èª¿æŸ»ãŒå¿…è¦")
        
        # ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢é–¢é€£åº¦ã§ã‚½ãƒ¼ãƒˆ
        category_items.sort(key=lambda x: x['engineer_score'], reverse=True)
        category_items = category_items[:MAX_ITEMS_PER_CATEGORY]
        
        # ç¿»è¨³å‡¦ç†
        if TRANSLATE_TO_JA:
            print(f"ğŸŒ {category} ã‚«ãƒ†ã‚´ãƒªç¿»è¨³ä¸­...")
            for item in category_items:
                if item['title'] and not all(ord(c) < 128 for c in item['title']):
                    # ã™ã§ã«æ—¥æœ¬èªã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
                    
                item['title_ja'] = translate_text(item['title'], 'ja', translation_cache)
                if item['summary']:
                    item['summary_ja'] = translate_text(item['summary'], 'ja', translation_cache)
        
        all_categories[category.lower()] = category_items
        print(f"âœ… {category}: {len(category_items)}ä»¶ (å¹³å‡ã‚¹ã‚³ã‚¢: {sum(item['engineer_score'] for item in category_items) / len(category_items):.1f})")
        print(f"   â†’ all_categories['{category.lower()}'] ã«ä¿å­˜")
    
    # ç¿»è¨³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    if TRANSLATE_TO_JA:
        save_translation_cache(translation_cache)
        print("ğŸ’¾ ç¿»è¨³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†")
    
    # çµ±è¨ˆæƒ…å ±
    total_items = sum(len(items) for items in all_categories.values())
    high_priority = sum(1 for items in all_categories.values() for item in items if item['engineer_score'] >= 7.0)
    
    print(f"\nğŸ“Š åé›†çµæœ:")
    print(f"   ç·è¨˜äº‹æ•°: {total_items}ä»¶")
    print(f"   é«˜å„ªå…ˆåº¦: {high_priority}ä»¶")
    print(f"   æƒ…å ±æº: {sum(len(feeds) for feeds in feeds_config.values())}å€‹")
    
    # Top Picksï¼ˆå…¨ã‚«ãƒ†ã‚´ãƒªæ¨ªæ–­ã®ä¸Šä½ï¼‰
    all_items_flat = [it for items in all_categories.values() for it in items]
    # URLé‡è¤‡é™¤å»ï¼ˆå…ˆã«é«˜ã‚¹ã‚³ã‚¢ã«ä¸¦ã¹ã¦ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰
    all_items_flat.sort(key=lambda x: x['engineer_score'], reverse=True)
    seen = set()
    top_picks = []
    for it in all_items_flat:
        u = it.get('url')
        if u and u not in seen:
            top_picks.append(it)
            seen.add(u)
        if len(top_picks) >= TOP_PICKS_COUNT:
            break

    # HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    html_template = f'''<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Daily AI News â€” {now}</title>
  <link rel="stylesheet" href="style.css"/>
</head>
<body>
  <header class="site-header">
    <div class="brand">ğŸ“° Daily AI News</div>
    <div class="updated">æœ€çµ‚æ›´æ–°ï¼š{now}</div>
  </header>

  <main class="container">
    <h1 class="page-title">ä»Šæ—¥ã®æœ€æ–°AIæƒ…å ±</h1>
    <p class="lead">
        æœ‰ç”¨åº¦ã‚¹ã‚³ã‚¢ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤ºï¼ˆAIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢/æ¥­å‹™åŠ¹ç‡åŒ–å‘ã‘ï¼‰ã€‚å®Ÿè£…å¯èƒ½æ€§ãƒ»åŠ¹ç‡åŒ–åŠ¹æœãƒ»å­¦ç¿’ä¾¡å€¤ã‚’é‡è¦–ã—ã¦è‡ªå‹•ã‚½ãƒ¼ãƒˆã€‚
        è±Šå¯Œãªæƒ…å ±é‡ï¼ˆ{total_items}ä»¶ï¼‰ã‚’ç¶­æŒã—ã¤ã¤ã€é‡è¦åº¦ã§æ•´ç†è¡¨ç¤ºã€‚
    </p>

    <section class="kpi-grid">
      <div class="kpi-card">
        <div class="kpi-value">{total_items}ä»¶</div>
        <div class="kpi-label">ç·è¨˜äº‹æ•°</div>
        <div class="kpi-note">æƒ…å ±é‡ç¶­æŒ</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{high_priority}ä»¶</div>
        <div class="kpi-label">é«˜å„ªå…ˆåº¦è¨˜äº‹</div>
        <div class="kpi-note">ã‚¹ã‚³ã‚¢7.0+</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{sum(len(feeds) for feeds in feeds_config.values())}å€‹</div>
        <div class="kpi-label">æƒ…å ±æº</div>
        <div class="kpi-note">å¤šè§’çš„åé›†</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{HOURS_LOOKBACK}h</div>
        <div class="kpi-label">åé›†ç¯„å›²</div>
        <div class="kpi-note">æœ€æ–°æ€§é‡è¦–</div>
      </div>
    </section>

    <!-- Top Picks: æœ‰ç”¨åº¦ä¸Šä½ -->
    <section class="top-picks" aria-label="Top Picks">
      <h2 class="section-title">ğŸ† Top Picks â€” æœ‰ç”¨åº¦ä¸Šä½ï¼ˆä¸Šä½ {min(TOP_PICKS_COUNT, len(top_picks))} ä»¶ï¼‰</h2>
      <div class="tab-content">
'''

    for item in top_picks:
        score = item['engineer_score']
        if score >= 7.0:
            priority = 'high'; priority_text = 'é«˜'
        elif score >= 4.0:
            priority = 'medium'; priority_text = 'ä¸­'
        else:
            priority = 'low'; priority_text = 'ä½'

        display_title = item.get('title_ja', item['title'])
        display_summary = item.get('summary_ja', item['summary'])
        time_ago = format_time_ago(item['published'])

        html_template += f'''
        <article class="enhanced-card" data-score="{score:.1f}" data-source="{item['source']}" data-time="{item['published']}">
          <div class="card-priority {priority}">{priority_text} {score:.1f}</div>
          <header class="card-header">
            <h3 class="card-title">
              <a href="{item['url']}" target="_blank" rel="noopener">{html.escape(display_title)}</a>
            </h3>
          </header>
          <div class="card-meta">
            <span class="card-source">{item['source']}</span>
            {f'<span class="card-time">{time_ago}</span>' if time_ago else ''}
          </div>
          <div class="card-summary">{html.escape(display_summary[:200] + '...' if len(display_summary) > 200 else display_summary)}</div>
          <footer class="card-footer">
            <span class="card-score">æœ‰ç”¨åº¦: {score:.1f}</span>
            <span class="card-time">{time_ago}</span>
          </footer>
        </article>
'''

    html_template += '''
      </div>
    </section>

    <section class="filters">
      <div class="filter-group">
        <label class="filter-label">æ¤œç´¢:</label>
        <input type="text" id="searchInput" class="filter-input" placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢..."/>
      </div>
      <div class="filter-group">
        <label class="filter-label">é‡è¦åº¦:</label>
        <select id="importanceFilter" class="filter-select">
          <option value="all">ã™ã¹ã¦</option>
          <option value="high">é«˜ (7.0+)</option>
          <option value="medium">ä¸­ (4.0-6.9)</option>
          <option value="low">ä½ (0-3.9)</option>
        </select>
      </div>
      <div class="filter-group">
        <label class="filter-label">ä¸¦ã³é †:</label>
        <select id="sortBy" class="filter-select">
          <option value="score">é‡è¦åº¦é †</option>
          <option value="time">æ–°ç€é †</option>
          <option value="source">æƒ…å ±æºé †</option>
        </select>
      </div>
    </section>

    <nav class="tabs">
      <button class="tab-button active" data-tab="business">
        ğŸ“ˆ Business ({len(all_categories.get('business', []))})
      </button>
      <button class="tab-button" data-tab="tools">
        ğŸ”§ Tools ({len(all_categories.get('tools', []))})
      </button>
      <button class="tab-button" data-tab="posts">
        ğŸ’¬ Posts ({len(all_categories.get('posts', []))})
      </button>
    </nav>
'''
    
    # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆï¼ˆbusinessã‚’æœ€åˆã«ç¢ºå®Ÿã«è¡¨ç¤ºï¼‰
    category_order = ['business', 'tools', 'posts']
    for category_name in category_order:
        # ã‚«ãƒ†ã‚´ãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†
        items = all_categories.get(category_name, [])
        print(f"DEBUG: {category_name} ã‚«ãƒ†ã‚´ãƒª - {len(items)}ä»¶ã®è¨˜äº‹")
        is_active = category_name == 'business'
        panel_class = 'tab-panel' if is_active else 'tab-panel hidden'
        
        html_template += f'''
    <section class="{panel_class}" data-category="{category_name.lower()}">
      <div class="tab-content">
'''
        
        for item in items:
            # å„ªå…ˆåº¦ãƒ©ãƒ™ãƒ«
            score = item['engineer_score']
            if score >= 7.0:
                priority = 'high'
                priority_text = 'é«˜'
            elif score >= 4.0:
                priority = 'medium' 
                priority_text = 'ä¸­'
            else:
                priority = 'low'
                priority_text = 'ä½'
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ï¼ˆç¿»è¨³ç‰ˆãŒã‚ã‚Œã°ä½¿ç”¨ï¼‰
            display_title = item.get('title_ja', item['title'])
            display_summary = item.get('summary_ja', item['summary'])
            
            # æ™‚é–“è¡¨ç¤º
            time_ago = format_time_ago(item['published'])
            
            html_template += f'''
        <article class="enhanced-card" data-score="{score:.1f}" data-source="{item['source']}" data-time="{item['published']}">
          <div class="card-priority {priority}">{priority_text} {score:.1f}</div>
          <header class="card-header">
            <h3 class="card-title">
              <a href="{item['url']}" target="_blank" rel="noopener">{html.escape(display_title)}</a>
            </h3>
          </header>
          <div class="card-meta">
            <span class="card-source">{item['source']}</span>
            {f'<span class="card-time">{time_ago}</span>' if time_ago else ''}
          </div>
          <div class="card-summary">{html.escape(display_summary[:200] + '...' if len(display_summary) > 200 else display_summary)}</div>
          <footer class="card-footer">
            <span class="card-score">æœ‰ç”¨åº¦: {score:.1f}</span>
            <span class="card-time">{time_ago}</span>
          </footer>
        </article>
'''
        
        html_template += '''
      </div>
    </section>
'''
    
    # JavaScriptè¿½åŠ 
    html_template += '''
  </main>

<script>
// ã‚¿ãƒ–åˆ‡ã‚Šæ›¿ãˆæ©Ÿèƒ½
class TabController {
  constructor() {
    this.activeTab = 'business';
    this.init();
  }
  
  init() {
    // ã‚¿ãƒ–ãƒœã‚¿ãƒ³ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼
    document.querySelectorAll('.tab-button').forEach(button => {
      button.addEventListener('click', (e) => {
        const tab = e.target.dataset.tab;
        this.switchTab(tab);
      });
    });
    
    // ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½
    this.setupFilters();
    
    // åˆæœŸè¡¨ç¤ºï¼šbusinessã‚¿ãƒ–ã‚’æ˜ç¤ºçš„ã«è¡¨ç¤º
    this.switchTab('business');
  }
  
  switchTab(tabName) {
    if (this.activeTab === tabName) return;
    
    // ãƒœã‚¿ãƒ³ã®çŠ¶æ…‹æ›´æ–°
    document.querySelectorAll('.tab-button').forEach(btn => {
      btn.classList.remove('active');
    });
    document.querySelector(`[data-tab="${tabName}"]`).classList.add('active');
    
    // ãƒ‘ãƒãƒ«ã®è¡¨ç¤ºåˆ‡ã‚Šæ›¿ãˆï¼ˆhidden classä½¿ç”¨ï¼‰
    document.querySelectorAll('.tab-panel').forEach(panel => {
      panel.classList.add('hidden');
    });
    document.querySelector(`[data-category="${tabName}"]`).classList.remove('hidden');
    
    this.activeTab = tabName;
    this.updateTabCounts(); // ã‚¿ãƒ–ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
    this.applyFilters(); // ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å†é©ç”¨
  }
  
  updateTabCounts() {
    // å„ã‚¿ãƒ–ã®è¨˜äº‹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¦è¡¨ç¤ºæ›´æ–°
    const tabs = ['business', 'tools', 'posts'];
    const tabLabels = {
      'business': 'ğŸ“ˆ Business',
      'tools': 'ğŸ”§ Tools', 
      'posts': 'ğŸ’¬ Posts'
    };
    
    tabs.forEach(tabName => {
      const panel = document.querySelector(`[data-category="${tabName}"]`);
      const count = panel ? panel.querySelectorAll('.enhanced-card').length : 0;
      const button = document.querySelector(`[data-tab="${tabName}"]`);
      if (button) {
        button.textContent = `${tabLabels[tabName]} (${count})`;
      }
    });
  }
  
  setupFilters() {
    // æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    document.getElementById('searchInput').addEventListener('input', () => {
      this.applyFilters();
    });
    
    // é‡è¦åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    document.getElementById('importanceFilter').addEventListener('change', () => {
      this.applyFilters();
    });
    
    // ã‚½ãƒ¼ãƒˆ
    document.getElementById('sortBy').addEventListener('change', () => {
      this.applySorting();
    });
  }
  
  applyFilters() {
    const searchTerm = document.getElementById('searchInput').value.toLowerCase();
    const importance = document.getElementById('importanceFilter').value;
    
    // ç¾åœ¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¿ãƒ–ã®ã‚«ãƒ¼ãƒ‰ã®ã¿å‡¦ç†
    const activePanel = document.querySelector(`[data-category="${this.activeTab}"]`);
    const cards = activePanel.querySelectorAll('.enhanced-card');
    
    cards.forEach(card => {
      let showCard = true;
      
      // æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
      if (searchTerm) {
        const title = card.querySelector('.card-title a').textContent.toLowerCase();
        const summary = card.querySelector('.card-summary').textContent.toLowerCase();
        const source = card.querySelector('.card-source').textContent.toLowerCase();
        
        showCard = title.includes(searchTerm) || 
                  summary.includes(searchTerm) || 
                  source.includes(searchTerm);
      }
      
      // é‡è¦åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
      if (showCard && importance !== 'all') {
        const score = parseFloat(card.dataset.score);
        if (importance === 'high' && score < 7.0) showCard = false;
        if (importance === 'medium' && (score < 4.0 || score >= 7.0)) showCard = false;
        if (importance === 'low' && score >= 4.0) showCard = false;
      }
      
      card.style.display = showCard ? 'block' : 'none';
    });
  }
  
  applySorting() {
    const sortBy = document.getElementById('sortBy').value;
    const activePanel = document.querySelector(`[data-category="${this.activeTab}"]`);
    const container = activePanel.querySelector('.tab-content');
    const cards = Array.from(container.querySelectorAll('.enhanced-card'));
    
    cards.sort((a, b) => {
      if (sortBy === 'score') {
        return parseFloat(b.dataset.score) - parseFloat(a.dataset.score);
      } else if (sortBy === 'time') {
        const timeA = new Date(a.dataset.time || 0);
        const timeB = new Date(b.dataset.time || 0);
        return timeB - timeA;
      } else if (sortBy === 'source') {
        return a.dataset.source.localeCompare(b.dataset.source);
      }
      return 0;
    });
    
    // DOMå†æ§‹ç¯‰
    cards.forEach(card => container.appendChild(card));
    
    // ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å†é©ç”¨
    this.applyFilters();
  }
}

// åˆæœŸåŒ–
document.addEventListener('DOMContentLoaded', () => {
  new TabController();
});
</script>

</body>
</html>
'''
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    output_file = Path('index.html')
    output_file.write_text(html_template, encoding='utf-8')
    
    # CSSç”Ÿæˆ
    css_content = generate_css()
    css_file = Path('style.css')
    css_file.write_text(css_content, encoding='utf-8')
    print("âœ… CSS file generated")
    
    print(f"âœ… ç”Ÿæˆå®Œäº†: {output_file}")
    print(f"ğŸ“Š ç·è¨˜äº‹æ•°: {total_items}ä»¶")
    print(f"ğŸ† é«˜å„ªå…ˆåº¦: {high_priority}ä»¶")
    print(f"â­ å¹³å‡ã‚¹ã‚³ã‚¢: {sum(item['engineer_score'] for items in all_categories.values() for item in items) / total_items:.1f}")
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        if success:
            print("\nğŸ‰ Daily AI News ç”ŸæˆæˆåŠŸ!")
            print("ğŸŒ GitHub Pages ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦ã”ç¢ºèªãã ã•ã„")
        else:
            sys.exit(1)
    except KeyboardInterrupt:
        print("\nâš ï¸ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)
