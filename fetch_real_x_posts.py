#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Ÿéš›ã®CSVã‹ã‚‰Xãƒã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»å‡¦ç†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import requests
import csv
import io
import html
from datetime import datetime, timezone
import re
import unicodedata

def fetch_and_process_x_posts():
    """Google Sheetsã‹ã‚‰CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€å®Ÿéš›ã®Xãƒã‚¹ãƒˆã‚’å‡¦ç†"""
    
    csv_url = 'https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0'
    
    print("ğŸ“± å®Ÿéš›ã®Xãƒã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    print(f"URL: {csv_url}")
    print("=" * 60)
    
    try:
        # CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        response = requests.get(csv_url, timeout=30)
        print(f"HTTP Status: {response.status_code}")
        
        if response.status_code != 200:
            print(f"âŒ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
            return []
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«è¨­å®š
        response.encoding = 'utf-8'
        content = response.text
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(content)} æ–‡å­—")
        
        # CSVã‚’è§£æ
        reader = csv.reader(io.StringIO(content))
        rows = list(reader)
        print(f"CSVè¡Œæ•°: {len(rows)}")
        
        if not rows:
            print("âŒ CSVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return []
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ç¢ºèª
        headers = rows[0] if rows else []
        print(f"ãƒ˜ãƒƒãƒ€ãƒ¼: {headers}")
        
        # å®Ÿéš›ã®Xãƒã‚¹ãƒˆã‚’å‡¦ç†
        x_posts = []
        processed_count = 0
        
        for i, row in enumerate(rows[1:], 1):  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if len(row) < 3:
                continue
                
            # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            date_str = row[0].strip('"').strip() if len(row) > 0 else ""
            username = row[1].strip('"').strip() if len(row) > 1 else ""
            text = row[2].strip('"').strip() if len(row) > 2 else ""
            media_url = row[3].strip('"').strip() if len(row) > 3 else ""
            tweet_url = row[4].strip('"').strip() if len(row) > 4 else ""
            
            # åŸºæœ¬çš„ãªæ¤œè¨¼
            if not text or len(text.strip()) < 10:  # æœ€ä½10æ–‡å­—
                continue
            
            # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
            text = html.unescape(text)
            username = html.unescape(username)
            
            # æ–‡å­—ã®æ­£è¦åŒ–
            text = unicodedata.normalize('NFKC', text)
            username = unicodedata.normalize('NFKC', username)
            
            # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
            text = ''.join(char for char in text if char.isprintable() or char in '\n\r\t')
            text = re.sub(r'\s+', ' ', text).strip()
            
            # URLãŒãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼URLã‚’ç”Ÿæˆ
            if not tweet_url and username:
                username_clean = username.replace('@', '').replace('"', '')
                tweet_url = f"https://x.com/{username_clean}/status/example_{i}"
            elif not tweet_url:
                tweet_url = f"https://x.com/unknown/status/example_{i}"
            
            # æ—¥ä»˜å‡¦ç†
            try:
                # è¤‡æ•°ã®æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œ
                date_formats = [
                    "%B %d, %Y at %I:%M%p",   # "August 10, 2025 at 02:41AM"
                    "%B %d, %Y",              # "August 13, 2025"
                    "%Y-%m-%d %H:%M:%S",      # "2025-08-18 14:30:00"
                    "%Y-%m-%d",               # "2025-08-18"
                    "%m/%d/%Y",               # "8/18/2025"
                ]
                
                parsed_date = None
                for fmt in date_formats:
                    try:
                        parsed_date = datetime.strptime(date_str, fmt)
                        break
                    except:
                        continue
                
                if parsed_date is None:
                    parsed_date = datetime.now()
                    
            except Exception as e:
                parsed_date = datetime.now()
            
            # Xãƒã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            x_post = {
                'date': date_str,
                'username': username,
                'text': text,
                'media_url': media_url,
                'tweet_url': tweet_url,
                'parsed_date': parsed_date,
                'display_title': f"{username} - AIé–¢é€£ãƒã‚¹ãƒˆ",
                'display_summary': text[:200] + '...' if len(text) > 200 else text
            }
            
            x_posts.append(x_post)
            processed_count += 1
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šæœ€åˆã®3ã¤ã‚’è©³ç´°è¡¨ç¤º
            if processed_count <= 3:
                print(f"\nğŸ“ ãƒã‚¹ãƒˆ {processed_count}:")
                print(f"  æ—¥ä»˜: {date_str}")
                print(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼: {username}")
                print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text[:100]}...")
                print(f"  URL: {tweet_url}")
        
        print(f"\nâœ… å‡¦ç†å®Œäº†: {len(x_posts)}ä»¶ã®å®Ÿéš›ã®Xãƒã‚¹ãƒˆã‚’å–å¾—")
        return x_posts
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []

def convert_to_html_format(x_posts):
    """Xãƒã‚¹ãƒˆã‚’HTMLå½¢å¼ã«å¤‰æ›"""
    html_cards = []
    
    for i, post in enumerate(x_posts[:10], 1):  # æœ€å¤§10ä»¶
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
        if post['username']:
            title = f"ğŸ¦ {post['username']} - AIé–¢é€£æŠ•ç¨¿"
        else:
            title = f"ğŸ¦ Xãƒã‚¹ãƒˆ #{i}"
        
        # è¦ç´„ã‚’ç”Ÿæˆï¼ˆæ—¥æœ¬èªã«ç¿»è¨³ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼‰
        summary = post['display_summary']
        if len(summary) > 300:
            summary = summary[:300] + '...'
        
        # ç›¸å¯¾æ™‚é–“ã‚’è¨ˆç®—
        time_diff = datetime.now() - post['parsed_date']
        hours_ago = int(time_diff.total_seconds() / 3600)
        if hours_ago < 1:
            time_ago = "1æ™‚é–“å‰"
        elif hours_ago < 24:
            time_ago = f"{hours_ago}æ™‚é–“å‰"
        else:
            days_ago = int(hours_ago / 24)
            time_ago = f"{days_ago}æ—¥å‰"
        
        html_card = f'''        <article class="enhanced-card" data-score="10.0" data-source="X / SNS (å®Ÿãƒ‡ãƒ¼ã‚¿)" data-time="{post['parsed_date'].isoformat()}">
          <div class="card-priority high">æœ€é«˜ 10.0</div>
          <header class="card-header">
            <h3 class="card-title">
              <a href="{post['tweet_url']}" target="_blank" rel="noopener">{html.escape(title)}</a>
            </h3>
          </header>
          <div class="card-meta">
            <span class="card-source">X / SNS (å®Ÿãƒ‡ãƒ¼ã‚¿)</span>
          </div>
          <div class="card-summary">{html.escape(summary)}</div>
          <footer class="card-footer">
            <span class="card-score">ã‚¹ã‚³ã‚¢: 10.0</span>
            <span class="card-time">{time_ago}</span>
          </footer>
        </article>

'''
        html_cards.append(html_card)
    
    return ''.join(html_cards)

if __name__ == "__main__":
    # å®Ÿéš›ã®Xãƒã‚¹ãƒˆã‚’å–å¾—
    x_posts = fetch_and_process_x_posts()
    
    if x_posts:
        print(f"\nğŸ“Š å–å¾—æˆåŠŸ: {len(x_posts)}ä»¶")
        
        # HTMLå½¢å¼ã«å¤‰æ›
        html_content = convert_to_html_format(x_posts)
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open('x_posts_real.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print("ğŸ’¾ å®Ÿéš›ã®Xãƒã‚¹ãƒˆHTMLï¼ˆx_posts_real.htmlï¼‰ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        print("\nğŸ”§ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. x_posts_real.htmlã®å†…å®¹ã‚’index.htmlã®Postsã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«æŒ¿å…¥")
        print("2. GitHubã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦åæ˜ ")
        
    else:
        print("âŒ Xãƒã‚¹ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")