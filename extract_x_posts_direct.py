#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSVã‹ã‚‰å®Ÿéš›ã®Xãƒã‚¹ãƒˆã‚’ç›´æ¥æŠ½å‡º
"""
import requests
import re
from datetime import datetime
import html

def extract_real_x_posts():
    """CSVã‹ã‚‰å®Ÿéš›ã®Xãƒã‚¹ãƒˆã‚’æŠ½å‡º"""
    
    csv_url = 'https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0'
    
    try:
        print("ğŸ“± å®Ÿéš›ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        response = requests.get(csv_url, timeout=30)
        
        if response.status_code != 200:
            print(f"âŒ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
            return []
        
        content = response.text
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(content)} æ–‡å­—")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ•ç¨¿ã‚’æŠ½å‡ºï¼ˆAugust 2025ã®æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ï¼‰
        x_posts = []
        
        # August XX, 2025 at XX:XXAM/PM å½¢å¼ã®æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        date_pattern = r'(August \d{1,2}, 2025 at \d{1,2}:\d{2}[AP]M)'
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã¦å‡¦ç†
        lines = content.split('\n')
        current_post = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ—¥ä»˜ã‚’æ¤œå‡º
            date_match = re.search(date_pattern, line)
            if date_match:
                # å‰ã®æŠ•ç¨¿ãŒã‚ã‚Œã°ä¿å­˜
                if current_post.get('text') and len(current_post['text'].strip()) > 20:
                    x_posts.append(current_post.copy())
                
                # æ–°ã—ã„æŠ•ç¨¿ã‚’é–‹å§‹
                current_post = {
                    'date': date_match.group(1),
                    'username': '',
                    'text': '',
                    'urls': []
                }
                continue
            
            # @ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æ¤œå‡º
            username_matches = re.findall(r'@([a-zA-Z0-9_]+)', line)
            if username_matches and not current_post.get('username'):
                current_post['username'] = username_matches[0]
            
            # URLã‚’æ¤œå‡º
            url_matches = re.findall(r'https?://[^\s,"\'"]+', line)
            for url in url_matches:
                if url not in current_post.get('urls', []):
                    current_post.setdefault('urls', []).append(url)
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è“„ç©ï¼ˆæ—¥ä»˜è¡Œä»¥å¤–ï¼‰
            if not re.search(date_pattern, line) and len(line) > 10:
                # CSVã®å¼•ç”¨ç¬¦ã‚’é™¤å»
                cleaned_line = line.strip('"').strip()
                if cleaned_line:
                    if current_post.get('text'):
                        current_post['text'] += ' ' + cleaned_line
                    else:
                        current_post['text'] = cleaned_line
        
        # æœ€å¾Œã®æŠ•ç¨¿ã‚‚è¿½åŠ 
        if current_post.get('text') and len(current_post['text'].strip()) > 20:
            x_posts.append(current_post)
        
        print(f"æŠ½å‡ºã•ã‚ŒãŸæŠ•ç¨¿æ•°: {len(x_posts)}")
        
        # é‡è¤‡é™¤å»ã¨å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_posts = []
        seen_texts = set()
        
        for post in x_posts:
            # ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–
            normalized_text = re.sub(r'\s+', ' ', post['text'].lower().strip())
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            text_signature = normalized_text[:100]  # æœ€åˆã®100æ–‡å­—ã§é‡è¤‡åˆ¤å®š
            if text_signature in seen_texts:
                continue
            seen_texts.add(text_signature)
            
            # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if (len(post['text']) > 30 and  # 30æ–‡å­—ä»¥ä¸Š
                post.get('username') and    # ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒå­˜åœ¨
                not post['text'].startswith('RT')):  # RTã§ã¯ãªã„
                
                # URLã®å„ªå…ˆé †ä½ä»˜ã‘
                tweet_url = ""
                if post.get('urls'):
                    # Twitter/X URLã‚’å„ªå…ˆ
                    for url in post['urls']:
                        if 'twitter.com' in url or 'x.com' in url:
                            tweet_url = url
                            break
                    if not tweet_url:
                        tweet_url = post['urls'][0]
                
                # URLãŒãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼URL
                if not tweet_url:
                    username_clean = post['username'].replace('@', '')
                    tweet_url = f"https://x.com/{username_clean}/status/example_{len(filtered_posts)+1}"
                
                filtered_posts.append({
                    'date': post['date'],
                    'username': f"@{post['username']}" if not post['username'].startswith('@') else post['username'],
                    'text': html.unescape(post['text'].strip()),
                    'tweet_url': tweet_url,
                    'display_summary': post['text'][:250] + '...' if len(post['text']) > 250 else post['text']
                })
        
        print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered_posts)}ä»¶")
        
        # æœ€åˆã®5ä»¶ã‚’è©³ç´°è¡¨ç¤º
        print("\nğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸXãƒã‚¹ãƒˆï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
        print("=" * 60)
        
        for i, post in enumerate(filtered_posts[:5], 1):
            print(f"\n{i}. æ—¥ä»˜: {post['date']}")
            print(f"   ãƒ¦ãƒ¼ã‚¶ãƒ¼: {post['username']}")
            print(f"   URL: {post['tweet_url']}")
            print(f"   ãƒ†ã‚­ã‚¹ãƒˆ: {post['text'][:150]}...")
            print(f"   æ–‡å­—æ•°: {len(post['text'])}")
        
        return filtered_posts[:10]  # æœ€å¤§10ä»¶ã‚’è¿”ã™
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []

def create_html_from_real_posts(posts):
    """å®Ÿéš›ã®Xãƒã‚¹ãƒˆã‹ã‚‰HTMLå½¢å¼ã‚’ç”Ÿæˆ"""
    if not posts:
        return ""
    
    html_cards = []
    
    for i, post in enumerate(posts, 1):
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
        title = f"ğŸ¦ {post['username']} - AIé–¢é€£æŠ•ç¨¿"
        
        # è¦ç´„ã‚’ç”Ÿæˆ
        summary = post['display_summary']
        
        # æ™‚é–“è¡¨ç¤ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        time_ago = "æ•°æ™‚é–“å‰"
        
        html_card = f'''        <article class="enhanced-card" data-score="10.0" data-source="X / SNS (å®Ÿãƒ‡ãƒ¼ã‚¿)" data-time="{datetime.now().isoformat()}">
          <div class="card-priority high">æœ€é«˜ 10.0</div>
          <header class="card-header">
            <h3 class="card-title">
              <a href="{post['tweet_url']}" target="_blank" rel="noopener">{html.escape(title)}</a>
            </h3>
          </header>
          <div class="card-meta">
            <span class="card-source">X / SNS (å®Ÿãƒ‡ãƒ¼ã‚¿)</span>
          </div>
          <div class="card-summary">{html.escape(summary)}</div>
          <footer class="card-footer">
            <span class="card-score">ã‚¹ã‚³ã‚¢: 10.0</span>
            <span class="card-time">{time_ago}</span>
          </footer>
        </article>

'''
        html_cards.append(html_card)
    
    return ''.join(html_cards)

if __name__ == "__main__":
    # å®Ÿéš›ã®Xãƒã‚¹ãƒˆã‚’æŠ½å‡º
    real_posts = extract_real_x_posts()
    
    if real_posts:
        # HTMLç”Ÿæˆ
        html_content = create_html_from_real_posts(real_posts)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open('real_x_posts.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"\nâœ… æˆåŠŸ: {len(real_posts)}ä»¶ã®å®Ÿéš›ã®Xãƒã‚¹ãƒˆã‚’æŠ½å‡º")
        print("ğŸ’¾ real_x_posts.htmlã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # çµ±è¨ˆæƒ…å ±
        usernames = set(post['username'] for post in real_posts)
        print(f"\nğŸ“Š çµ±è¨ˆ:")
        print(f"   æŠ•ç¨¿æ•°: {len(real_posts)}")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼: {len(usernames)}")
        print(f"   ä¸»è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼: {', '.join(list(usernames)[:5])}")
        
    else:
        print("âŒ Xãƒã‚¹ãƒˆã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")