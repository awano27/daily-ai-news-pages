#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
BeautifulSoup + Gemini AI ã«ã‚ˆã‚‹é«˜åº¦ãªã‚¦ã‚§ãƒ–è§£æ
"""

import sys
import os
import argparse
import json
from pathlib import Path

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # python-dotenvãŒãªã„å ´åˆã¯æ‰‹å‹•ã§.envã‚’èª­ã¿è¾¼ã¿
    env_path = os.path.join(Path(__file__).parent.parent, '.env')
    if os.path.exists(env_path):
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scrapers.beautifulsoup_scraper import BeautifulSoupScraper
from scrapers.gemini_extractor import GeminiExtractor

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Free Scraping Platform - çµ±åˆã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"
    )
    
    parser.add_argument(
        'url',
        help='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL'
    )
    
    parser.add_argument(
        '--method',
        choices=['basic', 'ai', 'full'],
        default='basic',
        help='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ–¹æ³• (basic: HTMLè§£æã®ã¿, ai: AIæŠ½å‡ºã®ã¿, full: ä¸¡æ–¹)'
    )
    
    parser.add_argument(
        '--ai-extraction',
        choices=['summary', 'keywords', 'structure', 'analysis'],
        default='summary',
        help='AIæŠ½å‡ºã‚¿ã‚¤ãƒ—'
    )
    
    parser.add_argument(
        '--output',
        help='çµæœå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (JSONå½¢å¼)'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è©³ç´°å‡ºåŠ›'
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ Free Scraping Platform é–‹å§‹")
    print(f"ğŸ¯ å¯¾è±¡URL: {args.url}")
    print(f"ğŸ“Š æ–¹æ³•: {args.method}")
    
    results = {}
    
    try:
        # Basic ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        if args.method in ['basic', 'full']:
            print("\nğŸ” åŸºæœ¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œä¸­...")
            scraper = BeautifulSoupScraper()
            basic_result = scraper.scrape(args.url)
            results['basic'] = basic_result
            
            if args.verbose:
                print(f"   ã‚¿ã‚¤ãƒˆãƒ«: {basic_result.get('title', 'N/A')}")
                print(f"   ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·: {len(basic_result.get('content', ''))}æ–‡å­—")
                print(f"   ãƒªãƒ³ã‚¯æ•°: {len(basic_result.get('links', []))}")
            
            scraper.close()
        
        # AI æŠ½å‡º
        if args.method in ['ai', 'full']:
            print(f"\nğŸ¤– AIæŠ½å‡ºå®Ÿè¡Œä¸­ ({args.ai_extraction})...")
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ï¼ˆbasicã‹ã‚‰ã€ã¾ãŸã¯æ–°è¦å–å¾—ï¼‰
            if 'basic' in results and results['basic']['success']:
                content = results['basic']['content']
            else:
                # AIæŠ½å‡ºã®ã¿ã®å ´åˆã¯åŸºæœ¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
                scraper = BeautifulSoupScraper()
                basic_result = scraper.scrape(args.url)
                content = basic_result.get('content', '')
                scraper.close()
                
                if not content:
                    print("âŒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—å¤±æ•—")
                    return
            
            # GeminiæŠ½å‡ºå®Ÿè¡Œ
            try:
                extractor = GeminiExtractor()
                ai_result = extractor.extract(content, args.ai_extraction)
                results['ai'] = ai_result
                
                if args.verbose and ai_result.get('success'):
                    print(f"   æŠ½å‡ºã‚¿ã‚¤ãƒ—: {ai_result.get('extraction_type')}")
                    if 'summary' in ai_result:
                        print(f"   è¦ç´„: {ai_result['summary'][:100]}...")
                    
            except Exception as e:
                print(f"âŒ AIæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                results['ai'] = {
                    'success': False,
                    'error': str(e)
                }
        
        # çµæœè¡¨ç¤º
        print("\nğŸ“‹ å®Ÿè¡Œçµæœ:")
        
        if 'basic' in results:
            status = "âœ… æˆåŠŸ" if results['basic']['success'] else "âŒ å¤±æ•—"
            print(f"   åŸºæœ¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {status}")
        
        if 'ai' in results:
            status = "âœ… æˆåŠŸ" if results['ai']['success'] else "âŒ å¤±æ•—"
            print(f"   AIæŠ½å‡º: {status}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ çµæœã‚’ä¿å­˜: {args.output}")
        
        # ç°¡æ˜“è¡¨ç¤º
        if not args.output and not args.verbose:
            if 'basic' in results and results['basic']['success']:
                print(f"\nğŸ“„ ã‚¿ã‚¤ãƒˆãƒ«: {results['basic']['title']}")
                print(f"ğŸ“ è¦ç´„: {results['basic']['content'][:200]}...")
            
            if 'ai' in results and results['ai']['success']:
                if 'summary' in results['ai']:
                    print(f"ğŸ¤– AIè¦ç´„: {results['ai']['summary'][:200]}...")
        
        print("\nâœ… å‡¦ç†å®Œäº†")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()