# -*- coding: utf-8 -*-
"""
Enhanced Daily AI News - with Engineer-focused Ranking System
æ—¢å­˜ã®è±Šå¯Œãªæƒ…å ±é‡ã‚’ç¶­æŒã—ã¤ã¤ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã‚¹ãƒãƒ¼ãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å®Ÿè£…

- å…¨è¨˜äº‹ã‚’å–å¾—ï¼ˆæƒ…å ±é‡ç¶­æŒï¼‰
- ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢é–¢é€£åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
- æŠ€è¡“çš„ä¾¡å€¤ã«åŸºã¥ãè‡ªå‹•ãƒ©ãƒ³ã‚­ãƒ³ã‚°
- å„ªå…ˆåº¦è¡¨ç¤ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½
"""

import os, re, sys, json, time, html, csv, io
from datetime import datetime, timezone, timedelta
from pathlib import Path
from urllib.parse import urlparse
from urllib.request import urlopen
import yaml
import feedparser
import requests
import random

# Enhanced X Processing Integration
try:
    from enhanced_x_processor import EnhancedXProcessor
    ENHANCED_X_AVAILABLE = True
    print("âœ… Enhanced X Processor: Integrated")
except ImportError:
    ENHANCED_X_AVAILABLE = False
    print("âš ï¸ Enhanced X Processor: Using fallback")

# URL ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from url_filter import filter_403_urls, is_403_url
    print("âœ… URL ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½: æœ‰åŠ¹")
except ImportError:
    print("âš ï¸ URL ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½: ç„¡åŠ¹")
    def filter_403_urls(items):
        return items
    def is_403_url(url):
        return False

# ç¿»è¨³æ©Ÿèƒ½
try:
    from deep_translator import GoogleTranslator, MyMemoryTranslator
    TRANSLATE_AVAILABLE = True
    print("âœ… ç¿»è¨³æ©Ÿèƒ½: åˆ©ç”¨å¯èƒ½")
except ImportError:
    print("âš ï¸ ç¿»è¨³æ©Ÿèƒ½: deep-translatoræœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    TRANSLATE_AVAILABLE = False

class EngineerRankingSystem:
    """ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘è¨˜äº‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    # ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé‡ã¿ä»˜ãï¼‰
    TECH_KEYWORDS = {
        # å®Ÿè£…ãƒ»é–‹ç™ºé–¢é€£ï¼ˆé«˜å„ªå…ˆåº¦ï¼‰
        'implementation': {
            'keywords': ['code', 'api', 'sdk', 'library', 'framework', 'tutorial', 'example', 'github', 'implementation', 'coding'],
            'weight': 3.0
        },
        
        # AI/MLæŠ€è¡“ï¼ˆé«˜å„ªå…ˆåº¦ï¼‰
        'ai_ml': {
            'keywords': ['pytorch', 'tensorflow', 'huggingface', 'langchain', 'openai', 'anthropic', 'gpt', 'llm', 'transformer', 'neural', 'model', 'training', 'inference'],
            'weight': 2.5
        },
        
        # ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»æœ¬ç•ªç’°å¢ƒï¼ˆä¸­é«˜å„ªå…ˆåº¦ï¼‰
        'infrastructure': {
            'keywords': ['docker', 'kubernetes', 'aws', 'gcp', 'azure', 'production', 'deploy', 'devops', 'mlops', 'scaling'],
            'weight': 2.0
        },
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆä¸­å„ªå…ˆåº¦ï¼‰
        'performance': {
            'keywords': ['optimization', 'performance', 'benchmark', 'speed', 'latency', 'throughput', 'memory', 'gpu'],
            'weight': 1.8
        },
        
        # ç ”ç©¶ãƒ»è«–æ–‡ï¼ˆä¸­å„ªå…ˆåº¦ï¼‰
        'research': {
            'keywords': ['paper', 'research', 'arxiv', 'study', 'algorithm', 'method', 'evaluation'],
            'weight': 1.5
        },
        
        # ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆä¸­å„ªå…ˆåº¦ï¼‰
        'tools': {
            'keywords': ['tool', 'platform', 'service', 'database', 'vector', 'embedding', 'search'],
            'weight': 1.3
        }
    }
    
    # ãƒ“ã‚¸ãƒã‚¹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä½å„ªå…ˆåº¦ã€ä½†ã—é‡è¦ãªã‚‚ã®ã¯é™¤å¤–ã—ãªã„ï¼‰
    BUSINESS_KEYWORDS = {
        'funding': ['funding', 'investment', 'series', 'valuation'],
        'partnership': ['partnership', 'acquisition', 'merger'],
        'release': ['launch', 'release', 'announce', 'unveil']
    }
    
    # ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹ï¼ˆãƒœãƒ¼ãƒŠã‚¹ï¼‰
    TRUSTED_SOURCES = {
        'arxiv.org': 2.0,
        'github.com': 1.8,
        'pytorch.org': 1.8,
        'tensorflow.org': 1.8,
        'huggingface.co': 1.8,
        'openai.com': 1.5,
        'anthropic.com': 1.5,
        'engineering.fb.com': 1.5,
        'ai.googleblog.com': 1.5,
        'deepmind.com': 1.5,
        'research.google.com': 1.3,
        'microsoft.com': 1.3,
        'aws.amazon.com': 1.3
    }
    
    @classmethod
    def calculate_engineer_score(cls, item):
        """ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢é–¢é€£åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        title = item.get('title', '').lower()
        summary = item.get('summary', '').lower()
        url = item.get('url', '')
        source = item.get('source', '').lower()
        
        content = f"{title} {summary}".lower()
        score = 0.0
        
        # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢
        for category, config in cls.TECH_KEYWORDS.items():
            keywords = config['keywords']
            weight = config['weight']
            
            matches = sum(1 for keyword in keywords if keyword in content)
            if matches > 0:
                # è¤‡æ•°ãƒãƒƒãƒã«ãƒœãƒ¼ãƒŠã‚¹ï¼ˆä¸Šé™ã‚ã‚Šï¼‰
                match_bonus = min(matches * 0.3, 1.0)
                score += weight * (1 + match_bonus)
        
        # ã‚½ãƒ¼ã‚¹ä¿¡é ¼åº¦ãƒœãƒ¼ãƒŠã‚¹
        domain = urlparse(url).netloc.lower()
        for trusted_domain, bonus in cls.TRUSTED_SOURCES.items():
            if trusted_domain in domain:
                score *= bonus
                break
        
        # ã‚³ãƒ¼ãƒ‰/å®Ÿè£…é–¢é€£ã®ç‰¹åˆ¥ãƒœãƒ¼ãƒŠã‚¹
        if any(indicator in content for indicator in ['```', 'github.com', 'code example', 'implementation']):
            score *= 1.5
        
        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒé‡è¦–ï¼‰
        if re.search(r'\d+[%x]|\d+ms|\d+gb|\d+fps|benchmark|performance', content):
            score *= 1.3
        
        # è«–æ–‡ãƒ»ç ”ç©¶ï¼ˆå­¦è¡“çš„ä¾¡å€¤ï¼‰
        if 'arxiv' in url or any(word in content for word in ['paper', 'research', 'study']):
            score *= 1.2
        
        # ãƒ“ã‚¸ãƒã‚¹ç³»ã§ã‚‚æŠ€è¡“çš„ä¾¡å€¤ãŒã‚ã‚‹ã‚‚ã®ã¯é™¤å¤–ã—ãªã„
        business_score = 0
        for category, keywords in cls.BUSINESS_KEYWORDS.items():
            business_matches = sum(1 for keyword in keywords if keyword in content)
            if business_matches > 0:
                business_score += 0.3
        
        # ç´”ç²‹ãªãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯é‡ã¿è»½æ¸›ï¼ˆãŸã ã—å®Œå…¨æ’é™¤ã¯ã—ãªã„ï¼‰
        if business_score > 0.5 and score < 2.0:
            score *= 0.7
        
        return min(score, 10.0)  # æœ€å¤§10ç‚¹
    
    @classmethod
    def get_priority_level(cls, score):
        """ã‚¹ã‚³ã‚¢ã‹ã‚‰å„ªå…ˆåº¦ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š"""
        if score >= 6.0:
            return "ğŸ”¥", "hot", "æœ€é«˜å„ªå…ˆ"
        elif score >= 4.0:
            return "âš¡", "high", "é«˜å„ªå…ˆ"
        elif score >= 2.5:
            return "ğŸ“–", "medium", "ä¸­å„ªå…ˆ"
        elif score >= 1.0:
            return "ğŸ“°", "low", "ä½å„ªå…ˆ"
        else:
            return "ğŸ“„", "minimal", "å‚è€ƒ"
    
    @classmethod
    def detect_tech_categories(cls, item):
        """è¨˜äº‹ã®æŠ€è¡“ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’æ¤œå‡º"""
        content = f"{item.get('title', '')} {item.get('summary', '')}".lower()
        categories = []
        
        # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª
        if re.search(r'\b(python|javascript|rust|go|c\+\+|java)\b', content):
            categories.append('Programming')
        
        # AI/ML
        if any(word in content for word in ['ai', 'ml', 'machine learning', 'deep learning', 'neural', 'gpt', 'llm']):
            categories.append('AI/ML')
        
        # ã‚¤ãƒ³ãƒ•ãƒ©
        if any(word in content for word in ['docker', 'kubernetes', 'aws', 'cloud', 'devops']):
            categories.append('Infrastructure')
        
        # Webé–‹ç™º
        if any(word in content for word in ['web', 'frontend', 'backend', 'api', 'react', 'vue']):
            categories.append('Web Dev')
        
        # ãƒ‡ãƒ¼ã‚¿
        if any(word in content for word in ['data', 'database', 'analytics', 'visualization']):
            categories.append('Data')
        
        return categories[:3]  # æœ€å¤§3ã¤ã¾ã§

def load_config():
    """è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    config = {
        'HOURS_LOOKBACK': int(os.getenv('HOURS_LOOKBACK', '24')),
        'MAX_ITEMS_PER_CATEGORY': int(os.getenv('MAX_ITEMS_PER_CATEGORY', '25')),  # å¢—é‡
        'TRANSLATE_TO_JA': os.getenv('TRANSLATE_TO_JA', '1') == '1',
        'TRANSLATE_ENGINE': os.getenv('TRANSLATE_ENGINE', 'google'),
        'X_POSTS_CSV': os.getenv('X_POSTS_CSV', 'https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0'),
        'TZ': os.getenv('TZ', 'Asia/Tokyo')
    }
    return config

def load_feeds():
    """ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    feeds_file = Path('feeds.yml')
    if not feeds_file.exists():
        print(f"âš ï¸ {feeds_file}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return {
            'Business': ['https://venturebeat.com/ai/feed/'],
            'Tools': ['https://huggingface.co/blog/feed.xml'],
            'Posts': []
        }
    
    with open(feeds_file, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def fetch_and_rank_articles(config, feeds):
    """è¨˜äº‹ã‚’å–å¾—ã—ã¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
    all_items = []
    cutoff_time = datetime.now(timezone.utc) - timedelta(hours=config['HOURS_LOOKBACK'])
    
    print(f"ğŸ“° è¨˜äº‹å–å¾—é–‹å§‹: {config['HOURS_LOOKBACK']}æ™‚é–“ä»¥å†…")
    
    # RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹å–å¾—
    for category, feed_list in feeds.items():
        if category == 'Posts':  # X posts ã¯å¾Œã§å‡¦ç†
            continue
            
        print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒª: {category}")
        
        for feed_entry in feed_list:
            try:
                # feed_entryãŒè¾æ›¸å½¢å¼ï¼ˆname, urlï¼‰ã‹URLæ–‡å­—åˆ—ã‹ã‚’åˆ¤å®š
                if isinstance(feed_entry, dict):
                    feed_url = feed_entry.get('url', '')
                    feed_name = feed_entry.get('name', 'Unknown')
                    print(f"  ğŸ”„ å–å¾—ä¸­: {feed_name} - {feed_url}")
                else:
                    feed_url = feed_entry
                    feed_name = get_source_name(feed_url)
                    print(f"  ğŸ”„ å–å¾—ä¸­: {feed_url}")
                
                if not feed_url:
                    continue
                    
                feed = feedparser.parse(feed_url)
                
                for entry in feed.entries:
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                    
                    if pub_date and pub_date < cutoff_time:
                        continue
                    
                    item = {
                        'title': entry.get('title', 'No title'),
                        'url': entry.get('link', ''),
                        'summary': clean_html(entry.get('summary', entry.get('description', ''))),
                        'published': pub_date,
                        'source': feed_name if isinstance(feed_entry, dict) else get_source_name(feed_url),
                        'category': category,
                        'is_x_post': False
                    }
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—
                    item['engineer_score'] = EngineerRankingSystem.calculate_engineer_score(item)
                    item['priority_icon'], item['priority_class'], item['priority_text'] = EngineerRankingSystem.get_priority_level(item['engineer_score'])
                    item['tech_categories'] = EngineerRankingSystem.detect_tech_categories(item)
                    
                    all_items.append(item)
                    
            except Exception as e:
                print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    # X posts ã‚’è¿½åŠ 
    try:
        x_posts = fetch_x_posts(config['X_POSTS_CSV'])
        for post in x_posts:
            post['engineer_score'] = EngineerRankingSystem.calculate_engineer_score(post)
            post['priority_icon'], post['priority_class'], post['priority_text'] = EngineerRankingSystem.get_priority_level(post['engineer_score'])
            post['tech_categories'] = EngineerRankingSystem.detect_tech_categories(post)
        all_items.extend(x_posts)
        print(f"âœ… X posts: {len(x_posts)}ä»¶è¿½åŠ ")
    except Exception as e:
        print(f"âŒ X postså–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 403ã‚¨ãƒ©ãƒ¼URLã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    all_items = filter_403_urls(all_items)
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚¹ã‚³ã‚¢é †ï¼‰
    all_items.sort(key=lambda x: x['engineer_score'], reverse=True)
    
    print(f"ğŸ“Š ç·è¨˜äº‹æ•°: {len(all_items)}ä»¶")
    print(f"ğŸ”¥ é«˜å„ªå…ˆåº¦ï¼ˆ4.0+ï¼‰: {len([x for x in all_items if x['engineer_score'] >= 4.0])}ä»¶")
    print(f"âš¡ ä¸­å„ªå…ˆåº¦ï¼ˆ2.5+ï¼‰: {len([x for x in all_items if x['engineer_score'] >= 2.5 and x['engineer_score'] < 4.0])}ä»¶")
    
    return all_items

def fetch_x_posts(csv_url):
    """X posts ã‚’å–å¾—"""
    if not csv_url:
        return []
    
    posts = []
    try:
        if ENHANCED_X_AVAILABLE:
            processor = EnhancedXProcessor()
            posts = processor.process_csv_posts(csv_url)
        else:
            # Fallbackå‡¦ç†
            response = requests.get(csv_url, timeout=10)
            response.encoding = 'utf-8'
            
            reader = csv.DictReader(io.StringIO(response.text))
            for row in reader:
                if not row.get('content') or not row.get('url'):
                    continue
                
                posts.append({
                    'title': (row.get('content') or '')[:100] + '...',
                    'summary': row.get('content', ''),
                    'url': row.get('url', ''),
                    'published': datetime.now(timezone.utc),
                    'source': 'X (Twitter)',
                    'category': 'Posts',
                    'is_x_post': True
                })
    except Exception as e:
        print(f"X postså–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return posts

def clean_html(text):
    """HTMLã‚¿ã‚°ã‚’é™¤å»"""
    if not text:
        return ''
    text = re.sub(r'<[^>]+>', '', text)
    text = html.unescape(text)
    return text.strip()

def get_source_name(url):
    """URLã‹ã‚‰ã‚½ãƒ¼ã‚¹åã‚’æŠ½å‡º"""
    domain = urlparse(url).netloc
    domain = domain.replace('www.', '').replace('.com', '').replace('.org', '')
    return domain.title()

def translate_summaries(items, config):
    """è¦ç´„ã‚’ç¿»è¨³"""
    if not config['TRANSLATE_TO_JA'] or not TRANSLATE_AVAILABLE:
        return
    
    cache_file = Path('_cache/translations.json')
    cache_file.parent.mkdir(exist_ok=True)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
    cache = {}
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache = json.load(f)
        except:
            cache = {}
    
    # ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
    if config['TRANSLATE_ENGINE'] == 'mymemory':
        translator = MyMemoryTranslator(source='en', target='ja-JP')
    else:
        translator = GoogleTranslator(source='en', target='ja')
    
    translated_count = 0
    for item in items:
        summary = item.get('summary', '')
        if not summary or len(summary) < 10:
            continue
        
        cache_key = f"{config['TRANSLATE_ENGINE']}:{hash(summary)}"
        
        if cache_key in cache:
            item['summary'] = cache[cache_key]
        else:
            try:
                translated = translator.translate(summary)
                item['summary'] = translated
                cache[cache_key] = translated
                translated_count += 1
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                if translated_count % 5 == 0:
                    time.sleep(1)
                    
            except Exception as e:
                print(f"ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"ğŸ”¤ æ–°è¦ç¿»è¨³: {translated_count}ä»¶")

def generate_enhanced_html(items, config):
    """ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘æ‹¡å¼µHTMLã‚’ç”Ÿæˆ"""
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°é †ã‚’ç¶­æŒï¼‰
    categorized = {}
    for item in items:
        category = item['category']
        if category not in categorized:
            categorized[category] = []
        categorized[category].append(item)
    
    # çµ±è¨ˆè¨ˆç®—
    total_items = len(items)
    high_priority = len([x for x in items if x['engineer_score'] >= 4.0])
    medium_priority = len([x for x in items if x['engineer_score'] >= 2.5 and x['engineer_score'] < 4.0])
    
    now = datetime.now().strftime("%Y-%m-%d %H:%M JST")
    
    html = f'''<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Daily AI News â€” {now}</title>
  <link rel="stylesheet" href="style_enhanced_ranking.css"/>
</head>
<body>
  <header class="site-header">
    <div class="brand">ğŸ“° Daily AI News</div>
    <div class="updated">æœ€çµ‚æ›´æ–°ï¼š{now}</div>
  </header>

  <main class="container">
    <h1 class="page-title">ä»Šæ—¥ã®æœ€æ–°AIæƒ…å ±</h1>
    <p class="lead">
        ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢é–¢é€£åº¦ã‚¹ã‚³ã‚¢ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤ºã€‚å®Ÿè£…å¯èƒ½æ€§ãƒ»æŠ€è¡“çš„ä¾¡å€¤ãƒ»å­¦ç¿’åŠ¹æœã‚’é‡è¦–ã—ãŸè‡ªå‹•ã‚½ãƒ¼ãƒˆã€‚
        é«˜å„ªå…ˆåº¦ï¼ˆğŸ”¥ï¼‰ã€ä¸­å„ªå…ˆåº¦ï¼ˆâš¡ğŸ“–ï¼‰ã€å‚è€ƒæƒ…å ±ï¼ˆğŸ“°ğŸ“„ï¼‰ã§åˆ†é¡è¡¨ç¤ºã€‚
    </p>

    <section class="kpi-grid">
      <div class="kpi-card">
        <div class="kpi-value">{total_items}ä»¶</div>
        <div class="kpi-label">ç·è¨˜äº‹æ•°</div>
        <div class="kpi-note">å…¨æƒ…å ±ã‚’ä¿æŒ</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{high_priority}ä»¶</div>
        <div class="kpi-label">é«˜å„ªå…ˆåº¦è¨˜äº‹</div>
        <div class="kpi-note">ã‚¹ã‚³ã‚¢4.0ä»¥ä¸Š</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{medium_priority}ä»¶</div>
        <div class="kpi-label">ä¸­å„ªå…ˆåº¦è¨˜äº‹</div>
        <div class="kpi-note">ã‚¹ã‚³ã‚¢2.5ä»¥ä¸Š</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{now}</div>
        <div class="kpi-label">æœ€çµ‚æ›´æ–°</div>
        <div class="kpi-note">è‡ªå‹•æ›´æ–°</div>
      </div>
    </section>

    <!-- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ -->
    <div class="filter-controls">
      <div class="filter-group">
        <label>å„ªå…ˆåº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼:</label>
        <button class="filter-btn active" data-filter="all">ã™ã¹ã¦ ({total_items})</button>
        <button class="filter-btn" data-filter="hot">ğŸ”¥ æœ€é«˜å„ªå…ˆ ({len([x for x in items if x['engineer_score'] >= 6.0])})</button>
        <button class="filter-btn" data-filter="high">âš¡ é«˜å„ªå…ˆ ({len([x for x in items if x['engineer_score'] >= 4.0 and x['engineer_score'] < 6.0])})</button>
        <button class="filter-btn" data-filter="medium">ğŸ“– ä¸­å„ªå…ˆ ({len([x for x in items if x['engineer_score'] >= 2.5 and x['engineer_score'] < 4.0])})</button>
      </div>
      <div class="search-container">
        <input id="searchBox" type="text" placeholder="ğŸ” æŠ€è¡“ã€ä¼æ¥­ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢..." />
      </div>
    </div>

    <nav class="tabs" role="tablist">'''
    
    # ã‚¿ãƒ–ãƒœã‚¿ãƒ³ç”Ÿæˆ
    for i, (category, cat_items) in enumerate(categorized.items()):
        active = 'active' if i == 0 else ''
        icon = get_category_icon(category)
        html += f'''
      <button class="tab {active}" data-target="#{category.lower()}" aria-selected="{str(i==0).lower()}">
        {icon} {category} ({len(cat_items)})
      </button>'''
    
    html += '''
    </nav>

'''
    
    # ã‚¿ãƒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
    for i, (category, cat_items) in enumerate(categorized.items()):
        active = 'active' if i == 0 else ''
        html += f'''    <section id="{category.lower()}" class="tab-panel {active}">
'''
        
        for item in cat_items:
            html += generate_enhanced_article_card(item)
        
        html += '''    </section>

'''
    
    html += '''  </main>

  <footer class="site-footer">
    <div class="footer-content">
      <p>Â© 2025 Daily AI News - Enhanced with Engineer Ranking System</p>
      <p>æŠ€è¡“é–¢é€£åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹è‡ªå‹•ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤º</p>
    </div>
  </footer>

  <script src="script_enhanced_ranking.js"></script>
</body>
</html>'''
    
    return html

def get_category_icon(category):
    """ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚¢ã‚¤ã‚³ãƒ³ã‚’å–å¾—"""
    icons = {
        'Business': 'ğŸ¢',
        'Tools': 'âš¡', 
        'Posts': 'ğŸ§ª',
        'Research': 'ğŸ”¬',
        'Implementation': 'âš™ï¸'
    }
    return icons.get(category, 'ğŸ“„')

def generate_enhanced_article_card(item):
    """æ‹¡å¼µè¨˜äº‹ã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
    
    # æ™‚é–“è¡¨ç¤º
    time_ago = "ä»Š"
    if item.get('published'):
        diff = datetime.now(timezone.utc) - item['published']
        hours = int(diff.total_seconds() / 3600)
        if hours >= 24:
            time_ago = f"{hours // 24}æ—¥å‰"
        elif hours > 0:
            time_ago = f"{hours}æ™‚é–“å‰"
    
    # æŠ€è¡“ã‚«ãƒ†ã‚´ãƒªãƒ¼
    tech_categories = item.get('tech_categories', [])
    tech_tags = ''.join(f'<span class="tech-tag">{cat}</span>' for cat in tech_categories)
    
    # å„ªå…ˆåº¦è¡¨ç¤º
    priority_icon = item.get('priority_icon', 'ğŸ“„')
    priority_class = item.get('priority_class', 'minimal')
    priority_text = item.get('priority_text', 'å‚è€ƒ')
    engineer_score = item.get('engineer_score', 0)
    
    # è¦ç´„ã®é•·ã•èª¿æ•´
    summary = item.get('summary', '')
    if len(summary) > 200:
        summary = summary[:197] + '...'
    
    return f'''
<article class="card enhanced-card" data-score="{engineer_score:.1f}" data-priority="{priority_class}">
  <div class="card-header">
    <div class="priority-indicator {priority_class}">
      <span class="priority-icon">{priority_icon}</span>
      <span class="priority-text">{priority_text}</span>
      <span class="score-badge">ã‚¹ã‚³ã‚¢: {engineer_score:.1f}</span>
    </div>
    <div class="card-meta">
      <span class="meta-time">ğŸ“… {time_ago}</span>
      <span class="meta-source">ğŸ“– {item.get('source', '')}</span>
    </div>
  </div>
  
  <div class="card-body">
    <a class="card-title" href="{item.get('url', '')}" target="_blank" rel="noopener">
      {item.get('title', 'No title')}
    </a>
    <p class="card-summary">{summary}</p>
    
    <div class="tech-categories">
      {tech_tags}
    </div>
  </div>
  
  <div class="card-footer">
    <div class="card-actions">
      <a href="{item.get('url', '')}" class="action-btn primary" target="_blank">ğŸ“– è©³ç´°ã‚’èª­ã‚€</a>
      <button class="action-btn bookmark" data-url="{item.get('url', '')}">ğŸ”– ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯</button>
    </div>
    <div class="source-link">
      <small>å‡ºå…¸: <a href="{item.get('url', '')}" target="_blank">{item.get('url', '')[:50]}...</a></small>
    </div>
  </div>
</article>
'''

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ Enhanced Daily AI News with Engineer Ranking")
    print("=" * 50)
    
    config = load_config()
    feeds = load_feeds()
    
    print(f"âš™ï¸ è¨­å®š: {config['HOURS_LOOKBACK']}æ™‚é–“ã€æœ€å¤§{config['MAX_ITEMS_PER_CATEGORY']}ä»¶/ã‚«ãƒ†ã‚´ãƒª")
    
    # è¨˜äº‹å–å¾—ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    items = fetch_and_rank_articles(config, feeds)
    
    # ç¿»è¨³å‡¦ç†
    translate_summaries(items, config)
    
    # HTMLç”Ÿæˆ
    html_content = generate_enhanced_html(items, config)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    output_file = Path('index.html')
    output_file.write_text(html_content, encoding='utf-8')
    
    print(f"âœ… ç”Ÿæˆå®Œäº†: {output_file}")
    print(f"ğŸ“Š ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœ:")
    print(f"   ğŸ”¥ æœ€é«˜å„ªå…ˆåº¦: {len([x for x in items if x['engineer_score'] >= 6.0])}ä»¶")
    print(f"   âš¡ é«˜å„ªå…ˆåº¦: {len([x for x in items if x['engineer_score'] >= 4.0 and x['engineer_score'] < 6.0])}ä»¶")
    print(f"   ğŸ“– ä¸­å„ªå…ˆåº¦: {len([x for x in items if x['engineer_score'] >= 2.5 and x['engineer_score'] < 4.0])}ä»¶")
    print(f"   ğŸ“° ä½å„ªå…ˆåº¦: {len([x for x in items if x['engineer_score'] >= 1.0 and x['engineer_score'] < 2.5])}ä»¶")
    print(f"   ğŸ“„ å‚è€ƒ: {len([x for x in items if x['engineer_score'] < 1.0])}ä»¶")

if __name__ == "__main__":
    main()