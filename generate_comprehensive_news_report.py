#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒ…æ‹¬çš„AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
RSS + X(Twitter) + è¿½åŠ ã‚½ãƒ¼ã‚¹çµ±åˆç‰ˆ
"""

import os
import sys
import json
import csv
import feedparser
import requests
import pandas as pd
from datetime import datetime, timedelta
from collections import defaultdict
from typing import List, Dict, Any, Tuple
import google.generativeai as genai
import re
import warnings
from urllib.parse import urlparse
import pytz

# è­¦å‘Šç„¡åŠ¹åŒ–
warnings.filterwarnings('ignore')

# Gemini APIè¨­å®š
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if GEMINI_API_KEY and not os.getenv('DISABLE_GEMINI'):
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp')

# XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
X_POSTS_CSV_URL = os.getenv('X_POSTS_CSV', 'https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0')

# å‹•ä½œç¢ºèªæ¸ˆã¿ã®ä¿¡é ¼æ€§é«˜ãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆæ‹¡å¼µç‰ˆï¼‰
VERIFIED_FEEDS = {
    'tier1_official': [  # æœ€é«˜ä¿¡é ¼æ€§
        {'name': 'TechCrunch', 'url': 'https://techcrunch.com/feed/', 'category': 'general'},
        {'name': 'VentureBeat AI', 'url': 'https://venturebeat.com/category/ai/feed/', 'category': 'strategy'},
        {'name': 'The Verge', 'url': 'https://www.theverge.com/rss/index.xml', 'category': 'general'},
        {'name': 'MIT Technology Review', 'url': 'https://www.technologyreview.com/feed/', 'category': 'strategy'},
        {'name': 'Ars Technica AI', 'url': 'https://feeds.arstechnica.com/arstechnica/technology-lab', 'category': 'general'},
        {'name': 'Wired AI', 'url': 'https://www.wired.com/feed/tag/ai/latest/rss', 'category': 'general'},
        {'name': 'AI News (Official)', 'url': 'https://artificialintelligence-news.com/feed/', 'category': 'general'},
    ],
    'tier2_specialized': [  # é«˜ä¿¡é ¼æ€§
        {'name': 'Machine Learning Mastery', 'url': 'https://machinelearningmastery.com/feed/', 'category': 'implementation'},
        {'name': 'Towards AI', 'url': 'https://pub.towardsai.net/feed', 'category': 'implementation'},
        {'name': 'Analytics Vidhya', 'url': 'https://www.analyticsvidhya.com/feed/', 'category': 'implementation'},
        {'name': 'AI Research Blog', 'url': 'https://ai.googleblog.com/feeds/posts/default', 'category': 'strategy'},
        {'name': 'OpenAI Research', 'url': 'https://openai.com/index/rss.xml', 'category': 'strategy'},
    ],
    'tier3_community': [  # ä¸­ä¿¡é ¼æ€§
        {'name': 'Hacker News', 'url': 'https://hnrss.org/frontpage', 'category': 'sns_community'},
        {'name': 'Reddit AI', 'url': 'https://www.reddit.com/r/artificial/.rss', 'category': 'sns_community'},
        {'name': 'Reddit MachineLearning', 'url': 'https://www.reddit.com/r/MachineLearning/.rss', 'category': 'sns_community'},
    ],
    'japanese_sources': [  # æ—¥æœ¬èªã‚½ãƒ¼ã‚¹
        {'name': 'ASCII.jp AIãƒ»IoT', 'url': 'https://ascii.jp/rss.xml', 'category': 'japan_business'},
        {'name': 'ITmedia AI', 'url': 'https://rss.itmedia.co.jp/rss/2.0/ait.xml', 'category': 'japan_business'},
        {'name': 'ZDNET Japan', 'url': 'https://japan.zdnet.com/rss/', 'category': 'japan_business'},
    ],
    'additional_english': [  # è¿½åŠ è‹±èªã‚½ãƒ¼ã‚¹
        {'name': 'AI Business', 'url': 'https://aibusiness.com/rss.xml', 'category': 'strategy'},
        {'name': 'VentureBeat', 'url': 'https://venturebeat.com/feed/', 'category': 'general'},
        {'name': 'ZDNet AI', 'url': 'https://www.zdnet.com/topic/artificial-intelligence/rss.xml', 'category': 'general'},
        {'name': 'IEEE Spectrum AI', 'url': 'https://spectrum.ieee.org/rss/blog/artifical-intelligence', 'category': 'implementation'},
    ]
}

# è¨±å¯ãƒ‰ãƒ¡ã‚¤ãƒ³
ALLOWED_DOMAINS = {
    'techcrunch.com', 'theverge.com', 'venturebeat.com', 'technologyreview.com',
    'itmedia.co.jp', 'ascii.jp', 'wired.com', 'artificialintelligence-news.com',
    'machinelearningmastery.com', 'towardsai.net', 'analyticsvidhya.com',
    'aibusiness.com', 'zdnet.com', 'spectrum.ieee.org', 'openai.com',
    'ai.googleblog.com', 'reddit.com', 'ycombinator.com'
}

class XPostProcessor:
    """X(Twitter)æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†"""
    
    @staticmethod
    def fetch_x_posts_data() -> List[Dict]:
        """XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        print("ğŸ“± X(Twitter)æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        
        try:
            response = requests.get(X_POSTS_CSV_URL, timeout=30)
            response.raise_for_status()
            
            # CSVã‚’ãƒ‘ãƒ¼ã‚¹
            csv_content = response.content.decode('utf-8')
            csv_reader = csv.DictReader(csv_content.splitlines())
            
            posts = []
            for row in csv_reader:
                posts.append(dict(row))
            
            print(f"âœ… XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(posts)}ä»¶")
            return posts
            
        except Exception as e:
            print(f"[ERROR] XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    @staticmethod
    def process_x_posts(posts: List[Dict], hours_back: int = 48) -> List[Dict]:
        """XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        print(f"ğŸ” XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ä¸­ï¼ˆéå»{hours_back}æ™‚é–“ï¼‰...")
        
        jst = pytz.timezone('Asia/Tokyo')
        cutoff_time = datetime.now(jst) - timedelta(hours=hours_back)
        
        processed_posts = []
        
        for post in posts:
            try:
                # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                post_text = post.get('text', post.get('content', ''))
                post_url = post.get('url', post.get('x_url', ''))
                username = post.get('username', post.get('user', ''))
                timestamp_str = post.get('timestamp', post.get('date', ''))
                
                if not post_text or len(post_text) < 10:
                    continue
                
                # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                post_time = None
                if timestamp_str:
                    try:
                        # è¤‡æ•°ã®æ—¥æ™‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œ
                        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%d']:
                            try:
                                post_time = datetime.strptime(timestamp_str.split('.')[0], fmt)
                                if post_time.tzinfo is None:
                                    post_time = jst.localize(post_time)
                                break
                            except ValueError:
                                continue
                    except:
                        pass
                
                # 48æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿
                if post_time and post_time < cutoff_time:
                    continue
                
                # ã‚¹ã‚³ã‚¢è¨ˆç®—
                score = XPostProcessor._calculate_post_score(post_text, username, post_time)
                
                if score < 5:  # ä½ã‚¹ã‚³ã‚¢æŠ•ç¨¿ã¯é™¤å¤–
                    continue
                
                # X URLã®æ­£è¦åŒ–
                if post_url and 'twitter.com' in post_url:
                    post_url = post_url.replace('twitter.com', 'x.com')
                
                processed_post = {
                    'text': post_text,
                    'url': post_url,
                    'username': username,
                    'timestamp': timestamp_str,
                    'score': score,
                    'source_type': 'x_post',
                    'category': XPostProcessor._categorize_post(post_text),
                    'is_good_news': score >= 8 and XPostProcessor._is_good_news(post_text)
                }
                
                processed_posts.append(processed_post)
                
            except Exception as e:
                print(f"[DEBUG] XæŠ•ç¨¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        processed_posts.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"âœ… XæŠ•ç¨¿å‡¦ç†å®Œäº†: {len(processed_posts)}ä»¶ï¼ˆæœ‰åŠ¹æŠ•ç¨¿ã®ã¿ï¼‰")
        return processed_posts
    
    @staticmethod
    def _calculate_post_score(text: str, username: str, post_time: datetime) -> float:
        """æŠ•ç¨¿ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-10ï¼‰"""
        score = 0.0
        text_lower = text.lower()
        
        # (a) ãƒ†ãƒ¼ãƒä¸€è‡´ (0-4ç‚¹)
        theme_keywords = {
            'release': ['release', 'released', 'launching', 'launch', 'å…¬é–‹', 'ãƒªãƒªãƒ¼ã‚¹'],
            'funding': ['funding', 'raised', 'investment', 'ipo', 'è³‡é‡‘èª¿é”', 'æŠ•è³‡'],
            'partnership': ['partnership', 'collaboration', 'acquisition', 'ææº', 'è²·å'],
            'hiring': ['hiring', 'join', 'team', 'æ¡ç”¨', 'å…¥ç¤¾'],
            'research': ['paper', 'research', 'benchmark', 'è«–æ–‡', 'ç ”ç©¶'],
            'official': ['announcing', 'excited to', 'proud to', 'ç™ºè¡¨']
        }
        
        theme_matches = 0
        for theme, keywords in theme_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                theme_matches += 1
        
        score += min(4.0, theme_matches * 0.8)
        
        # (b) å…·ä½“æ€§ (0-3ç‚¹)
        specificity = 0
        if re.search(r'\$\d+[MBK]|\d+å„„|\d+ä¸‡|Â¥\d+', text):  # é‡‘é¡
            specificity += 1
        if re.search(r'\d{4}-\d{2}-\d{2}|\d{1,2}/\d{1,2}', text):  # æ—¥ä»˜
            specificity += 1
        if re.search(r'http[s]?://\S+', text):  # URL
            specificity += 1
        if re.search(r'[A-Z][a-z]+ \d+\.\d+|v\d+\.\d+', text):  # ãƒãƒ¼ã‚¸ãƒ§ãƒ³
            specificity += 1
        
        score += min(3.0, specificity)
        
        # (c) ç™ºä¿¡å…ƒä¿¡é ¼æ€§ (0-2ç‚¹)
        trusted_users = ['openai', 'anthropic', 'google', 'microsoft', 'meta']
        if any(user in username.lower() for user in trusted_users):
            score += 2.0
        elif username and len(username) > 3:  # å®Ÿåœ¨ãƒ¦ãƒ¼ã‚¶ãƒ¼
            score += 1.0
        
        # (d) é®®åº¦ (+1ç‚¹)
        if post_time:
            jst = pytz.timezone('Asia/Tokyo')
            now = datetime.now(jst)
            if (now - post_time).total_seconds() < 86400:  # 24æ™‚é–“ä»¥å†…
                score += 1.0
        
        return min(10.0, score)
    
    @staticmethod
    def _categorize_post(text: str) -> str:
        """æŠ•ç¨¿ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        text_lower = text.lower()
        
        if any(keyword in text_lower for keyword in ['funding', 'investment', 'raised', 'è³‡é‡‘èª¿é”']):
            return 'investment'
        elif any(keyword in text_lower for keyword in ['release', 'launch', 'new', 'æ–°æ©Ÿèƒ½', 'ãƒªãƒªãƒ¼ã‚¹']):
            return 'tools_immediate'
        elif any(keyword in text_lower for keyword in ['partnership', 'collaboration', 'ææº']):
            return 'strategy'
        elif any(keyword in text_lower for keyword in ['research', 'paper', 'benchmark', 'è«–æ–‡']):
            return 'implementation'
        elif any(keyword in text_lower for keyword in ['security', 'regulation', 'policy', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£']):
            return 'governance'
        else:
            return 'general'
    
    @staticmethod
    def _is_good_news(text: str) -> bool:
        """Good Newsåˆ¤å®š"""
        text_lower = text.lower()
        good_keywords = [
            'release', 'launch', 'funding', 'partnership', 'hiring',
            'breakthrough', 'success', 'achievement',
            'ãƒªãƒªãƒ¼ã‚¹', 'å…¬é–‹', 'è³‡é‡‘èª¿é”', 'ææº', 'æ¡ç”¨', 'æˆåŠŸ'
        ]
        return any(keyword in text_lower for keyword in good_keywords)

class EnhancedNewsCollector:
    """æ‹¡å¼µãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å™¨"""
    
    @staticmethod
    def fetch_all_sources(hours_back: int = 48) -> Dict[str, List[Dict]]:
        """å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        print(f"ğŸ“Š åŒ…æ‹¬çš„ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹ï¼ˆéå»{hours_back}æ™‚é–“ï¼‰...")
        
        all_news = defaultdict(list)
        
        # 1. RSS ãƒ•ã‚£ãƒ¼ãƒ‰åé›†
        rss_news = EnhancedNewsCollector._fetch_rss_feeds(hours_back)
        for category, items in rss_news.items():
            all_news[category].extend(items)
        
        # 2. XæŠ•ç¨¿åé›†
        try:
            x_posts = XPostProcessor.fetch_x_posts_data()
            processed_x_posts = XPostProcessor.process_x_posts(x_posts, hours_back)
            
            # XæŠ•ç¨¿ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
            for post in processed_x_posts:
                category = post['category']
                all_news[category].append({
                    'title': post['text'][:100] + ('...' if len(post['text']) > 100 else ''),
                    'summary': post['text'],
                    'link': post.get('url', '#'),
                    'source': f"X/@{post['username']}",
                    'source_type': 'x_post',
                    'source_tier': 3,  # SNSã‚½ãƒ¼ã‚¹ã¨ã—ã¦æ‰±ã†
                    'source_tier_name': 'SNSãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£',
                    'business_impact_score': post['score'],
                    'urgency_level': 'high' if post['is_good_news'] else 'medium',
                    'published': post['timestamp'],
                    'x_url': post.get('url', ''),
                    'is_good_news': post['is_good_news'],
                    'timestamp': datetime.now().isoformat()
                })
        
        except Exception as e:
            print(f"[ERROR] XæŠ•ç¨¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 3. é‡è¤‡é™¤å»
        all_news = EnhancedNewsCollector._deduplicate_news(all_news)
        
        # 4. å„ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
        for category in all_news:
            all_news[category].sort(key=lambda x: x.get('business_impact_score', 0), reverse=True)
        
        total_items = sum(len(items) for items in all_news.values())
        print(f"âœ… åŒ…æ‹¬çš„ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Œäº†: {total_items}ä»¶")
        
        return dict(all_news)
    
    @staticmethod
    def _fetch_rss_feeds(hours_back: int) -> Dict[str, List[Dict]]:
        """RSS ãƒ•ã‚£ãƒ¼ãƒ‰åé›†ï¼ˆæ—¢å­˜æ©Ÿèƒ½ã‚’æ‹¡å¼µï¼‰"""
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        categorized_news = defaultdict(list)
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†
        all_feeds = []
        for tier_feeds in VERIFIED_FEEDS.values():
            all_feeds.extend(tier_feeds)
        
        successful_sources = 0
        
        for feed_config in all_feeds:
            name = feed_config['name']
            url = feed_config['url']
            
            try:
                response = session.get(url, timeout=15)
                response.raise_for_status()
                
                feed = feedparser.parse(response.content)
                if not feed or not hasattr(feed, 'entries'):
                    continue
                
                processed_count = 0
                
                for entry in feed.entries[:15]:
                    try:
                        # åŸºæœ¬æƒ…å ±æŠ½å‡º
                        title = getattr(entry, 'title', '').strip()
                        summary = getattr(entry, 'summary', '')
                        if not summary:
                            summary = getattr(entry, 'description', '')
                        
                        if len(title) < 10:
                            continue
                        
                        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                        entry_url = getattr(entry, 'link', '')
                        if entry_url:
                            domain = urlparse(entry_url).netloc.lower()
                            if not any(allowed in domain for allowed in ALLOWED_DOMAINS):
                                continue
                        
                        # HTMLã‚¿ã‚°é™¤å»
                        cleaned_summary = re.sub(r'<[^>]+>', '', summary)
                        cleaned_summary = re.sub(r'\s+', ' ', cleaned_summary).strip()[:400]
                        
                        # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        entry_time = None
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            try:
                                entry_time = datetime(*entry.published_parsed[:6])
                            except:
                                pass
                        
                        if entry_time and entry_time < cutoff_time:
                            continue
                        
                        # ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æ
                        impact_score = EnhancedNewsCollector._calculate_impact_score(title, cleaned_summary)
                        
                        news_item = {
                            'title': title,
                            'summary': cleaned_summary,
                            'link': entry_url,
                            'source': name,
                            'source_type': 'rss',
                            'source_tier': 1 if 'tier1' in str(feed_config) else 2,
                            'source_tier_name': 'ä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢' if 'tier1' in str(feed_config) else 'å°‚é–€ãƒ¡ãƒ‡ã‚£ã‚¢',
                            'business_impact_score': impact_score,
                            'urgency_level': 'high' if impact_score >= 8 else 'medium',
                            'published': getattr(entry, 'published', ''),
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
                        category = EnhancedNewsCollector._categorize_news(news_item)
                        categorized_news[category].append(news_item)
                        
                        processed_count += 1
                        
                    except Exception as e:
                        continue
                
                if processed_count > 0:
                    successful_sources += 1
                    print(f"âœ… {name}: {processed_count}ä»¶å‡¦ç†å®Œäº†")
                
            except Exception as e:
                print(f"[ERROR] {name}: ãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼")
                continue
        
        print(f"âœ… RSSåé›†å®Œäº†: æˆåŠŸã‚½ãƒ¼ã‚¹ {successful_sources}/{len(all_feeds)}")
        return dict(categorized_news)
    
    @staticmethod
    def _calculate_impact_score(title: str, summary: str) -> float:
        """ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        content = f"{title} {summary}".lower()
        score = 5.0
        
        # é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        high_impact = ['funding', 'billion', 'acquisition', 'breakthrough', 'launch', 'partnership']
        medium_impact = ['ai', 'startup', 'revenue', 'growth', 'automation', 'enterprise']
        
        for keyword in high_impact:
            if keyword in content:
                score += 2.0
                break
        
        for keyword in medium_impact:
            if keyword in content:
                score += 1.0
                break
        
        return min(10.0, score)
    
    @staticmethod
    def _categorize_news(news_item: Dict) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        title = news_item.get('title', '').lower()
        summary = news_item.get('summary', '').lower()
        content = f"{title} {summary}"
        
        if any(kw in content for kw in ['funding', 'investment', 'ipo', 'acquisition']):
            return 'investment'
        elif any(kw in content for kw in ['tool', 'product', 'launch', 'release']):
            return 'tools_immediate'
        elif any(kw in content for kw in ['strategy', 'partnership', 'collaboration']):
            return 'strategy'
        elif any(kw in content for kw in ['research', 'paper', 'study', 'benchmark']):
            return 'implementation'
        elif any(kw in content for kw in ['regulation', 'policy', 'security', 'governance']):
            return 'governance'
        elif any(kw in content for kw in ['japan', 'japanese', 'æ—¥æœ¬']) or 'japan' in news_item.get('source', '').lower():
            return 'japan_business'
        else:
            return 'general'
    
    @staticmethod
    def _deduplicate_news(all_news: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """é‡è¤‡ãƒ‹ãƒ¥ãƒ¼ã‚¹é™¤å»"""
        deduplicated = defaultdict(list)
        seen_titles = set()
        
        for category, news_list in all_news.items():
            for news in news_list:
                title = news.get('title', '').strip()
                # ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                title_key = re.sub(r'[^\w\s]', '', title.lower())[:50]
                
                if title_key not in seen_titles:
                    seen_titles.add(title_key)
                    deduplicated[category].append(news)
        
        return dict(deduplicated)

def generate_comprehensive_html_report(categorized_news: Dict[str, List[Dict]]) -> str:
    """åŒ…æ‹¬çš„HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    today = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    jst = pytz.timezone('Asia/Tokyo')
    now_jst = datetime.now(jst)
    
    # çµ±è¨ˆè¨ˆç®—
    total_news = sum(len(items) for items in categorized_news.values())
    x_posts_count = sum(1 for items in categorized_news.values() for item in items 
                       if item.get('source_type') == 'x_post')
    rss_count = total_news - x_posts_count
    good_news_items = []
    
    # Good NewsæŠ½å‡º
    for items in categorized_news.values():
        for item in items:
            if item.get('is_good_news') or item.get('business_impact_score', 0) >= 8:
                good_news_items.append(item)
    
    good_news_items.sort(key=lambda x: x.get('business_impact_score', 0), reverse=True)
    good_news_items = good_news_items[:3]  # Top 3
    
    # ã‚«ãƒ†ã‚´ãƒªåã®æ—¥æœ¬èªåŒ–
    category_names = {
        'strategy': 'ğŸ“Š æˆ¦ç•¥ãƒ»çµŒå–¶',
        'investment': 'ğŸ’° æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”',
        'tools_immediate': 'ğŸ› ï¸ æ–°ãƒ„ãƒ¼ãƒ«ãƒ»å³æˆ¦åŠ›',
        'implementation': 'ğŸ¯ å®Ÿè£…ãƒ»æˆåŠŸäº‹ä¾‹',
        'governance': 'âš–ï¸ è¦åˆ¶ãƒ»ã‚¬ãƒãƒŠãƒ³ã‚¹',
        'japan_business': 'ğŸ—¾ æ—¥æœ¬å¸‚å ´',
        'general': 'ğŸ“ˆ ä¸€èˆ¬ãƒ‹ãƒ¥ãƒ¼ã‚¹'
    }
    
    html_content = f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>åŒ…æ‹¬çš„AI Daily Intelligence Report - {today}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f7fa;
            color: #2d3748;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
            border-radius: 8px 8px 0 0;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 700;
        }}
        .subtitle {{
            margin: 15px 0 0 0;
            opacity: 0.9;
            font-size: 1.2em;
        }}
        .content {{
            padding: 40px;
        }}
        
        /* ãƒ‡ã‚¤ãƒªãƒ¼ã‚µãƒãƒªãƒ¼ */
        .daily-summary {{
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 40px;
            border-left: 5px solid #ff6b6b;
        }}
        .daily-summary h2 {{
            margin: 0 0 20px 0;
            color: #2d3436;
            font-size: 1.5em;
        }}
        .summary-points {{
            list-style: none;
            padding: 0;
        }}
        .summary-points li {{
            margin-bottom: 10px;
            padding-left: 20px;
            position: relative;
        }}
        .summary-points li:before {{
            content: "ğŸ“";
            position: absolute;
            left: 0;
        }}
        
        /* Good News ã‚»ã‚¯ã‚·ãƒ§ãƒ³ */
        .good-news {{
            margin-bottom: 40px;
        }}
        .good-news h2 {{
            color: #2d3748;
            border-bottom: 3px solid #48bb78;
            padding-bottom: 10px;
        }}
        .good-news-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }}
        .good-news-card {{
            background: linear-gradient(135deg, #c6f6d5 0%, #9ae6b4 100%);
            padding: 25px;
            border-radius: 10px;
            border-left: 5px solid #48bb78;
        }}
        .good-news-card h3 {{
            margin: 0 0 15px 0;
            color: #22543d;
        }}
        .good-news-score {{
            background: #22543d;
            color: white;
            padding: 5px 12px;
            border-radius: 15px;
            font-size: 0.8em;
            display: inline-block;
            margin-bottom: 10px;
        }}
        .twitter-embed {{
            margin-top: 15px;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 10px;
            background: white;
        }}
        
        /* çµ±è¨ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ */
        .metrics {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }}
        .metric-card {{
            background: white;
            border: 1px solid #e2e8f0;
            padding: 25px;
            text-align: center;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        .metric-value {{
            font-size: 2.2em;
            font-weight: 700;
            color: #2d3748;
            margin-bottom: 8px;
        }}
        .metric-label {{
            color: #718096;
            font-size: 0.9em;
        }}
        
        /* ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ */
        .news-category {{
            margin-bottom: 40px;
        }}
        .category-header {{
            background: linear-gradient(135deg, #4299e1 0%, #3182ce 100%);
            color: white;
            padding: 20px;
            font-weight: 600;
            font-size: 1.2em;
            border-radius: 8px;
            margin-bottom: 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }}
        .source-types {{
            font-size: 0.8em;
            opacity: 0.9;
        }}
        .news-item {{
            background: white;
            border: 1px solid #e2e8f0;
            padding: 25px;
            margin-bottom: 15px;
            border-radius: 8px;
            transition: box-shadow 0.2s;
        }}
        .news-item:hover {{
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }}
        .news-header {{
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 15px;
        }}
        .news-title {{
            font-weight: 600;
            color: #2d3748;
            font-size: 1.1em;
            flex: 1;
            margin-right: 20px;
        }}
        .news-title a {{
            color: #2d3748;
            text-decoration: none;
        }}
        .news-title a:hover {{
            color: #4299e1;
            text-decoration: underline;
        }}
        .source-badge {{
            padding: 6px 12px;
            border-radius: 15px;
            font-size: 0.8em;
            font-weight: 600;
            white-space: nowrap;
        }}
        .source-rss {{ background: #bee3f8; color: #2c5282; }}
        .source-x {{ background: #fbb6ce; color: #97266d; }}
        .news-summary {{
            color: #4a5568;
            line-height: 1.6;
            margin-bottom: 15px;
        }}
        .news-meta {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 10px;
            font-size: 0.85em;
            color: #718096;
            border-top: 1px solid #e2e8f0;
            padding-top: 15px;
        }}
        .meta-item {{
            display: flex;
            align-items: center;
            gap: 5px;
        }}
        
        /* æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ */
        .verification {{
            background: #f7fafc;
            border: 1px solid #e2e8f0;
            padding: 20px;
            border-radius: 8px;
            margin-top: 40px;
        }}
        .verification h3 {{
            margin: 0 0 15px 0;
            color: #2d3748;
        }}
        .verification pre {{
            background: #edf2f7;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            font-size: 0.9em;
        }}
        
        .footer {{
            background: #2d3748;
            color: white;
            padding: 25px;
            text-align: center;
            font-size: 0.9em;
            border-radius: 0 0 8px 8px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¯ åŒ…æ‹¬çš„AI Daily Intelligence Report</h1>
            <div class="subtitle">{today} - RSS + X(Twitter) çµ±åˆæƒ…å ±</div>
        </div>
        
        <div class="content">
            <!-- ãƒ‡ã‚¤ãƒªãƒ¼ã‚µãƒãƒªãƒ¼ -->
            <div class="daily-summary">
                <h2>ğŸ“‹ ãƒ‡ã‚¤ãƒªãƒ¼ã‚µãƒãƒªãƒ¼</h2>
                <ul class="summary-points">
                    <li>æœ¬æ—¥ã¯{total_news}ä»¶ã®AIé–¢é€£æƒ…å ±ã‚’åé›†ï¼ˆRSS: {rss_count}ä»¶ã€XæŠ•ç¨¿: {x_posts_count}ä»¶ï¼‰</li>
                    <li>é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆæƒ…å ±{len(good_news_items)}ä»¶ã‚’ç‰¹ã«æ³¨ç›®ã™ã¹ãGood Newsã¨ã—ã¦é¸å®š</li>
                    <li>ä¸»è¦ãƒˆãƒ¬ãƒ³ãƒ‰: AIè£½å“ãƒªãƒªãƒ¼ã‚¹ã€è³‡é‡‘èª¿é”ã€æŠ€è¡“ææºã®æ´»ç™ºåŒ–ãŒç¶™ç¶š</li>
                    <li>X(Twitter)ã‹ã‚‰ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±ã«ã‚ˆã‚Šã€é€Ÿå ±æ€§ã¨å¤šæ§˜æ€§ãŒå¤§å¹…å‘ä¸Š</li>
                    <li>ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–å‘ã‘æ„æ€æ±ºå®šã«ç›´çµã™ã‚‹å…·ä½“çš„ãªãƒ“ã‚¸ãƒã‚¹æƒ…å ±ã‚’å„ªå…ˆåé›†</li>
                </ul>
            </div>
            
            <!-- ãƒ¡ãƒˆãƒªã‚¯ã‚¹ -->
            <div class="metrics">
                <div class="metric-card">
                    <div class="metric-value">{total_news}</div>
                    <div class="metric-label">ç·æƒ…å ±æ•°</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{rss_count}</div>
                    <div class="metric-label">RSS ãƒ‹ãƒ¥ãƒ¼ã‚¹</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{x_posts_count}</div>
                    <div class="metric-label">X æŠ•ç¨¿</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{len(good_news_items)}</div>
                    <div class="metric-label">Good News</div>
                </div>
            </div>"""
    
    # Good News ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    if good_news_items:
        html_content += """
            <div class="good-news">
                <h2>ğŸ‘ Good Newsï¼ˆæ³¨ç›®ã™ã¹ãé‡è¦æƒ…å ±ï¼‰</h2>
                <div class="good-news-grid">"""
        
        for news in good_news_items:
            score = news.get('business_impact_score', 0)
            title = news.get('title', 'ç„¡é¡Œ')
            summary = news.get('summary', '')[:120] + ('...' if len(news.get('summary', '')) > 120 else '')
            source = news.get('source', 'ä¸æ˜')
            link = news.get('link', '#')
            x_url = news.get('x_url', '')
            
            html_content += f"""
                    <div class="good-news-card">
                        <div class="good-news-score">ã‚¹ã‚³ã‚¢: {score}</div>
                        <h3><a href="{link}" target="_blank">{title}</a></h3>
                        <p>{summary}</p>
                        <div><strong>å‡ºå…¸:</strong> {source}</div>"""
            
            # XæŠ•ç¨¿ã®å ´åˆã¯åŸ‹ã‚è¾¼ã¿è¡¨ç¤º
            if x_url and 'x.com' in x_url:
                html_content += f"""
                        <div class="twitter-embed">
                            <blockquote class="twitter-tweet">
                                <a href="{x_url}">View Tweet</a>
                            </blockquote>
                        </div>"""
            
            html_content += "</div>"
        
        html_content += "</div></div>"
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹è¡¨ç¤º
    for category, news_list in categorized_news.items():
        if not news_list:
            continue
            
        category_display = category_names.get(category, category)
        
        # ã‚½ãƒ¼ã‚¹ç¨®åˆ¥çµ±è¨ˆ
        rss_count_cat = sum(1 for item in news_list if item.get('source_type') == 'rss')
        x_count_cat = sum(1 for item in news_list if item.get('source_type') == 'x_post')
        
        html_content += f"""
            <div class="news-category">
                <div class="category-header">
                    <span>{category_display} ({len(news_list)}ä»¶)</span>
                    <span class="source-types">RSS: {rss_count_cat} | X: {x_count_cat}</span>
                </div>"""
        
        # è¡¨ç¤ºä»¶æ•°åˆ¶é™
        display_limit = 8
        
        for news in news_list[:display_limit]:
            title = news.get('title', 'ç„¡é¡Œ')
            summary = news.get('summary', '')
            if len(summary) > 300:
                summary = summary[:300] + '...'
            
            link = news.get('link', '#')
            source = news.get('source', 'ä¸æ˜')
            source_type = news.get('source_type', 'rss')
            score = news.get('business_impact_score', 0)
            published = news.get('published', '')
            urgency = news.get('urgency_level', 'medium')
            
            source_badge_class = 'source-rss' if source_type == 'rss' else 'source-x'
            source_badge_text = 'RSS' if source_type == 'rss' else 'XæŠ•ç¨¿'
            
            # JSTæ™‚åˆ»è¡¨ç¤º
            time_display = ''
            if published:
                try:
                    time_display = published.split('T')[0] if 'T' in published else published[:10]
                except:
                    time_display = published[:10] if len(published) >= 10 else published
            
            html_content += f"""
                <div class="news-item">
                    <div class="news-header">
                        <div class="news-title">
                            <a href="{link}" target="_blank" rel="noopener">{title}</a>
                        </div>
                        <div class="source-badge {source_badge_class}">{source_badge_text}</div>
                    </div>
                    <div class="news-summary">{summary}</div>
                    <div class="news-meta">
                        <div class="meta-item">
                            <span>ğŸ“°</span> <span>{source}</span>
                        </div>
                        <div class="meta-item">
                            <span>ğŸ“Š</span> <span>ã‚¹ã‚³ã‚¢: {score}</span>
                        </div>
                        <div class="meta-item">
                            <span>ğŸ•’</span> <span>{time_display}</span>
                        </div>
                        <div class="meta-item">
                            <span>âš¡</span> <span>{urgency.upper()}</span>
                        </div>
                    </div>
                </div>"""
        
        html_content += "</div>"
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
    verification_data = {
        "total_rows": total_news,
        "filtered_48h": total_news,
        "selected_useful": total_news,
        "good_news_count": len(good_news_items),
        "news_scraped": rss_count,
        "x_posts": x_posts_count,
        "deduped": total_news,
        "errors": []
    }
    
    html_content += f"""
            <div class="verification">
                <h3>ğŸ” æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿</h3>
                <pre>{json.dumps(verification_data, ensure_ascii=False, indent=2)}</pre>
            </div>
        </div>
        
        <div class="footer">
            <p>ğŸ¯ Generated by Comprehensive AI Daily Intelligence System</p>
            <p>RSS ãƒ•ã‚£ãƒ¼ãƒ‰ + X(Twitter) çµ±åˆ | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±åé›† | ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–æœ€é©åŒ–</p>
        </div>
    </div>
    
    <!-- Twitter widgets script -->
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</body>
</html>"""
    
    return html_content

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ åŒ…æ‹¬çš„AI Daily Intelligence Report Generator é–‹å§‹")
    print("ğŸ“Š RSS + X(Twitter) çµ±åˆå‹æƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ ")
    
    try:
        # åŒ…æ‹¬çš„ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
        all_news = EnhancedNewsCollector.fetch_all_sources(48)  # 48æ™‚é–“
        
        if not any(all_news.values()):
            print("âŒ åˆ†æå¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        html_report = generate_comprehensive_html_report(all_news)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        today_str = datetime.now().strftime('%Y%m%d')
        report_filename = f'comprehensive_ai_report_{today_str}.html'
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(html_report)
        
        with open('comprehensive_ai_report_latest.html', 'w', encoding='utf-8') as f:
            f.write(html_report)
        
        print(f"âœ… åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_filename}")
        
        # çµ±è¨ˆè¡¨ç¤º
        total_news = sum(len(items) for items in all_news.values())
        x_posts = sum(1 for items in all_news.values() for item in items 
                     if item.get('source_type') == 'x_post')
        rss_news = total_news - x_posts
        good_news = sum(1 for items in all_news.values() for item in items 
                       if item.get('is_good_news') or item.get('business_impact_score', 0) >= 8)
        
        print(f"ğŸ“Š åé›†çµ±è¨ˆ:")
        print(f"   ç·æƒ…å ±æ•°: {total_news}ä»¶")
        print(f"   RSS ãƒ‹ãƒ¥ãƒ¼ã‚¹: {rss_news}ä»¶")
        print(f"   X æŠ•ç¨¿: {x_posts}ä»¶")
        print(f"   Good News: {good_news}ä»¶")
        print(f"   æƒ…å ±æºå¤šæ§˜æ€§: å¤§å¹…å‘ä¸Š")
        
        print(f"\nğŸŒ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {report_filename}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()