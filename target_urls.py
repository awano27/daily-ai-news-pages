#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¾è±¡URLè¨­å®šã¨ãƒãƒƒãƒå‡¦ç†
"""

import sys
import os
from pathlib import Path

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # python-dotenvãŒãªã„å ´åˆã¯æ‰‹å‹•ã§.envã‚’èª­ã¿è¾¼ã¿
    env_path = os.path.join(os.path.dirname(__file__), '.env')
    if os.path.exists(env_path):
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from scrapers.beautifulsoup_scraper import BeautifulSoupScraper
from scrapers.gemini_extractor import GeminiExtractor

# å¯¾è±¡URLãƒªã‚¹ãƒˆï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰
TARGET_URLS = {
    "ai_news": [
        "https://techcrunch.com/category/artificial-intelligence/",
        "https://venturebeat.com/ai/",
        "https://www.theverge.com/ai-artificial-intelligence",
    ],
    
    "ai_research": [
        "https://openai.com/blog/",
        "https://blog.google/technology/ai/",
        "https://www.anthropic.com/news",
    ],
    
    "tech_news": [
        "https://news.ycombinator.com/",
        "https://techcrunch.com/",
        "https://www.producthunt.com/",
    ],
    
    "business": [
        "https://techcrunch.com/category/startups/",
        "https://www.crunchbase.com/",
        "https://pitchbook.com/news",
    ]
}

def scrape_category(category: str, max_urls: int = 3):
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
    if category not in TARGET_URLS:
        print(f"âŒ ã‚«ãƒ†ã‚´ãƒª '{category}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"åˆ©ç”¨å¯èƒ½: {list(TARGET_URLS.keys())}")
        return
    
    urls = TARGET_URLS[category][:max_urls]
    print(f"ðŸŽ¯ ã‚«ãƒ†ã‚´ãƒª '{category}' ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
    print(f"ðŸ“Š å¯¾è±¡URL: {len(urls)}ä»¶")
    
    scraper = BeautifulSoupScraper()
    extractor = GeminiExtractor()
    
    results = []
    
    for i, url in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}] {url}")
        
        try:
            # åŸºæœ¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            basic_result = scraper.scrape(url)
            
            if basic_result['success']:
                # AIè¦ç´„
                ai_result = extractor.extract(
                    basic_result['content'], 
                    "summary"
                )
                
                result = {
                    'url': url,
                    'title': basic_result['title'],
                    'content_length': len(basic_result['content']),
                    'ai_summary': ai_result.get('summary', 'AIè¦ç´„å¤±æ•—') if ai_result['success'] else 'AIè¦ç´„å¤±æ•—'
                }
                
                results.append(result)
                
                print(f"âœ… å®Œäº†: {basic_result['title']}")
                print(f"ðŸ“ è¦ç´„: {result['ai_summary'][:100]}...")
            else:
                print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {basic_result.get('error', 'ä¸æ˜Ž')}")
                
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    scraper.close()
    
    # çµæžœä¿å­˜
    import json
    from datetime import datetime
    
    output_file = f"results_{category}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… çµæžœä¿å­˜: {output_file}")
    print(f"ðŸ“Š æˆåŠŸ: {len(results)}ä»¶")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ðŸš€ å¯¾è±¡URLè¨­å®šãƒãƒƒãƒå‡¦ç†")
    print("\nåˆ©ç”¨å¯èƒ½ã‚«ãƒ†ã‚´ãƒª:")
    for category in TARGET_URLS.keys():
        print(f"  - {category}")
    
    if len(sys.argv) > 1:
        category = sys.argv[1]
        max_urls = int(sys.argv[2]) if len(sys.argv) > 2 else 3
        scrape_category(category, max_urls)
    else:
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python target_urls.py <category> [max_urls]")
        print("  ä¾‹: python target_urls.py ai_news 2")

if __name__ == "__main__":
    main()