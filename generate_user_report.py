#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
JSONãƒ‡ãƒ¼ã‚¿ã‚’åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªãƒ¬ãƒãƒ¼ãƒˆã«å¤‰æ›
"""

import json
import os
from datetime import datetime
from pathlib import Path

def generate_user_summary(analysis_file: str):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
    
    # åˆ†æçµæœèª­ã¿è¾¼ã¿
    with open(analysis_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±å–å¾—
    summary_file = analysis_file.replace('comprehensive_analysis_', 'analysis_summary_')
    if os.path.exists(summary_file):
        with open(summary_file, 'r', encoding='utf-8') as f:
            summary = json.load(f)
    else:
        summary = {}
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    timestamp = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')
    
    report = f"""
# ğŸš€ AIæ¥­ç•Œã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ
**ç”Ÿæˆæ—¥æ™‚**: {timestamp}

## ğŸ“Š å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼

- **ğŸ“ˆ ç·å‡¦ç†ä»¶æ•°**: {summary.get('total_processed', 0)}ä»¶
- **âœ… æˆåŠŸå–å¾—ä»¶æ•°**: {summary.get('total_successful', 0)}ä»¶  
- **ğŸ“Š æˆåŠŸç‡**: {summary.get('success_rate', '0%')}
- **ğŸ·ï¸ åˆ†æã‚«ãƒ†ã‚´ãƒªæ•°**: {len(data)}å€‹
- **ğŸ¤– AIåˆ†æã‚¿ã‚¤ãƒ—**: {', '.join(summary.get('analysis_types', []))}

---

## ğŸ¯ ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ

"""
    
    # ã‚«ãƒ†ã‚´ãƒªåã¨èª¬æ˜
    category_info = {
        'ai_breaking_news': {
            'name': 'ğŸ”¥ AIæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹',
            'description': 'TechCrunchã€VentureBeatç­‰ã‹ã‚‰ã®æœ€æ–°AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹'
        },
        'ai_research_labs': {
            'name': 'ğŸ§ª AIç ”ç©¶ãƒ©ãƒœ',
            'description': 'OpenAIã€Google AIã€Anthropicç­‰ã®ç ”ç©¶æ©Ÿé–¢ãƒ–ãƒ­ã‚°'
        },
        'business_startup': {
            'name': 'ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹ãƒ»ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—',
            'description': 'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—æŠ•è³‡ã€ä¼æ¥­å‹•å‘ã€ãƒ“ã‚¸ãƒã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰'
        },
        'tech_innovation': {
            'name': 'âš¡ æŠ€è¡“é©æ–°',
            'description': 'æœ€æ–°ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã€ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã€ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±'
        },
        'policy_regulation': {
            'name': 'ğŸ“œ æ”¿ç­–ãƒ»è¦åˆ¶',
            'description': 'AIé–¢é€£ã®æ”¿ç­–ã€è¦åˆ¶ã€æ³•çš„å‹•å‘'
        },
        'academic_research': {
            'name': 'ğŸ“ å­¦è¡“ç ”ç©¶',
            'description': 'arXivè«–æ–‡ã€å­¦è¡“æ©Ÿé–¢ã®ç ”ç©¶æˆæœ'
        }
    }
    
    # é‡è¦ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
    all_topics = []
    top_articles = []
    
    for category, articles in data.items():
        if not articles:
            continue
            
        cat_info = category_info.get(category, {'name': category, 'description': ''})
        report += f"""
### {cat_info['name']} ({len(articles)}ä»¶å–å¾—)
*{cat_info['description']}*

"""
        
        for i, article in enumerate(articles[:3], 1):  # ä¸Šä½3ä»¶ã®ã¿è¡¨ç¤º
            basic = article.get('basic', {})
            ai_analysis = article.get('ai_analysis', {})
            
            title = basic.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')[:80]
            url = basic.get('url', '#')
            
            # AIè¦ç´„å–å¾—
            summary_text = "è¦ç´„ãªã—"
            if 'summary' in ai_analysis and ai_analysis['summary'].get('success'):
                summary_data = ai_analysis['summary']
                if 'summary' in summary_data:
                    summary_text = summary_data['summary'][:150]
                elif 'raw_response' in summary_data:
                    summary_text = summary_data['raw_response'][:150]
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—
            keywords = []
            if 'keywords' in ai_analysis and ai_analysis['keywords'].get('success'):
                keywords_data = ai_analysis['keywords']
                if 'primary_keywords' in keywords_data:
                    keywords = keywords_data['primary_keywords'][:3]
            
            report += f"""
**{i}. {title}**
- ğŸ“ **è¦ç´„**: {summary_text}...
- ğŸ·ï¸ **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: {', '.join(keywords) if keywords else 'ãªã—'}
- ğŸ”— **URL**: {url}

"""
            
            # ãƒˆãƒƒãƒ—è¨˜äº‹åé›†ï¼ˆå¾Œã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨ï¼‰
            content_stats = article.get('content_stats', {})
            char_count = content_stats.get('character_count', 0)
            if char_count > 1000:  # å……å®Ÿã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                top_articles.append({
                    'title': title,
                    'summary': summary_text,
                    'keywords': keywords,
                    'category': cat_info['name'],
                    'url': url,
                    'char_count': char_count
                })
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
    report += """
---

## ğŸ” æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰

"""
    
    keyword_count = {}
    for category, articles in data.items():
        for article in articles:
            ai_analysis = article.get('ai_analysis', {})
            if 'keywords' in ai_analysis and ai_analysis['keywords'].get('success'):
                keywords_data = ai_analysis['keywords']
                if 'primary_keywords' in keywords_data:
                    for keyword in keywords_data['primary_keywords']:
                        keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
    
    # ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    top_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:10]
    
    for i, (keyword, count) in enumerate(top_keywords, 1):
        report += f"**{i}.** {keyword} ({count}å›è¨€åŠ)\n"
    
    # é‡è¦è¨˜äº‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    report += """

---

## ğŸ† æ³¨ç›®è¨˜äº‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„å……å®Ÿåº¦é †ï¼‰

"""
    
    top_articles.sort(key=lambda x: x['char_count'], reverse=True)
    
    for i, article in enumerate(top_articles[:5], 1):
        report += f"""
### {i}ä½: {article['title']}
- **ã‚«ãƒ†ã‚´ãƒª**: {article['category']}
- **è¦ç´„**: {article['summary']}...
- **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: {', '.join(article['keywords']) if article['keywords'] else 'ãªã—'}
- **æ–‡å­—æ•°**: {article['char_count']:,}æ–‡å­—
- **URL**: {article['url']}

"""
    
    # æŠ€è¡“å‹•å‘åˆ†æ
    report += """
---

## ğŸš€ ä»Šé€±ã®æŠ€è¡“å‹•å‘

### ğŸ¤– AIæŠ€è¡“é€²åŒ–
"""
    
    ai_topics = []
    for category, articles in data.items():
        for article in articles:
            title = article.get('basic', {}).get('title', '')
            if any(keyword in title.lower() for keyword in ['gpt', 'claude', 'gemini', 'llm', 'ai model']):
                ai_topics.append(title)
    
    for topic in ai_topics[:5]:
        report += f"- {topic}\n"
    
    report += """
### ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹ãƒ»æŠ•è³‡å‹•å‘
"""
    
    business_topics = []
    for category, articles in data.items():
        if category == 'business_startup':
            for article in articles:
                title = article.get('basic', {}).get('title', '')
                business_topics.append(title)
    
    for topic in business_topics[:5]:
        report += f"- {topic}\n"
    
    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
    report += """

---

## ğŸ“‹ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

### ğŸ¯ çŸ­æœŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆä»Šé€±ï¼‰
1. **AIæŠ€è¡“å‹•å‘ã®è¿½è·¡**: GPT-5ã€Claudeã€Geminiã®æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã‚’ç¢ºèª
2. **è¦åˆ¶å‹•å‘ã®æŠŠæ¡**: AIé–¢é€£ã®æ–°ã—ã„æ”¿ç­–ãƒ»è¦åˆ¶æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯
3. **ç«¶åˆåˆ†æ**: ä¸»è¦AIä¼æ¥­ã®æˆ¦ç•¥å¤‰æ›´ã‚’åˆ†æ

### ğŸš€ ä¸­é•·æœŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ¥æœˆï¼‰
1. **æŠ€è¡“å°å…¥æ¤œè¨**: æ–°ã—ã„AIãƒ„ãƒ¼ãƒ«ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®è©•ä¾¡
2. **äººææˆ¦ç•¥è¦‹ç›´ã—**: AIé–¢é€£ã‚¹ã‚­ãƒ«ã®ç¤¾å†…è‚²æˆè¨ˆç”»
3. **ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—**: AIä¼æ¥­ã¨ã®å”æ¥­æ©Ÿä¼šã®æ¢ç´¢

---

## ğŸ“ ãŠå•ã„åˆã‚ã›

ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã«é–¢ã™ã‚‹ã”è³ªå•ãƒ»ã”è¦æœ›ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã¾ã§ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚

**æ¬¡å›æ›´æ–°äºˆå®š**: 24æ™‚é–“å¾Œï¼ˆè‡ªå‹•å®Ÿè¡Œï¼‰

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ Free Scraping Platform ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*
"""
    
    return report

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    # æœ€æ–°ã®åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    analysis_files = list(Path('.').glob('comprehensive_analysis_*.json'))
    
    if not analysis_files:
        print("âŒ åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("å…ˆã« python run_comprehensive_analysis.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
    latest_file = max(analysis_files, key=lambda f: f.stat().st_mtime)
    print(f"ğŸ“Š åˆ†æãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = generate_user_summary(str(latest_file))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Markdownãƒ•ã‚¡ã‚¤ãƒ«
    md_file = f"user_summary_report_{timestamp}.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    txt_file = f"user_summary_report_{timestamp}.txt"
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†:")
    print(f"   ğŸ“„ Markdown: {md_file}")
    print(f"   ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆ: {txt_file}")
    
    # è¦ç´„è¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆã‚µãƒãƒªãƒ¼")
    print("="*60)
    print(report[:1000] + "...")
    print("\nè©³ç´°ã¯ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()