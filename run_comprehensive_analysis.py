#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒ…æ‹¬çš„æƒ…å ±å–å¾—ãƒ»åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…¨ã‚«ãƒ†ã‚´ãƒªã®æœ€æ–°æƒ…å ±ã‚’ä¸€æ‹¬å–å¾—ãƒ»AIåˆ†æ
"""

import sys
import os
import json
from pathlib import Path
from datetime import datetime
import time

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    env_path = os.path.join(os.path.dirname(__file__), '.env')
    if os.path.exists(env_path):
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from scrapers.beautifulsoup_scraper import BeautifulSoupScraper
from scrapers.gemini_extractor import GeminiExtractor

# åŒ…æ‹¬çš„å¯¾è±¡URLãƒªã‚¹ãƒˆ
COMPREHENSIVE_URLS = {
    "ai_breaking_news": [
        "https://techcrunch.com/category/artificial-intelligence/",
        "https://venturebeat.com/ai/",
        "https://www.theverge.com/ai-artificial-intelligence",
        "https://spectrum.ieee.org/topic/artificial-intelligence/",
        "https://www.wired.com/tag/artificial-intelligence/",
    ],
    
    "ai_research_labs": [
        "https://openai.com/blog/",
        "https://blog.google/technology/ai/",
        "https://www.anthropic.com/news",
        "https://deepmind.google/discover/blog/",
        "https://www.microsoft.com/en-us/research/research-area/artificial-intelligence/",
        "https://ai.meta.com/blog/",
        "https://blogs.nvidia.com/blog/category/deep-learning/",
    ],
    
    "business_startup": [
        "https://techcrunch.com/category/startups/",
        "https://www.crunchbase.com/",
        "https://pitchbook.com/news",
        "https://news.ycombinator.com/",
        "https://www.producthunt.com/",
        "https://techcrunch.com/category/venture/",
        "https://www.bloomberg.com/technology",
    ],
    
    "tech_innovation": [
        "https://techcrunch.com/",
        "https://arstechnica.com/",
        "https://www.technologyreview.com/",
        "https://www.engadget.com/",
        "https://thenextweb.com/",
        "https://www.zdnet.com/",
    ],
    
    "policy_regulation": [
        "https://www.politico.com/tech",
        "https://techcrunch.com/category/government-policy/",
        "https://www.reuters.com/technology/",
        "https://www.wsj.com/tech",
    ],
    
    "academic_research": [
        "https://arxiv.org/list/cs.AI/recent",
        "https://www.nature.com/subjects/machine-learning",
        "https://www.science.org/topic/computer-science",
        "https://distill.pub/",
    ]
}

def comprehensive_analysis(max_per_category: int = 5, analysis_types: list = None):
    """åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ"""
    if analysis_types is None:
        analysis_types = ["summary", "keywords", "analysis"]
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    print("ğŸš€ åŒ…æ‹¬çš„æƒ…å ±å–å¾—ãƒ»åˆ†æé–‹å§‹")
    print(f"ğŸ“Š å¯¾è±¡ã‚«ãƒ†ã‚´ãƒª: {len(COMPREHENSIVE_URLS)}å€‹")
    print(f"ğŸ¯ å„ã‚«ãƒ†ã‚´ãƒªæœ€å¤§: {max_per_category}ä»¶")
    print(f"ğŸ¤– AIåˆ†æã‚¿ã‚¤ãƒ—: {', '.join(analysis_types)}")
    print("=" * 60)
    
    scraper = BeautifulSoupScraper()
    extractor = GeminiExtractor()
    
    all_results = {}
    total_processed = 0
    total_successful = 0
    
    for category, urls in COMPREHENSIVE_URLS.items():
        print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒª: {category}")
        print(f"ğŸ¯ å¯¾è±¡URL: {min(len(urls), max_per_category)}ä»¶")
        
        category_results = []
        category_urls = urls[:max_per_category]
        
        for i, url in enumerate(category_urls, 1):
            print(f"\n[{i}/{len(category_urls)}] {url}")
            total_processed += 1
            
            try:
                # åŸºæœ¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                basic_result = scraper.scrape(url)
                
                if basic_result['success']:
                    print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {basic_result['title']}")
                    
                    # AIåˆ†æï¼ˆè¤‡æ•°ã‚¿ã‚¤ãƒ—ï¼‰
                    ai_results = {}
                    for analysis_type in analysis_types:
                        try:
                            ai_result = extractor.extract(
                                basic_result['content'], 
                                analysis_type
                            )
                            ai_results[analysis_type] = ai_result
                            print(f"ğŸ¤– {analysis_type}åˆ†æ: âœ…")
                        except Exception as e:
                            print(f"ğŸ¤– {analysis_type}åˆ†æ: âŒ {e}")
                            ai_results[analysis_type] = {'success': False, 'error': str(e)}
                    
                    # çµæœçµ±åˆ
                    result = {
                        'url': url,
                        'category': category,
                        'timestamp': timestamp,
                        'basic': basic_result,
                        'ai_analysis': ai_results,
                        'content_stats': {
                            'character_count': len(basic_result.get('content', '')),
                            'link_count': len(basic_result.get('links', [])),
                            'image_count': len(basic_result.get('images', [])),
                            'title': basic_result.get('title', 'N/A')
                        }
                    }
                    
                    category_results.append(result)
                    total_successful += 1
                    
                else:
                    print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {basic_result.get('error', 'ä¸æ˜')}")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                time.sleep(2)
                
            except Exception as e:
                print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        all_results[category] = category_results
        print(f"ğŸ“Š {category}: {len(category_results)}ä»¶æˆåŠŸ")
    
    scraper.close()
    
    # çµæœä¿å­˜
    output_file = f"comprehensive_analysis_{timestamp}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
    summary = {
        'timestamp': timestamp,
        'total_processed': total_processed,
        'total_successful': total_successful,
        'success_rate': f"{(total_successful/total_processed)*100:.1f}%" if total_processed > 0 else "0%",
        'categories': {cat: len(results) for cat, results in all_results.items()},
        'analysis_types': analysis_types,
        'output_file': output_file
    }
    
    summary_file = f"analysis_summary_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    # çµæœè¡¨ç¤º
    print("\n" + "=" * 60)
    print("ğŸ“‹ åŒ…æ‹¬çš„åˆ†æå®Œäº†")
    print("=" * 60)
    print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿: {total_processed}ä»¶")
    print(f"âœ… æˆåŠŸ: {total_successful}ä»¶")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']}")
    print(f"ğŸ’¾ è©³ç´°çµæœ: {output_file}")
    print(f"ğŸ“„ ã‚µãƒãƒªãƒ¼: {summary_file}")
    
    print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥æˆåŠŸä»¶æ•°:")
    for category, count in summary['categories'].items():
        print(f"   {category}: {count}ä»¶")
    
    return all_results, summary

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åŒ…æ‹¬çš„æƒ…å ±å–å¾—ãƒ»åˆ†æ")
    parser.add_argument('--max-per-category', type=int, default=5, help='å„ã‚«ãƒ†ã‚´ãƒªã®æœ€å¤§URLæ•°')
    parser.add_argument('--analysis-types', nargs='+', 
                       choices=['summary', 'keywords', 'structure', 'analysis'],
                       default=['summary', 'keywords', 'analysis'],
                       help='AIåˆ†æã‚¿ã‚¤ãƒ—')
    parser.add_argument('--quick', action='store_true', help='ã‚¯ã‚¤ãƒƒã‚¯å®Ÿè¡Œï¼ˆå„ã‚«ãƒ†ã‚´ãƒª2ä»¶ã€è¦ç´„ã®ã¿ï¼‰')
    
    args = parser.parse_args()
    
    if args.quick:
        print("âš¡ ã‚¯ã‚¤ãƒƒã‚¯å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰")
        comprehensive_analysis(max_per_category=2, analysis_types=['summary'])
    else:
        comprehensive_analysis(max_per_category=args.max_per_category, 
                             analysis_types=args.analysis_types)

if __name__ == "__main__":
    main()