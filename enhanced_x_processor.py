#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced X Posts Processor - é‡è¤‡é™¤å»ã¨è©³ç´°è¦ç´„ã®æ”¹å–„
"""
import os
import re
import csv
import io
import hashlib
from datetime import datetime, timezone, timedelta
from pathlib import Path
from urllib.parse import urlparse
import requests

# Gemini URL contextãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
try:
    from gemini_url_context import GeminiURLContextClient
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

def load_env():
    """ç’°å¢ƒå¤‰æ•°ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
    env_path = Path('.env')
    if env_path.exists():
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

class EnhancedXProcessor:
    def __init__(self):
        load_env()
        self.gemini_client = None
        if GEMINI_AVAILABLE and os.getenv("GEMINI_API_KEY"):
            try:
                self.gemini_client = GeminiURLContextClient()
                print("âœ… Gemini URL context client initialized for X post enhancement")
            except Exception as e:
                print(f"âš ï¸ Gemini client initialization failed: {e}")
                self.gemini_client = None
        
    def create_content_hash(self, text: str) -> str:
        """æŠ•ç¨¿å†…å®¹ã®ãƒãƒƒã‚·ãƒ¥ã‚’ä½œæˆï¼ˆé‡è¤‡æ¤œå‡ºç”¨ï¼‰"""
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–
        normalized = re.sub(r'\s+', ' ', text.lower().strip())
        # URLã‚„ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚’é™¤å»ã—ã¦ã‚³ã‚¢å†…å®¹ã‚’æŠ½å‡º
        normalized = re.sub(r'https?://\S+', '', normalized)
        normalized = re.sub(r'@\w+', '', normalized)
        normalized = re.sub(r'#\w+', '', normalized)
        
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()[:12]
    
    def is_similar_content(self, text1: str, text2: str, threshold: float = 0.7) -> bool:
        """2ã¤ã®æŠ•ç¨¿å†…å®¹ãŒé¡ä¼¼ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        def extract_keywords(text):
            # æ—¥æœ¬èªã¨è‹±èªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            words = set()
            # è‹±èªã®å˜èª
            english_words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
            words.update(english_words)
            # æ—¥æœ¬èªã®å˜èªï¼ˆç°¡æ˜“ç‰ˆï¼‰
            japanese_words = re.findall(r'[ã-ã‚Ÿä¸€-é¾¯]{2,}', text)
            words.update(japanese_words)
            return words
        
        keywords1 = extract_keywords(text1)
        keywords2 = extract_keywords(text2)
        
        if not keywords1 or not keywords2:
            return False
        
        intersection = len(keywords1 & keywords2)
        union = len(keywords1 | keywords2)
        
        similarity = intersection / union if union > 0 else 0
        return similarity > threshold
    
    def enhance_post_with_gemini(self, post_data: dict) -> dict:
        """Gemini URL contextã‚’ä½¿ã£ã¦æŠ•ç¨¿ã‚’å¼·åŒ–"""
        if not self.gemini_client:
            return post_data
        
        try:
            # æŠ•ç¨¿URLãŒã‚ã‚‹å ´åˆã¯ã€ãã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æ
            post_url = post_data.get('url', '')
            original_text = post_data.get('text', '')
            
            if post_url and original_text:
                prompt = f"""
                ä»¥ä¸‹ã®XæŠ•ç¨¿ã®å†…å®¹ã‚’åˆ†æã—ã€300æ–‡å­—ä»¥å†…ã®ç°¡æ½”ãªè¦ç´„ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ï¼š

                æŠ•ç¨¿å†…å®¹: {original_text}

                ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
                ## è¦ç´„
                æŠ•ç¨¿ã®æ ¸å¿ƒçš„ãªå†…å®¹ã‚’200æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«è¦ç´„ï¼ˆæ”¹è¡Œãªã—ï¼‰

                ## ã‚«ãƒ†ã‚´ãƒª
                [AIæŠ€è¡“/ãƒ“ã‚¸ãƒã‚¹/é–‹ç™ºãƒ„ãƒ¼ãƒ«/ãã®ä»–]ã®ã„ãšã‚Œã‹

                ## é‡è¦åº¦
                [é«˜/ä¸­/ä½]

                è¦ç´„ã¯å¿…ãš300æ–‡å­—ä»¥å†…ã§ã€ç°¡æ½”ã§èª­ã¿ã‚„ã™ãã—ã¦ãã ã•ã„ã€‚
                """
                
                result = self.gemini_client.generate_from_urls(
                    prompt=prompt,
                    urls=[post_url] if post_url.startswith('http') else [],
                    enable_search=False
                )
                
                if result.get('text') and 'error' not in result:
                    # Geminiåˆ†æçµæœã‚’ãƒ‘ãƒ¼ã‚¹
                    analysis = result['text']
                    
                    # è¦ç´„éƒ¨åˆ†ã‚’æŠ½å‡º
                    summary_match = re.search(r'## è¦ç´„\s*\n(.+?)(?=\n##|\n$|$)', analysis, re.DOTALL)
                    if summary_match:
                        enhanced_summary = summary_match.group(1).strip()
                        # 300æ–‡å­—åˆ¶é™ã‚’é©ç”¨
                        if len(enhanced_summary) > 300:
                            enhanced_summary = enhanced_summary[:300] + '...'
                        # æ”¹è¡Œã‚’å‰Šé™¤ã—ã¦ä¸€è¡Œã«ã¾ã¨ã‚ã‚‹
                        enhanced_summary = re.sub(r'\s+', ' ', enhanced_summary).strip()
                        post_data['_enhanced_summary'] = enhanced_summary
                        post_data['_gemini_enhanced'] = True
                    
                    # ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º
                    category_match = re.search(r'## ã‚«ãƒ†ã‚´ãƒª\s*\n(.+?)(?=\n##|\n$|$)', analysis, re.DOTALL)
                    if category_match:
                        category = category_match.group(1).strip()
                        post_data['_category'] = category
                    
                    # é‡è¦åº¦ã‚’æŠ½å‡º
                    importance_match = re.search(r'## é‡è¦åº¦\s*\n(.+?)(?=\n##|\n$|$)', analysis, re.DOTALL)
                    if importance_match:
                        importance = importance_match.group(1).strip()
                        post_data['_importance'] = importance
                
        except Exception as e:
            print(f"âš ï¸ Gemini enhancement failed for post: {e}")
        
        return post_data
    
    def process_x_posts(self, csv_url: str, max_posts: int = 50) -> list:
        """XæŠ•ç¨¿ã‚’å‡¦ç†ã—ã¦é‡è¤‡é™¤å»ã¨è¦ç´„å¼·åŒ–ã‚’å®Ÿè¡Œ"""
        print(f"ğŸ”„ Processing X posts from: {csv_url}")
        
        try:
            # CSV ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
            response = requests.get(csv_url, timeout=30)
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«UTF-8ã«è¨­å®š
            response.encoding = 'utf-8'
            
            # CSV ã‚’ãƒ‘ãƒ¼ã‚¹  
            content = response.text
            
            # ã¾ãšé€šå¸¸ã®CSVãƒªãƒ¼ãƒ€ãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
            lines = content.strip().split('\n')
            print(f"[DEBUG] First 3 CSV lines:")
            for i, line in enumerate(lines[:3], 1):
                print(f"   {i}: {line}")
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã®CSVã¨ã—ã¦å‡¦ç†
            reader = csv.reader(io.StringIO(content))
            
            # CSVåˆ—åã‚’æ‰‹å‹•è¨­å®š
            expected_columns = ['Date', 'Username', 'Tweet Text', 'Media URL', 'Tweet URL']
            print(f"[DEBUG] Expected columns: {expected_columns}")
            
            posts = []
            seen_hashes = set()
            seen_texts = []
            processed_count = 0
            total_rows = 0
            
            for row in reader:
                total_rows += 1
                if processed_count >= max_posts:
                    break
                
                # è¡Œã«ååˆ†ãªåˆ—ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                if len(row) < 5:
                    print(f"[DEBUG] Skipping row {total_rows}: insufficient columns ({len(row)})")
                    continue
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                date_str = row[0].strip('"').strip() if len(row) > 0 else ''
                username = row[1].strip('"').strip() if len(row) > 1 else ''
                text = row[2].strip('"').strip() if len(row) > 2 else ''
                media_url = row[3].strip('"').strip() if len(row) > 3 else ''
                post_url = row[4].strip('"').strip() if len(row) > 4 else ''
                
                # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                import html
                text = html.unescape(text)
                username = html.unescape(username)
                
                # è¿½åŠ ã®æ–‡å­—åŒ–ã‘å¯¾ç­–
                # å…¨è§’æ–‡å­—ã®æ­£è¦åŒ–
                import unicodedata
                text = unicodedata.normalize('NFKC', text)
                username = unicodedata.normalize('NFKC', username)
                
                # ä¸æ­£ãªæ–‡å­—ã‚„åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
                text = ''.join(char for char in text if char.isprintable() or char in '\n\r\t')
                
                # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’æ­£è¦åŒ–
                import re
                text = re.sub(r'\s+', ' ', text).strip()
                
                print(f"[DEBUG] Row {total_rows}: date={date_str[:20]}..., user={username}, text_len={len(text)}")
                
                if not text or not username or len(text.strip()) < 5:
                    print(f"[DEBUG] Skipping row {total_rows}: invalid data")
                    continue
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯1: ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹
                content_hash = self.create_content_hash(text)
                if content_hash in seen_hashes:
                    print(f"[DEBUG] Skipping duplicate hash: {username}")
                    continue
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯2: é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                is_similar = any(self.is_similar_content(text, seen_text) for seen_text in seen_texts)
                if is_similar:
                    print(f"[DEBUG] Skipping similar content: {username}")
                    continue
                
                # åŸºæœ¬çš„ãªæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                post_data = {
                    'username': username.replace('@', ''),
                    'text': text,
                    'url': post_url,
                    'date': date_str,
                    '_content_hash': content_hash,
                    '_gemini_enhanced': False
                }
                
                # Geminiã§å¼·åŒ–
                post_data = self.enhance_post_with_gemini(post_data)
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚»ãƒƒãƒˆã«è¿½åŠ 
                seen_hashes.add(content_hash)
                seen_texts.append(text)
                
                posts.append(post_data)
                processed_count += 1
                
                print(f"[INFO] Processed post {processed_count}: {username}")
            
            print(f"ğŸ“Š Processing summary:")
            print(f"   Total CSV rows: {total_rows}")
            print(f"   Valid posts processed: {processed_count}")
            print(f"   Final unique posts: {len(posts)}")
            print(f"âœ… Processed {len(posts)} unique X posts")
            return posts
            
        except Exception as e:
            print(f"âŒ Failed to process X posts: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def convert_to_build_format(self, posts: list) -> list:
        """build.pyã®å½¢å¼ã«å¤‰æ›"""
        items = []
        
        for post in posts:
            username = post['username']
            text = post['text']
            url = post['url']
            
            # å¼·åŒ–ã•ã‚ŒãŸè¦ç´„ãŒã‚ã‚Œã°ä½¿ç”¨ã€ãªã‘ã‚Œã°å¾“æ¥ã®æ–¹æ³•
            if post.get('_enhanced_summary'):
                summary = post['_enhanced_summary']
                title = f"ğŸ§  {username} - {post.get('_category', 'AIé–¢é€£')}"
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹æ³•ï¼ˆ300æ–‡å­—åˆ¶é™ï¼‰
                if len(text) > 300:
                    summary = text[:300] + '...'
                else:
                    summary = text
                title = f"Xãƒã‚¹ãƒˆ {username}"
            
            # é‡è¦åº¦ã«åŸºã¥ããƒ—ãƒ©ã‚¤ã‚ªãƒªãƒ†ã‚£
            priority = 0
            if post.get('_importance') == 'é«˜':
                priority = 3
            elif post.get('_importance') == 'ä¸­':
                priority = 2
            else:
                priority = 1
            
            # æœ€çµ‚çš„ãªè¦ç´„ã®æ–‡å­—æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(summary) > 300:
                summary = summary[:300] + '...'
            
            item = {
                "title": title,
                "link": url,
                "_summary": summary,
                "_full_text": text,
                "_source": "X / SNS (Enhanced)",
                "_dt": datetime.now(timezone.utc),
                "_enhanced": post.get('_gemini_enhanced', False),
                "_priority": priority,
                "_category": post.get('_category', ''),
                "_content_hash": post.get('_content_hash', '')
            }
            
            items.append(item)
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜é‡è¦åº¦ãŒå…ˆé ­ï¼‰
        items.sort(key=lambda x: x['_priority'], reverse=True)
        
        return items

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    processor = EnhancedXProcessor()
    
    # Google Sheets URL
    csv_url = "https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0"
    
    # XæŠ•ç¨¿ã‚’å‡¦ç†
    posts = processor.process_x_posts(csv_url, max_posts=10)
    
    if posts:
        print(f"\nğŸ“Š å‡¦ç†çµæœ: {len(posts)}ä»¶ã®æŠ•ç¨¿")
        
        # build.pyå½¢å¼ã«å¤‰æ›
        build_items = processor.convert_to_build_format(posts)
        
        print("\nğŸ“ å¤‰æ›ã•ã‚ŒãŸæŠ•ç¨¿ä¾‹:")
        for i, item in enumerate(build_items[:3], 1):
            print(f"\n{i}. {item['title']}")
            print(f"   è¦ç´„: {item['_summary'][:100]}...")
            print(f"   å¼·åŒ–æ¸ˆã¿: {'âœ…' if item['_enhanced'] else 'âŒ'}")
            print(f"   é‡è¦åº¦: {item['_priority']}")
    else:
        print("âŒ æŠ•ç¨¿ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()