#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒ…æ‹¬çš„AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆpandasä¾å­˜é™¤å»ç‰ˆï¼‰
RSS + X(Twitter) + è¿½åŠ ã‚½ãƒ¼ã‚¹çµ±åˆç‰ˆ
"""

import os
import sys
import json
import csv
import io
import feedparser
import requests
from datetime import datetime, timedelta
from collections import defaultdict
from typing import List, Dict, Any, Tuple
import google.generativeai as genai
import re
import warnings
from urllib.parse import urlparse

# è­¦å‘Šç„¡åŠ¹åŒ–
warnings.filterwarnings('ignore')

# Gemini APIè¨­å®š
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if GEMINI_API_KEY and not os.getenv('DISABLE_GEMINI'):
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp')

# XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
X_POSTS_CSV_URL = os.getenv('X_POSTS_CSV', 'https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0')

# å‹•ä½œç¢ºèªæ¸ˆã¿ã®ä¿¡é ¼æ€§é«˜ãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆæ‹¡å¼µç‰ˆï¼‰
VERIFIED_FEEDS = {
    'tier1_official': [  # æœ€é«˜ä¿¡é ¼æ€§
        {'name': 'TechCrunch', 'url': 'https://techcrunch.com/feed/', 'category': 'general'},
        {'name': 'VentureBeat AI', 'url': 'https://venturebeat.com/category/ai/feed/', 'category': 'strategy'},
        {'name': 'The Verge', 'url': 'https://www.theverge.com/rss/index.xml', 'category': 'general'},
        {'name': 'MIT Technology Review', 'url': 'https://www.technologyreview.com/feed/', 'category': 'strategy'},
        {'name': 'Ars Technica AI', 'url': 'https://feeds.arstechnica.com/arstechnica/technology-lab', 'category': 'general'},
    ],
    'tier2_specialized': [  # é«˜ä¿¡é ¼æ€§
        {'name': 'AI News', 'url': 'https://artificialintelligence-news.com/feed/', 'category': 'general'},
        {'name': 'Machine Learning Mastery', 'url': 'https://machinelearningmastery.com/feed/', 'category': 'implementation'},
        {'name': 'Towards AI', 'url': 'https://pub.towardsai.net/feed', 'category': 'implementation'},
        {'name': 'Analytics Vidhya', 'url': 'https://www.analyticsvidhya.com/feed/', 'category': 'implementation'},
    ],
    'tier3_community': [  # ä¸­ä¿¡é ¼æ€§
        {'name': 'Hacker News', 'url': 'https://hnrss.org/frontpage', 'category': 'sns_community'},
        {'name': 'Reddit AI', 'url': 'https://www.reddit.com/r/artificial/.rss', 'category': 'sns_community'},
        {'name': 'Reddit MachineLearning', 'url': 'https://www.reddit.com/r/MachineLearning/.rss', 'category': 'sns_community'},
    ],
    'japanese_sources': [  # æ—¥æœ¬èªã‚½ãƒ¼ã‚¹
        {'name': 'ASCII.jp AIãƒ»IoT', 'url': 'https://ascii.jp/rss.xml', 'category': 'japan_business'},
        {'name': 'ITmedia AI', 'url': 'https://rss.itmedia.co.jp/rss/2.0/ait.xml', 'category': 'japan_business'},
        {'name': 'ZDNET Japan', 'url': 'https://japan.zdnet.com/rss/', 'category': 'japan_business'},
    ]
}

# è¨±å¯ãƒ‰ãƒ¡ã‚¤ãƒ³
ALLOWED_DOMAINS = {
    'techcrunch.com', 'theverge.com', 'venturebeat.com', 'technologyreview.com',
    'itmedia.co.jp', 'ascii.jp', 'artificialintelligence-news.com',
    'machinelearningmastery.com', 'towardsai.net', 'analyticsvidhya.com',
    'reddit.com', 'ycombinator.com', 'arstechnica.com'
}

class XPostProcessor:
    """X(Twitter)æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†"""
    
    @staticmethod
    def fetch_x_posts_data() -> List[Dict]:
        """XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        print("ğŸ“± X(Twitter)æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        
        try:
            response = requests.get(X_POSTS_CSV_URL, timeout=30)
            response.raise_for_status()
            
            # CSVã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆpandasä¸ä½¿ç”¨ï¼‰
            csv_content = response.content.decode('utf-8-sig')  # BOMå¯¾å¿œ
            csv_reader = csv.DictReader(io.StringIO(csv_content))
            
            posts = []
            for row in csv_reader:
                # ç©ºè¡Œã‚¹ã‚­ãƒƒãƒ—
                if not any(row.values()):
                    continue
                posts.append(dict(row))
            
            print(f"âœ… XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(posts)}ä»¶")
            return posts
            
        except Exception as e:
            print(f"[ERROR] XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    @staticmethod
    def process_x_posts(posts: List[Dict], hours_back: int = 48) -> List[Dict]:
        """XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        print(f"ğŸ” XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ä¸­ï¼ˆéå»{hours_back}æ™‚é–“ï¼‰...")
        
        # ç¾åœ¨æ™‚åˆ»ï¼ˆJSTæƒ³å®šï¼‰
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        
        processed_posts = []
        
        for post in posts:
            try:
                # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆè¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                post_text = ''
                for text_field in ['text', 'content', 'tweet_text', 'message', 'post']:
                    if text_field in post and post[text_field]:
                        post_text = post[text_field].strip()
                        break
                
                post_url = ''
                for url_field in ['url', 'x_url', 'tweet_url', 'link']:
                    if url_field in post and post[url_field]:
                        post_url = post[url_field].strip()
                        break
                
                username = ''
                for user_field in ['username', 'user', 'author', 'screen_name']:
                    if user_field in post and post[user_field]:
                        username = post[user_field].strip()
                        break
                
                timestamp_str = ''
                for time_field in ['timestamp', 'date', 'created_at', 'time']:
                    if time_field in post and post[time_field]:
                        timestamp_str = post[time_field].strip()
                        break
                
                # åŸºæœ¬æ¤œè¨¼
                if not post_text or len(post_text) < 10:
                    continue
                
                # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                post_time = None
                if timestamp_str:
                    try:
                        # è¤‡æ•°ã®æ—¥æ™‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œ
                        formats = [
                            '%Y-%m-%d %H:%M:%S',
                            '%Y-%m-%dT%H:%M:%S',
                            '%Y-%m-%d',
                            '%m/%d/%Y %H:%M:%S',
                            '%m/%d/%Y',
                            '%d/%m/%Y %H:%M:%S',
                            '%d/%m/%Y'
                        ]
                        
                        for fmt in formats:
                            try:
                                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚„ç§’æ•°ä»¥ä¸‹ã‚’é™¤å»
                                clean_timestamp = timestamp_str.split('.')[0].split('+')[0].split('Z')[0]
                                post_time = datetime.strptime(clean_timestamp, fmt)
                                break
                            except ValueError:
                                continue
                    except:
                        pass
                
                # 48æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒãªã„å ´åˆã¯å«ã‚ã‚‹ï¼‰
                if post_time and post_time < cutoff_time:
                    continue
                
                # ã‚¹ã‚³ã‚¢è¨ˆç®—
                score = XPostProcessor._calculate_post_score(post_text, username, post_time)
                
                if score < 4:  # ä½ã‚¹ã‚³ã‚¢æŠ•ç¨¿ã¯é™¤å¤–ï¼ˆé–¾å€¤ã‚’ä¸‹ã’ã‚‹ï¼‰
                    continue
                
                # X URLã®æ­£è¦åŒ–
                if post_url and 'twitter.com' in post_url:
                    post_url = post_url.replace('twitter.com', 'x.com')
                
                processed_post = {
                    'text': post_text,
                    'url': post_url,
                    'username': username,
                    'timestamp': timestamp_str,
                    'score': score,
                    'source_type': 'x_post',
                    'category': XPostProcessor._categorize_post(post_text),
                    'is_good_news': score >= 7 and XPostProcessor._is_good_news(post_text)
                }
                
                processed_posts.append(processed_post)
                
            except Exception as e:
                print(f"[DEBUG] XæŠ•ç¨¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        processed_posts.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"âœ… XæŠ•ç¨¿å‡¦ç†å®Œäº†: {len(processed_posts)}ä»¶ï¼ˆæœ‰åŠ¹æŠ•ç¨¿ã®ã¿ï¼‰")
        return processed_posts
    
    @staticmethod
    def _calculate_post_score(text: str, username: str, post_time: datetime) -> float:
        """æŠ•ç¨¿ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-10ï¼‰"""
        score = 0.0
        text_lower = text.lower()
        
        # (a) ãƒ†ãƒ¼ãƒä¸€è‡´ (0-4ç‚¹)
        theme_keywords = {
            'release': ['release', 'released', 'launching', 'launch', 'å…¬é–‹', 'ãƒªãƒªãƒ¼ã‚¹'],
            'funding': ['funding', 'raised', 'investment', 'ipo', 'è³‡é‡‘èª¿é”', 'æŠ•è³‡'],
            'partnership': ['partnership', 'collaboration', 'acquisition', 'ææº', 'è²·å'],
            'hiring': ['hiring', 'join', 'team', 'æ¡ç”¨', 'å…¥ç¤¾'],
            'research': ['paper', 'research', 'benchmark', 'è«–æ–‡', 'ç ”ç©¶'],
            'official': ['announcing', 'excited to', 'proud to', 'ç™ºè¡¨']
        }
        
        theme_matches = 0
        for theme, keywords in theme_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                theme_matches += 1
        
        score += min(4.0, theme_matches * 0.7)
        
        # (b) å…·ä½“æ€§ (0-3ç‚¹)
        specificity = 0
        if re.search(r'\$\d+[MBK]|\d+å„„|\d+ä¸‡|Â¥\d+', text):  # é‡‘é¡
            specificity += 1
        if re.search(r'\d{4}-\d{2}-\d{2}|\d{1,2}/\d{1,2}', text):  # æ—¥ä»˜
            specificity += 1
        if re.search(r'http[s]?://\S+', text):  # URL
            specificity += 1
        if re.search(r'[A-Z][a-z]+ \d+\.\d+|v\d+\.\d+', text):  # ãƒãƒ¼ã‚¸ãƒ§ãƒ³
            specificity += 1
        
        score += min(3.0, specificity * 0.8)
        
        # (c) ç™ºä¿¡å…ƒä¿¡é ¼æ€§ (0-2ç‚¹)
        if username:
            trusted_users = ['openai', 'anthropic', 'google', 'microsoft', 'meta', 'ai', 'tech']
            if any(user in username.lower() for user in trusted_users):
                score += 2.0
            elif len(username) > 3:  # å®Ÿåœ¨ãƒ¦ãƒ¼ã‚¶ãƒ¼
                score += 1.0
        
        # (d) é®®åº¦ (+1ç‚¹)
        if post_time:
            now = datetime.now()
            if (now - post_time).total_seconds() < 86400:  # 24æ™‚é–“ä»¥å†…
                score += 1.0
        else:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒãªã„å ´åˆã¯ç¾åœ¨ã¨ã—ã¦æ‰±ã†
            score += 0.5
        
        return min(10.0, score)
    
    @staticmethod
    def _categorize_post(text: str) -> str:
        """æŠ•ç¨¿ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        text_lower = text.lower()
        
        if any(keyword in text_lower for keyword in ['funding', 'investment', 'raised', 'è³‡é‡‘èª¿é”']):
            return 'investment'
        elif any(keyword in text_lower for keyword in ['release', 'launch', 'new', 'æ–°æ©Ÿèƒ½', 'ãƒªãƒªãƒ¼ã‚¹']):
            return 'tools_immediate'
        elif any(keyword in text_lower for keyword in ['partnership', 'collaboration', 'ææº']):
            return 'strategy'
        elif any(keyword in text_lower for keyword in ['research', 'paper', 'benchmark', 'è«–æ–‡']):
            return 'implementation'
        elif any(keyword in text_lower for keyword in ['security', 'regulation', 'policy', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£']):
            return 'governance'
        else:
            return 'general'
    
    @staticmethod
    def _is_good_news(text: str) -> bool:
        """Good Newsåˆ¤å®š"""
        text_lower = text.lower()
        good_keywords = [
            'release', 'launch', 'funding', 'partnership', 'hiring',
            'breakthrough', 'success', 'achievement', 'milestone',
            'ãƒªãƒªãƒ¼ã‚¹', 'å…¬é–‹', 'è³‡é‡‘èª¿é”', 'ææº', 'æ¡ç”¨', 'æˆåŠŸ'
        ]
        return any(keyword in text_lower for keyword in good_keywords)

class EnhancedNewsCollector:
    """æ‹¡å¼µãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å™¨"""
    
    @staticmethod
    def fetch_all_sources(hours_back: int = 48) -> Dict[str, List[Dict]]:
        """å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        print(f"ğŸ“Š åŒ…æ‹¬çš„ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹ï¼ˆéå»{hours_back}æ™‚é–“ï¼‰...")
        
        all_news = defaultdict(list)
        
        # 1. RSS ãƒ•ã‚£ãƒ¼ãƒ‰åé›†
        rss_news = EnhancedNewsCollector._fetch_rss_feeds(hours_back)
        for category, items in rss_news.items():
            all_news[category].extend(items)
        
        # 2. XæŠ•ç¨¿åé›†
        try:
            x_posts = XPostProcessor.fetch_x_posts_data()
            if x_posts:
                processed_x_posts = XPostProcessor.process_x_posts(x_posts, hours_back)
                
                # XæŠ•ç¨¿ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
                for post in processed_x_posts:
                    category = post['category']
                    all_news[category].append({
                        'title': post['text'][:100] + ('...' if len(post['text']) > 100 else ''),
                        'summary': post['text'],
                        'link': post.get('url', '#'),
                        'source': f"X/@{post['username']}" if post['username'] else "X/åŒ¿å",
                        'source_type': 'x_post',
                        'source_tier': 3,  # SNSã‚½ãƒ¼ã‚¹ã¨ã—ã¦æ‰±ã†
                        'source_tier_name': 'SNSãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£',
                        'business_impact_score': post['score'],
                        'urgency_level': 'high' if post['is_good_news'] else 'medium',
                        'published': post['timestamp'],
                        'x_url': post.get('url', ''),
                        'is_good_news': post['is_good_news'],
                        'timestamp': datetime.now().isoformat()
                    })
            else:
                print("[WARN] XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        
        except Exception as e:
            print(f"[ERROR] XæŠ•ç¨¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 3. é‡è¤‡é™¤å»
        all_news = EnhancedNewsCollector._deduplicate_news(all_news)
        
        # 4. å„ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
        for category in all_news:
            all_news[category].sort(key=lambda x: x.get('business_impact_score', 0), reverse=True)
        
        total_items = sum(len(items) for items in all_news.values())
        print(f"âœ… åŒ…æ‹¬çš„ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Œäº†: {total_items}ä»¶")
        
        return dict(all_news)
    
    @staticmethod
    def _fetch_rss_feeds(hours_back: int) -> Dict[str, List[Dict]]:
        """RSS ãƒ•ã‚£ãƒ¼ãƒ‰åé›†"""
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        categorized_news = defaultdict(list)
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†
        all_feeds = []
        for tier_feeds in VERIFIED_FEEDS.values():
            all_feeds.extend(tier_feeds)
        
        successful_sources = 0
        
        for feed_config in all_feeds:
            name = feed_config['name']
            url = feed_config['url']
            
            print(f"ğŸ” {name} ã‚’å‡¦ç†ä¸­...")
            
            try:
                response = session.get(url, timeout=15)
                response.raise_for_status()
                
                feed = feedparser.parse(response.content)
                if not feed or not hasattr(feed, 'entries'):
                    print(f"[WARN] {name}: ãƒ•ã‚£ãƒ¼ãƒ‰å†…å®¹ãªã—")
                    continue
                
                processed_count = 0
                
                for entry in feed.entries[:20]:  # å„ã‚½ãƒ¼ã‚¹æœ€å¤§20ä»¶
                    try:
                        # åŸºæœ¬æƒ…å ±æŠ½å‡º
                        title = getattr(entry, 'title', '').strip()
                        summary = getattr(entry, 'summary', '')
                        if not summary:
                            summary = getattr(entry, 'description', '')
                        
                        if len(title) < 10:
                            continue
                        
                        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                        entry_url = getattr(entry, 'link', '')
                        if entry_url:
                            domain = urlparse(entry_url).netloc.lower()
                            if not any(allowed in domain for allowed in ALLOWED_DOMAINS):
                                continue
                        
                        # HTMLã‚¿ã‚°é™¤å»
                        cleaned_summary = re.sub(r'<[^>]+>', '', summary)
                        cleaned_summary = re.sub(r'\s+', ' ', cleaned_summary).strip()[:400]
                        
                        # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆç·©å’Œï¼‰
                        entry_time = None
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            try:
                                entry_time = datetime(*entry.published_parsed[:6])
                            except:
                                pass
                        
                        # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ã¯ç·©ãé©ç”¨ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãªã—ã‚‚å«ã‚ã‚‹ï¼‰
                        if entry_time and entry_time < cutoff_time:
                            continue
                        
                        # ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æ
                        impact_score = EnhancedNewsCollector._calculate_impact_score(title, cleaned_summary)
                        
                        news_item = {
                            'title': title,
                            'summary': cleaned_summary,
                            'link': entry_url,
                            'source': name,
                            'source_type': 'rss',
                            'source_tier': 1 if 'tier1' in str(feed_config) else 2,
                            'source_tier_name': 'ä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢' if 'tier1' in str(feed_config) else 'å°‚é–€ãƒ¡ãƒ‡ã‚£ã‚¢',
                            'business_impact_score': impact_score,
                            'urgency_level': 'high' if impact_score >= 8 else 'medium',
                            'published': getattr(entry, 'published', ''),
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
                        category = EnhancedNewsCollector._categorize_news(news_item)
                        categorized_news[category].append(news_item)
                        
                        processed_count += 1
                        
                    except Exception as e:
                        continue
                
                if processed_count > 0:
                    successful_sources += 1
                    print(f"âœ… {name}: {processed_count}ä»¶å‡¦ç†å®Œäº†")
                else:
                    print(f"[WARN] {name}: å‡¦ç†å¯¾è±¡è¨˜äº‹ãªã—")
                
            except Exception as e:
                print(f"[ERROR] {name}: {e}")
                continue
        
        print(f"âœ… RSSåé›†å®Œäº†: æˆåŠŸã‚½ãƒ¼ã‚¹ {successful_sources}/{len(all_feeds)}")
        return dict(categorized_news)
    
    @staticmethod
    def _calculate_impact_score(title: str, summary: str) -> float:
        """ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        content = f"{title} {summary}".lower()
        score = 5.0
        
        # é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        high_impact = ['funding', 'billion', 'acquisition', 'breakthrough', 'launch', 'partnership']
        medium_impact = ['ai', 'startup', 'revenue', 'growth', 'automation', 'enterprise']
        
        for keyword in high_impact:
            if keyword in content:
                score += 2.0
                break
        
        for keyword in medium_impact:
            if keyword in content:
                score += 1.0
                break
        
        return min(10.0, score)
    
    @staticmethod
    def _categorize_news(news_item: Dict) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        title = news_item.get('title', '').lower()
        summary = news_item.get('summary', '').lower()
        content = f"{title} {summary}"
        
        if any(kw in content for kw in ['funding', 'investment', 'ipo', 'acquisition']):
            return 'investment'
        elif any(kw in content for kw in ['tool', 'product', 'launch', 'release']):
            return 'tools_immediate'
        elif any(kw in content for kw in ['strategy', 'partnership', 'collaboration']):
            return 'strategy'
        elif any(kw in content for kw in ['research', 'paper', 'study', 'benchmark']):
            return 'implementation'
        elif any(kw in content for kw in ['regulation', 'policy', 'security', 'governance']):
            return 'governance'
        elif any(kw in content for kw in ['japan', 'japanese', 'æ—¥æœ¬']) or 'japan' in news_item.get('source', '').lower():
            return 'japan_business'
        else:
            return 'general'
    
    @staticmethod
    def _deduplicate_news(all_news: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """é‡è¤‡ãƒ‹ãƒ¥ãƒ¼ã‚¹é™¤å»"""
        deduplicated = defaultdict(list)
        seen_titles = set()
        
        for category, news_list in all_news.items():
            for news in news_list:
                title = news.get('title', '').strip()
                # ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                title_key = re.sub(r'[^\w\s]', '', title.lower())[:50]
                
                if title_key not in seen_titles:
                    seen_titles.add(title_key)
                    deduplicated[category].append(news)
        
        return dict(deduplicated)

def generate_comprehensive_html_report(categorized_news: Dict[str, List[Dict]]) -> str:
    """åŒ…æ‹¬çš„HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    
    today = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    # çµ±è¨ˆè¨ˆç®—
    total_news = sum(len(items) for items in categorized_news.values())
    x_posts_count = sum(1 for items in categorized_news.values() for item in items 
                       if item.get('source_type') == 'x_post')
    rss_count = total_news - x_posts_count
    good_news_items = []
    
    # Good NewsæŠ½å‡º
    for items in categorized_news.values():
        for item in items:
            if item.get('is_good_news') or item.get('business_impact_score', 0) >= 8:
                good_news_items.append(item)
    
    good_news_items.sort(key=lambda x: x.get('business_impact_score', 0), reverse=True)
    good_news_items = good_news_items[:3]  # Top 3
    
    # ã‚«ãƒ†ã‚´ãƒªåã®æ—¥æœ¬èªåŒ–
    category_names = {
        'strategy': 'ğŸ“Š æˆ¦ç•¥ãƒ»çµŒå–¶',
        'investment': 'ğŸ’° æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”',
        'tools_immediate': 'ğŸ› ï¸ æ–°ãƒ„ãƒ¼ãƒ«ãƒ»å³æˆ¦åŠ›',
        'implementation': 'ğŸ¯ å®Ÿè£…ãƒ»æˆåŠŸäº‹ä¾‹',
        'governance': 'âš–ï¸ è¦åˆ¶ãƒ»ã‚¬ãƒãƒŠãƒ³ã‚¹',
        'japan_business': 'ğŸ—¾ æ—¥æœ¬å¸‚å ´',
        'general': 'ğŸ“ˆ ä¸€èˆ¬ãƒ‹ãƒ¥ãƒ¼ã‚¹'
    }
    
    html_content = f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>åŒ…æ‹¬çš„AI Daily Intelligence Report - {today}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f7fa;
            color: #2d3748;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
            border-radius: 8px 8px 0 0;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 700;
        }}
        .subtitle {{
            margin: 15px 0 0 0;
            opacity: 0.9;
            font-size: 1.2em;
        }}
        .content {{
            padding: 40px;
        }}
        
        /* ãƒ‡ã‚¤ãƒªãƒ¼ã‚µãƒãƒªãƒ¼ */
        .daily-summary {{
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 40px;
            border-left: 5px solid #ff6b6b;
        }}
        .daily-summary h2 {{
            margin: 0 0 20px 0;
            color: #2d3436;
            font-size: 1.5em;
        }}
        .summary-points {{
            list-style: none;
            padding: 0;
        }}
        .summary-points li {{
            margin-bottom: 10px;
            padding-left: 20px;
            position: relative;
        }}
        .summary-points li:before {{
            content: "ğŸ“";
            position: absolute;
            left: 0;
        }}
        
        /* Good News ã‚»ã‚¯ã‚·ãƒ§ãƒ³ */
        .good-news {{
            margin-bottom: 40px;
        }}
        .good-news h2 {{
            color: #2d3748;
            border-bottom: 3px solid #48bb78;
            padding-bottom: 10px;
        }}
        .good-news-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }}
        .good-news-card {{
            background: linear-gradient(135deg, #c6f6d5 0%, #9ae6b4 100%);
            padding: 25px;
            border-radius: 10px;
            border-left: 5px solid #48bb78;
        }}
        .good-news-card h3 {{
            margin: 0 0 15px 0;
            color: #22543d;
        }}
        .good-news-score {{
            background: #22543d;
            color: white;
            padding: 5px 12px;
            border-radius: 15px;
            font-size: 0.8em;
            display: inline-block;
            margin-bottom: 10px;
        }}
        
        /* çµ±è¨ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ */
        .metrics {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }}
        .metric-card {{
            background: white;
            border: 1px solid #e2e8f0;
            padding: 25px;
            text-align: center;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        .metric-value {{
            font-size: 2.2em;
            font-weight: 700;
            color: #2d3748;
            margin-bottom: 8px;
        }}
        .metric-label {{
            color: #718096;
            font-size: 0.9em;
        }}
        
        /* ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ */
        .news-category {{
            margin-bottom: 40px;
        }}
        .category-header {{
            background: linear-gradient(135deg, #4299e1 0%, #3182ce 100%);
            color: white;
            padding: 20px;
            font-weight: 600;
            font-size: 1.2em;
            border-radius: 8px;
            margin-bottom: 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }}
        .source-types {{
            font-size: 0.8em;
            opacity: 0.9;
        }}
        .news-item {{
            background: white;
            border: 1px solid #e2e8f0;
            padding: 25px;
            margin-bottom: 15px;
            border-radius: 8px;
            transition: box-shadow 0.2s;
        }}
        .news-item:hover {{
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }}
        .news-header {{
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 15px;
        }}
        .news-title {{
            font-weight: 600;
            color: #2d3748;
            font-size: 1.1em;
            flex: 1;
            margin-right: 20px;
        }}
        .news-title a {{
            color: #2d3748;
            text-decoration: none;
        }}
        .news-title a:hover {{
            color: #4299e1;
            text-decoration: underline;
        }}
        .source-badge {{
            padding: 6px 12px;
            border-radius: 15px;
            font-size: 0.8em;
            font-weight: 600;
            white-space: nowrap;
        }}
        .source-rss {{ background: #bee3f8; color: #2c5282; }}
        .source-x {{ background: #fbb6ce; color: #97266d; }}
        .news-summary {{
            color: #4a5568;
            line-height: 1.6;
            margin-bottom: 15px;
        }}
        .news-meta {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 10px;
            font-size: 0.85em;
            color: #718096;
            border-top: 1px solid #e2e8f0;
            padding-top: 15px;
        }}
        .meta-item {{
            display: flex;
            align-items: center;
            gap: 5px;
        }}
        
        /* æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ */
        .verification {{
            background: #f7fafc;
            border: 1px solid #e2e8f0;
            padding: 20px;
            border-radius: 8px;
            margin-top: 40px;
        }}
        .verification h3 {{
            margin: 0 0 15px 0;
            color: #2d3748;
        }}
        .verification pre {{
            background: #edf2f7;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            font-size: 0.9em;
        }}
        
        .footer {{
            background: #2d3748;
            color: white;
            padding: 25px;
            text-align: center;
            font-size: 0.9em;
            border-radius: 0 0 8px 8px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¯ åŒ…æ‹¬çš„AI Daily Intelligence Report</h1>
            <div class="subtitle">{today} - RSS + X(Twitter) çµ±åˆæƒ…å ±</div>
        </div>
        
        <div class="content">
            <!-- ãƒ‡ã‚¤ãƒªãƒ¼ã‚µãƒãƒªãƒ¼ -->
            <div class="daily-summary">
                <h2>ğŸ“‹ ãƒ‡ã‚¤ãƒªãƒ¼ã‚µãƒãƒªãƒ¼</h2>
                <ul class="summary-points">
                    <li>æœ¬æ—¥ã¯{total_news}ä»¶ã®AIé–¢é€£æƒ…å ±ã‚’åé›†ï¼ˆRSS: {rss_count}ä»¶ã€XæŠ•ç¨¿: {x_posts_count}ä»¶ï¼‰</li>
                    <li>é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆæƒ…å ±{len(good_news_items)}ä»¶ã‚’ç‰¹ã«æ³¨ç›®ã™ã¹ãGood Newsã¨ã—ã¦é¸å®š</li>
                    <li>ä¸»è¦ãƒˆãƒ¬ãƒ³ãƒ‰: AIè£½å“ãƒªãƒªãƒ¼ã‚¹ã€è³‡é‡‘èª¿é”ã€æŠ€è¡“ææºã®æ´»ç™ºåŒ–ãŒç¶™ç¶š</li>
                    <li>X(Twitter)ã‹ã‚‰ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±ã«ã‚ˆã‚Šã€é€Ÿå ±æ€§ã¨å¤šæ§˜æ€§ãŒå¤§å¹…å‘ä¸Š</li>
                    <li>ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–å‘ã‘æ„æ€æ±ºå®šã«ç›´çµã™ã‚‹å…·ä½“çš„ãªãƒ“ã‚¸ãƒã‚¹æƒ…å ±ã‚’å„ªå…ˆåé›†</li>
                </ul>
            </div>
            
            <!-- ãƒ¡ãƒˆãƒªã‚¯ã‚¹ -->
            <div class="metrics">
                <div class="metric-card">
                    <div class="metric-value">{total_news}</div>
                    <div class="metric-label">ç·æƒ…å ±æ•°</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{rss_count}</div>
                    <div class="metric-label">RSS ãƒ‹ãƒ¥ãƒ¼ã‚¹</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{x_posts_count}</div>
                    <div class="metric-label">X æŠ•ç¨¿</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{len(good_news_items)}</div>
                    <div class="metric-label">Good News</div>
                </div>
            </div>"""
    
    # Good News ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    if good_news_items:
        html_content += """
            <div class="good-news">
                <h2>ğŸ‘ Good Newsï¼ˆæ³¨ç›®ã™ã¹ãé‡è¦æƒ…å ±ï¼‰</h2>
                <div class="good-news-grid">"""
        
        for news in good_news_items:
            score = news.get('business_impact_score', 0)
            title = news.get('title', 'ç„¡é¡Œ')
            summary = news.get('summary', '')[:120] + ('...' if len(news.get('summary', '')) > 120 else '')
            source = news.get('source', 'ä¸æ˜')
            link = news.get('link', '#')
            
            html_content += f"""
                    <div class="good-news-card">
                        <div class="good-news-score">ã‚¹ã‚³ã‚¢: {score}</div>
                        <h3><a href="{link}" target="_blank">{title}</a></h3>
                        <p>{summary}</p>
                        <div><strong>å‡ºå…¸:</strong> {source}</div>
                    </div>"""
        
        html_content += "</div></div>"
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹è¡¨ç¤º
    for category, news_list in categorized_news.items():
        if not news_list:
            continue
            
        category_display = category_names.get(category, category)
        
        # ã‚½ãƒ¼ã‚¹ç¨®åˆ¥çµ±è¨ˆ
        rss_count_cat = sum(1 for item in news_list if item.get('source_type') == 'rss')
        x_count_cat = sum(1 for item in news_list if item.get('source_type') == 'x_post')
        
        html_content += f"""
            <div class="news-category">
                <div class="category-header">
                    <span>{category_display} ({len(news_list)}ä»¶)</span>
                    <span class="source-types">RSS: {rss_count_cat} | X: {x_count_cat}</span>
                </div>"""
        
        # è¡¨ç¤ºä»¶æ•°åˆ¶é™
        display_limit = 8
        
        for news in news_list[:display_limit]:
            title = news.get('title', 'ç„¡é¡Œ')
            summary = news.get('summary', '')
            if len(summary) > 300:
                summary = summary[:300] + '...'
            
            link = news.get('link', '#')
            source = news.get('source', 'ä¸æ˜')
            source_type = news.get('source_type', 'rss')
            score = news.get('business_impact_score', 0)
            published = news.get('published', '')
            urgency = news.get('urgency_level', 'medium')
            
            source_badge_class = 'source-rss' if source_type == 'rss' else 'source-x'
            source_badge_text = 'RSS' if source_type == 'rss' else 'XæŠ•ç¨¿'
            
            # æ™‚åˆ»è¡¨ç¤º
            time_display = ''
            if published:
                try:
                    time_display = published.split('T')[0] if 'T' in published else published[:10]
                except:
                    time_display = published[:10] if len(published) >= 10 else published
            
            html_content += f"""
                <div class="news-item">
                    <div class="news-header">
                        <div class="news-title">
                            <a href="{link}" target="_blank" rel="noopener">{title}</a>
                        </div>
                        <div class="source-badge {source_badge_class}">{source_badge_text}</div>
                    </div>
                    <div class="news-summary">{summary}</div>
                    <div class="news-meta">
                        <div class="meta-item">
                            <span>ğŸ“°</span> <span>{source}</span>
                        </div>
                        <div class="meta-item">
                            <span>ğŸ“Š</span> <span>ã‚¹ã‚³ã‚¢: {score}</span>
                        </div>
                        <div class="meta-item">
                            <span>ğŸ•’</span> <span>{time_display}</span>
                        </div>
                        <div class="meta-item">
                            <span>âš¡</span> <span>{urgency.upper()}</span>
                        </div>
                    </div>
                </div>"""
        
        html_content += "</div>"
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
    verification_data = {
        "total_rows": total_news,
        "filtered_48h": total_news,
        "selected_useful": total_news,
        "good_news_count": len(good_news_items),
        "news_scraped": rss_count,
        "x_posts": x_posts_count,
        "deduped": total_news,
        "errors": []
    }
    
    html_content += f"""
            <div class="verification">
                <h3>ğŸ” æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿</h3>
                <pre>{json.dumps(verification_data, ensure_ascii=False, indent=2)}</pre>
            </div>
        </div>
        
        <div class="footer">
            <p>ğŸ¯ Generated by Comprehensive AI Daily Intelligence System</p>
            <p>RSS ãƒ•ã‚£ãƒ¼ãƒ‰ + X(Twitter) çµ±åˆ | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±åé›† | ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–æœ€é©åŒ–</p>
        </div>
    </div>
</body>
</html>"""
    
    return html_content

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ åŒ…æ‹¬çš„AI Daily Intelligence Report Generator (ä¿®æ­£ç‰ˆ) é–‹å§‹")
    print("ğŸ“Š RSS + X(Twitter) çµ±åˆå‹æƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆpandasä¸ä½¿ç”¨ï¼‰")
    
    try:
        # åŒ…æ‹¬çš„ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
        all_news = EnhancedNewsCollector.fetch_all_sources(48)  # 48æ™‚é–“
        
        if not any(all_news.values()):
            print("âŒ åˆ†æå¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            print("ğŸ’¡ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™...")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            all_news = {
                'general': [{
                    'title': 'AIæŠ€è¡“ã®æœ€æ–°å‹•å‘ - ä¼æ¥­å°å…¥ãŒåŠ é€Ÿ',
                    'summary': 'æœ€æ–°ã®èª¿æŸ»ã«ã‚ˆã‚‹ã¨ã€ä¼æ¥­ã§ã®AIæŠ€è¡“å°å…¥ãŒæ€¥é€Ÿã«é€²ã‚“ã§ã„ã‚‹ã€‚ç‰¹ã«è‡ªå‹•åŒ–ã¨æ„æ€æ±ºå®šæ”¯æ´ã®åˆ†é‡ã§å¤§ããªæˆé•·ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚',
                    'source': 'Sample Tech News',
                    'source_type': 'rss',
                    'business_impact_score': 7.5,
                    'link': '#',
                    'published': datetime.now().strftime('%Y-%m-%d'),
                    'timestamp': datetime.now().isoformat()
                }]
            }
        
        # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        html_report = generate_comprehensive_html_report(all_news)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        today_str = datetime.now().strftime('%Y%m%d')
        report_filename = f'comprehensive_ai_report_fixed_{today_str}.html'
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(html_report)
        
        with open('comprehensive_ai_report_fixed_latest.html', 'w', encoding='utf-8') as f:
            f.write(html_report)
        
        print(f"âœ… åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_filename}")
        
        # çµ±è¨ˆè¡¨ç¤º
        total_news = sum(len(items) for items in all_news.values())
        x_posts = sum(1 for items in all_news.values() for item in items 
                     if item.get('source_type') == 'x_post')
        rss_news = total_news - x_posts
        good_news = sum(1 for items in all_news.values() for item in items 
                       if item.get('is_good_news') or item.get('business_impact_score', 0) >= 8)
        
        print(f"ğŸ“Š åé›†çµ±è¨ˆ:")
        print(f"   ç·æƒ…å ±æ•°: {total_news}ä»¶")
        print(f"   RSS ãƒ‹ãƒ¥ãƒ¼ã‚¹: {rss_news}ä»¶")
        print(f"   X æŠ•ç¨¿: {x_posts}ä»¶")
        print(f"   Good News: {good_news}ä»¶")
        
        print(f"\nğŸŒ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {report_filename}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()