# -*- coding: utf-8 -*-
"""
Daily AI News - static site generator (JST)
- Summaries are translated to Japanese (no API key) using deep-translator.
- Primary engine: GoogleTranslator (unofficial). Fallback: MyMemory (ja-JP).
- Caches translations to _cache/translations.json to avoid repeated calls.
- Reads RSS list from feeds.yml with categories: Business, Tools, Posts.
- Injects X posts from a CSV file into the 'Posts' category.

Env (optional):
  HOURS_LOOKBACK=24        # Fetch window in hours
  MAX_ITEMS_PER_CATEGORY=8 # Max cards per tab
  TRANSLATE_TO_JA=1        # 1=enable JA summaries, 0=disable
  TRANSLATE_ENGINE=google  # google|mymemory
  X_POSTS_CSV=_sources/x_favorites.csv # Path to X posts CSV
  TZ=Asia/Tokyo            # for timestamps
"""
import os, re, sys, json, time, html, csv, io
from datetime import datetime, timezone, timedelta
from pathlib import Path
from urllib.parse import urlparse
from urllib.request import urlopen

import yaml
import feedparser
import requests
import random
# Enhanced X Processing Integration
try:
    from enhanced_x_processor import EnhancedXProcessor
    ENHANCED_X_AVAILABLE = True
    print("âœ… Enhanced X Processor: Integrated")
except ImportError:
    ENHANCED_X_AVAILABLE = False
    print("âš ï¸ Enhanced X Processor: Using fallback")

import time
from urllib.parse import urljoin

# URL ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from url_filter import filter_403_urls, is_403_url
    print("âœ… URL ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½: æœ‰åŠ¹")
except ImportError:
    print("âš ï¸ URL ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½: ç„¡åŠ¹")
    def filter_403_urls(items):
        return items
    def is_403_url(url):
        return False

# ---------- config (æ”¹å–„ç‰ˆ) ----------
def get_config():
    """è¨­å®šèª­ã¿è¾¼ã¿ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    config = {
        'hours_lookback': int(os.getenv("HOURS_LOOKBACK", "24")),
        'max_items_per_category': int(os.getenv("MAX_ITEMS_PER_CATEGORY", "8")),
        'translate_to_ja': os.getenv("TRANSLATE_TO_JA", "1") == "1",
        'translate_engine': os.getenv("TRANSLATE_ENGINE", "google").lower(),
        'x_posts_csv': os.getenv("X_POSTS_CSV", ""),
        'gemini_api_key': os.getenv("GEMINI_API_KEY", ""),
        'debug_mode': os.getenv("DEBUG_MODE", "0") == "1"
    }

    # XæŠ•ç¨¿CSVã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    if not config['x_posts_csv']:
        config['x_posts_csv'] = "https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0"

    # è¨­å®šå€¤ã®æ¤œè¨¼
    if config['hours_lookback'] < 1 or config['hours_lookback'] > 168:  # 1æ™‚é–“ï½1é€±é–“
        print(f"[WARN] Invalid HOURS_LOOKBACK: {config['hours_lookback']}, using default 24")
        config['hours_lookback'] = 24

    if config['max_items_per_category'] < 1 or config['max_items_per_category'] > 20:
        print(f"[WARN] Invalid MAX_ITEMS_PER_CATEGORY: {config['max_items_per_category']}, using default 8")
        config['max_items_per_category'] = 8

    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰è¡¨ç¤º
    if config['debug_mode']:
        print("[DEBUG] Debug mode enabled")

    return config

# è¨­å®šèª­ã¿è¾¼ã¿
CONFIG = get_config()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°è¨­å®š
HOURS_LOOKBACK = CONFIG['hours_lookback']
MAX_ITEMS_PER_CATEGORY = CONFIG['max_items_per_category']
TRANSLATE_TO_JA = CONFIG['translate_to_ja']
TRANSLATE_ENGINE = CONFIG['translate_engine']
X_POSTS_CSV = CONFIG['x_posts_csv']

JST = timezone(timedelta(hours=9))
NOW = datetime.now(JST)

CACHE_DIR = Path("_cache")
CACHE_DIR.mkdir(exist_ok=True)
CACHE_FILE = CACHE_DIR / "translations.json"

def load_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    try:
        if CACHE_FILE.exists():
            cache_data = json.loads(CACHE_FILE.read_text(encoding="utf-8"))
            cache_size = len(cache_data)
            print(f"[INFO] Loaded cache with {cache_size} entries")
            return cache_data
        else:
            print("[INFO] No cache file found, starting fresh")
    except (json.JSONDecodeError, UnicodeDecodeError) as e:
        print(f"[WARN] Cache file corrupted: {e}")
    except Exception as e:
        print(f"[ERROR] Failed to load cache: {e}")
    return {}

def save_cache(cache):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    try:
        CACHE_DIR.mkdir(exist_ok=True)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        cache_size = len(cache)
        if cache_size > 10000:  # 10,000ã‚¨ãƒ³ãƒˆãƒªä»¥ä¸Šã¯è­¦å‘Š
            print(f"[WARN] Large cache detected ({cache_size} entries)")

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        if CACHE_FILE.exists():
            backup_file = CACHE_FILE.with_suffix('.json.backup')
            CACHE_FILE.rename(backup_file)

        # æ–°ã—ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
        CACHE_FILE.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding="utf-8")

        # ä¿å­˜ç¢ºèª
        saved_size = len(json.loads(CACHE_FILE.read_text(encoding="utf-8")))
        if saved_size == cache_size:
            print(f"[SUCCESS] Cache saved with {cache_size} entries")
        else:
            print(f"[WARN] Cache size mismatch: expected {cache_size}, got {saved_size}")

        # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤
        if backup_file.exists():
            backup_file.unlink()

    except Exception as e:
        print(f"[ERROR] Failed to save cache: {e}")
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ
        if 'backup_file' in locals() and backup_file.exists():
            backup_file.rename(CACHE_FILE)
            print("[INFO] Restored cache from backup")

def advanced_feed_fetch(url, name):
    """é«˜åº¦ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾— - Google News 403ã‚¨ãƒ©ãƒ¼å¯¾ç­–"""
    
    # è¤‡æ•°ã®User-Agentã‚’ç”¨æ„
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0'
    ]
    
    for i, user_agent in enumerate(user_agents):
        try:
            print(f"[INFO] Advanced fetch attempt {i+1}/{len(user_agents)} for {name}")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦è©³ç´°ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
            session = requests.Session()
            session.headers.update({
                'User-Agent': user_agent,
                'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            })
            
            # Google Newsã¾ãŸã¯Googleç³»ã®URLã®å ´åˆã€è¿½åŠ ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
            if 'google.com' in url or 'news.google.com' in url:
                session.headers.update({
                    'Referer': 'https://news.google.com/',
                    'Origin': 'https://news.google.com',
                    'Sec-Fetch-User': '?1'
                })
            
            # Googleç³»ã‚µãƒ¼ãƒ“ã‚¹ã®å ´åˆã€è¿½åŠ ã®é…å»¶
            if 'google.com' in url:
                delay = random.uniform(2, 5)  # 2-5ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ é…å»¶
                print(f"[INFO] Google service detected, applying {delay:.1f}s delay")
                time.sleep(delay)
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            response = session.get(url, timeout=30, allow_redirects=True)
            
            if response.status_code == 200:
                print(f"[SUCCESS] {name} fetched successfully with User-Agent {i+1}")
                # feedparserã«æ¸¡ã™ãŸã‚ã«BytesIOã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                import io
                feed_data = io.BytesIO(response.content)
                d = feedparser.parse(feed_data)
                return d
            elif response.status_code == 403:
                print(f"[WARN] 403 Forbidden with User-Agent {i+1} for {name}")
                continue
            else:
                print(f"[WARN] HTTP {response.status_code} with User-Agent {i+1} for {name}")
                continue
                
        except Exception as e:
            print(f"[WARN] Exception with User-Agent {i+1} for {name}: {e}")
            continue
    
    print(f"[ERROR] All advanced fetch attempts failed for {name}")
    
    # æœ€å¾Œã®æ‰‹æ®µ: Gemini Web Fetcherã‚’ä½¿ç”¨
    try:
        from gemini_web_fetcher import GeminiWebFetcher
        fetcher = GeminiWebFetcher()
        if fetcher.analyzer.enabled:
            print(f"[INFO] Trying Gemini Web Fetcher for {name}...")
            news_items = fetcher.fetch_from_problematic_source(url, name)
            if news_items:
                # feedparserãƒ©ã‚¤ã‚¯ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                fake_feed = type('FakeFeed', (), {})()
                fake_feed.entries = []
                
                for item in news_items:
                    entry = type('FakeEntry', (), {})()
                    entry.title = item.get('title', '')
                    entry.summary = item.get('summary', '')
                    entry.link = item.get('url', '#')
                    entry.published_parsed = item.get('_dt', datetime.now()).timetuple()
                    fake_feed.entries.append(entry)
                
                print(f"[SUCCESS] Gemini fetched {len(news_items)} items for {name}")
                return fake_feed
    except ImportError:
        print(f"[WARN] Gemini Web Fetcher not available")
    except Exception as e:
        print(f"[WARN] Gemini Web Fetcher failed: {e}")
    
    return None

# ---------- translation ----------
def looks_japanese(s: str) -> bool:
    if not s:
        return False
    # Hiragana, Katakana, CJK
    return re.search(r"[\u3040-\u30ff\u3400-\u9fff]", s) is not None

class JaTranslator:
    def __init__(self, engine="google"):
        self.engine = engine
        self._gt = None
        self._mm = None
        self.warned = False

    def _google(self, text: str) -> str:
        if self._gt is None:
            from deep_translator import GoogleTranslator
            self._gt = GoogleTranslator(source="auto", target="ja")
        return self._gt.translate(text)

    def _mymemory(self, text: str) -> str:
        # deep-translator >=1.11.0 expects region code 'ja-JP' for MyMemory
        if self._mm is None:
            from deep_translator import MyMemoryTranslator
            self._mm = MyMemoryTranslator(source="en-GB", target="ja-JP")
        return self._mm.translate(text)

    def translate(self, text: str) -> str:
        if not text or looks_japanese(text):
            return text
        try:
            if self.engine == "google":
                try:
                    return self._google(text)
                except Exception:
                    return self._mymemory(text)
            elif self.engine == "mymemory":
                try:
                    return self._mymemory(text)
                except Exception:
                    return self._google(text)
            else:
                # unknown -> google then mymemory
                try:
                    return self._google(text)
                except Exception:
                    return self._mymemory(text)
        except Exception as e:
            if not self.warned:
                print(f"[WARN] translation disabled due to error: {e.__class__.__name__}: {e}")
                self.warned = True
            return text

# ---------- X (Twitter) post injection ----------
def _read_csv_bytes(path_or_url: str) -> bytes:
    if re.match(r'^https?://', path_or_url, re.I):
        with urlopen(path_or_url) as r:
            return r.read()
    with open(path_or_url, 'rb') as f:
        return f.read()

def _extract_x_data_from_csv(raw: bytes) -> list[dict]:
    # æ–‡å­—ã‚³ãƒ¼ãƒ‰è‡ªå‹•åˆ¤å®š(UTF-8 BOMå„ªå…ˆâ†’UTF-8â†’CP932)
    for enc in ('utf-8-sig','utf-8','cp932'):
        try:
            txt = raw.decode(enc)
            break
        except Exception:
            continue
    else:
        txt = raw.decode('utf-8', errors='ignore')

    # CSVå½¢å¼: "æ—¥ä»˜", "@ãƒ¦ãƒ¼ã‚¶ãƒ¼", "ãƒ†ã‚­ã‚¹ãƒˆ", "ç”»åƒURL", "ãƒ„ã‚¤ãƒ¼ãƒˆURL"
    data = []
    try:
        rdr = csv.reader(io.StringIO(txt))
        row_count = 0
        for row in rdr:
            row_count += 1
            if row_count == 1:  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                continue
                
            if len(row) >= 3:  # æœ€ä½3åˆ—ã‚ã‚Œã°å‡¦ç†
                date_str = row[0].strip('"').strip() if len(row) > 0 else ""
                username = row[1].strip('"').strip() if len(row) > 1 else ""
                text = row[2].strip('"').strip() if len(row) > 2 else ""
                tweet_url = row[4].strip('"').strip() if len(row) > 4 else ""
                
                # URLãŒãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼URLã‚’ç”Ÿæˆ
                if not tweet_url and username:
                    username_clean = username.replace('@', '').replace('"', '')
                    tweet_url = f"https://x.com/{username_clean}/status/example"
                elif not tweet_url:
                    tweet_url = "https://x.com/unknown/status/example"
                
                # æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Œã°å‡¦ç†ï¼ˆæ¡ä»¶ã‚’å¤§å¹…ã«ç·©å’Œï¼‰
                if text and len(text.strip()) > 5:  # 5æ–‡å­—ä»¥ä¸Šã‚ã‚Œã°å‡¦ç†
                    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ã‚’è¡¨ç¤º
                    print(f"[DEBUG] Processing post: user={username}, text_len={len(text)}, text='{text[:50]}...'")
                    
                    # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆè¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œï¼‰
                    dt = None
                    # è¤‡æ•°ã®æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è©¦ã™
                    date_formats = [
                        "%B %d, %Y at %I:%M%p",  # "August 10, 2025 at 02:41AM"
                        "%B %d, %Y",             # "August 13, 2025"
                        "%Y-%m-%d %H:%M:%S",     # "2025-08-18 14:30:00"
                        "%Y-%m-%d",              # "2025-08-18"
                        "%Yå¹´%mæœˆ%dæ—¥",           # "2025å¹´8æœˆ18æ—¥"
                        "%m/%d/%Y",              # "8/18/2025"
                    ]
                    for fmt in date_formats:
                        try:
                            dt = datetime.strptime(date_str, fmt)
                            dt = dt.replace(tzinfo=JST)  # JSTã¨ã—ã¦æ‰±ã†
                            break
                        except:
                            continue
                    
                    # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨ï¼ˆæŠ•ç¨¿ã‚’è¡¨ç¤ºã•ã›ã‚‹ãŸã‚ï¼‰
                    if dt is None:
                        print(f"[WARN] Date parse failed for '{date_str}', using current time")
                        dt = datetime.now(JST)
                    
                    # å¸¸ã«æŠ•ç¨¿ã‚’è¿½åŠ 
                    data.append({
                        'url': tweet_url,
                        'username': username,
                        'text': text,
                        'datetime': dt
                    })
                    print(f"[DEBUG] Added post: url={tweet_url}, user={username}, text_len={len(text)}")
                else:
                    print(f"[DEBUG] Skipping invalid post: user={username}, text_len={len(text) if text else 0}")
    except Exception as e:
        print(f"[WARN] CSV parsing error: {e}")
        pass

    # å¤ã„å½¢å¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆURLæŠ½å‡ºã®ã¿ï¼‰
    if not data:
        urls = re.findall(r'https?://(?:x|twitter)\.com/[^\s,"]+', txt)
        for url in urls:
            data.append({
                'url': url,
                'username': '',
                'text': '',
                'datetime': NOW
            })
    
    return data

def _extract_x_urls_from_csv(raw: bytes) -> list[str]:
    # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
    data = _extract_x_data_from_csv(raw)
    
    # æ­£è¦åŒ– & é‡è¤‡é™¤å»ï¼ˆé †åºç¶­æŒï¼‰
    seen, out = set(), []
    for item in data:
        u = item['url'].replace('x.com/','twitter.com/')  # æ­£è¦åŒ–
        if u not in seen:
            seen.add(u); out.append(u)
    return out

def _author_from_url(u: str) -> str:
    try:
        p = urlparse(u)
        parts = p.path.strip('/').split('/')
        return '@'+parts[0] if parts and parts[0] else 'X'
    except Exception:
        return 'X'


def enhanced_gather_x_posts_implementation(csv_path: str) -> list[dict]:
    """Enhanced X Posts - é‡è¤‡é™¤å»ã¨Geminiå¼·åŒ–"""
    if ENHANCED_X_AVAILABLE:
        try:
            processor = EnhancedXProcessor()
            posts = processor.process_x_posts(csv_path, max_posts=25)
            
            if posts:
                build_items = processor.convert_to_build_format(posts)
                print(f"âœ… Enhanced Xå‡¦ç†: {len(build_items)}ä»¶ (é‡è¤‡é™¤å»ãƒ»Geminiå¼·åŒ–æ¸ˆã¿)")
                
                # çµ±è¨ˆè¡¨ç¤º
                enhanced_count = sum(1 for item in build_items if item.get('_enhanced', False))
                high_priority = sum(1 for item in build_items if item.get('_priority', 0) >= 3)
                
                print(f"   ğŸ§  Geminiå¼·åŒ–æ¸ˆã¿: {enhanced_count}ä»¶")
                print(f"   â­ é«˜é‡è¦åº¦æŠ•ç¨¿: {high_priority}ä»¶")
                
                return build_items
        except Exception as e:
            print(f"âš ï¸ Enhancedå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e} - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®å‡¦ç†
    return original_gather_x_posts(csv_path)


def gather_x_posts(csv_path: str) -> list[dict]:
    return enhanced_gather_x_posts_implementation(csv_path)

def original_gather_x_posts(csv_path: str) -> list[dict]:
    # Check if it's a URL or local file
    is_url = csv_path.startswith('http')
    
    if not is_url and not Path(csv_path).exists():
        print(f"[INFO] X posts CSV not found at: {csv_path}")
        return []
    
    if is_url:
        print(f"[INFO] Loading X posts from Google Sheets: {csv_path}")
    else:
        print(f"[INFO] Loading X posts from local file: {csv_path}")
    items = []
    try:
        raw = _read_csv_bytes(csv_path)
        x_data = _extract_x_data_from_csv(raw)
        print(f"[INFO] Extracted {len(x_data)} X posts from CSV.")
        
        # é‡è¤‡é™¤å»ã®ãŸã‚ã®ã‚»ãƒƒãƒˆ
        seen_urls = set()
        seen_username_text = set()
        
        for data in x_data:
            url = data['url']
            username = data['username'] or _author_from_url(url)
            post_date = data['datetime']
            # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚‚ä¿æŒï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã¯åˆ¥ã«ï¼‰
            full_text = data['text']
            text_preview = data['text'][:150] + '...' if len(data['text']) > 150 else data['text']
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            # 1. åŒã˜URLã®æŠ•ç¨¿ã¯é™¤å¤–
            if url in seen_urls:
                print(f"[DEBUG] Skipping duplicate URL: {url}")
                continue
            
            # 2. åŒã˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åŒã˜ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã¯é™¤å¤–
            username_text_key = f"{username}:{full_text[:50]}"
            if username_text_key in seen_username_text:
                print(f"[DEBUG] Skipping duplicate content from {username}")
                continue
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’é€šéã—ãŸæŠ•ç¨¿ã®ã¿è¿½åŠ 
            seen_urls.add(url)
            seen_username_text.add(username_text_key)
            
            # XæŠ•ç¨¿ã¯æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç„¡åŠ¹åŒ–ï¼ˆã™ã¹ã¦ã®æŠ•ç¨¿ã‚’å«ã‚ã‚‹ï¼‰
            # if (NOW - post_date) <= timedelta(days=7):  # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç„¡åŠ¹åŒ–
            if True:  # ã™ã¹ã¦ã®æŠ•ç¨¿ã‚’å‡¦ç†
                items.append({
                    "title": f"Xãƒã‚¹ãƒˆ {username}",
                    "link": url,
                    "_summary": text_preview or "XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿",
                    "_full_text": full_text,  # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
                    "_source": "X / SNS", 
                    "_dt": post_date,  # å®Ÿéš›ã®æŠ•ç¨¿æ—¥æ™‚ã‚’ä½¿ç”¨
                })
        
        print(f"[INFO] Created {len(items)} X post items (time filter disabled).")
    except Exception as e:
        print(f"[WARN] Failed to process X posts CSV: {e}")
    return items

# ---------- HTML template ----------
PAGE_TMPL = """<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Daily AI News â€” {updated_title}</title>
  <link rel="stylesheet" href="style.css"/>
</head>
<body>
  <header class="site-header">
    <div class="brand">ğŸ“° Daily AI News</div>
    <div class="updated">æœ€çµ‚æ›´æ–°ï¼š{updated_full}</div>
  </header>

  <main class="container">
    <h1 class="page-title">ä»Šæ—¥ã®æœ€æ–°AIæƒ…å ±</h1>
    <p class="lead">
        ä¸–ç•Œã®AIæ¥­ç•Œã®æœ€æ–°å‹•å‘ã‚’24æ™‚é–“365æ—¥ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã€‚OpenAIã€Googleã€Metaã€Anthropicãªã©ä¸»è¦ä¼æ¥­ã®å…¬å¼ç™ºè¡¨ã‹ã‚‰ã€
        arXivè«–æ–‡ã€é–‹ç™ºè€…ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æŠ€è¡“è¨è«–ã¾ã§å¹…åºƒãåé›†ã€‚ãƒ“ã‚¸ãƒã‚¹ï¼ˆè³‡é‡‘èª¿é”ãƒ»M&Aãƒ»æˆ¦ç•¥ææºï¼‰ã€
        ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ï¼ˆæ–°ãƒ¢ãƒ‡ãƒ«ãƒ»APIãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰ã€ç ”ç©¶ï¼ˆè«–æ–‡ãƒ»ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ï¼‰ã®3ã‚«ãƒ†ã‚´ãƒªã§æ•´ç†ã—ã€
        é‡è¦åº¦é †ã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‚å„è¨˜äº‹ã®è¦ç´„ã¯æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã€åŸæ–‡ãƒªãƒ³ã‚¯ã§è©³ç´°ç¢ºèªå¯èƒ½ã€‚
        ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ç ”ç©¶è€…ã€æŠ•è³‡å®¶ã€çµŒå–¶è€…ãªã©ã€AIæ¥­ç•Œã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«å‘ã‘ã®åŒ…æ‹¬çš„æƒ…å ±æºã¨ã—ã¦ã€
        ç›´è¿‘{lookback}æ™‚é–“ã®é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å³é¸é…ä¿¡ã€‚ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã¯æ¥­ç•Œå…¨ä½“åƒã®ä¿¯ç°åˆ†æã‚‚æä¾›ã€‚
    </p>

    <section class="kpi-grid">
      <div class="kpi-card">
        <div class="kpi-value">{cnt_business}ä»¶</div>
        <div class="kpi-label">ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹</div>
        <div class="kpi-note">é‡è¦åº¦é«˜ã‚</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{cnt_tools}ä»¶</div>
        <div class="kpi-label">ãƒ„ãƒ¼ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹</div>
        <div class="kpi-note">é–‹ç™ºè€…å‘ã‘</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{cnt_posts}ä»¶</div>
        <div class="kpi-label">SNS/è«–æ–‡ãƒã‚¹ãƒˆ</div>
        <div class="kpi-note">æ¤œè¨¼ç³»</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{updated_full}</div>
        <div class="kpi-label">æœ€çµ‚æ›´æ–°</div>
        <div class="kpi-note">JST</div>
      </div>
    </section>

    <nav class="tabs" role="tablist">
      <button class="tab active" data-target="#business" aria-selected="true">ğŸ¢ ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹</button>
      <button class="tab" data-target="#tools" aria-selected="false">âš¡ ãƒ„ãƒ¼ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹</button>
      <button class="tab" data-target="#posts" aria-selected="false">ğŸ§ª SNS/è«–æ–‡ãƒã‚¹ãƒˆ</button>
    </nav>

    <!-- æ”¹å–„ç‰ˆæ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°UI -->
    <div class="search-container">
      <div class="search-header">
        <input id="searchBox" type="text" placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è¨˜äº‹ã‚’æ¤œç´¢ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»è¦ç´„ãƒ»ã‚½ãƒ¼ã‚¹ï¼‰..." aria-label="æ¤œç´¢" />
        <div class="search-info">
          ğŸ’¡ é‡è¦åº¦ãƒãƒƒã‚¸ãƒ»ä¿¡é ¼åº¦ãƒãƒ¼ãƒ»é®®åº¦ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã§ä¾¡å€¤ã‚’åˆ¤å®š
        </div>
      </div>
      <div class="filter-controls" style="display: none;">
        <!-- JavaScriptã§å‹•çš„ã«è¿½åŠ ã•ã‚Œã‚‹ -->
      </div>
    </div>

    {sections}
    <section class="note">
      <p>æ–¹é‡ï¼šä¸€æ¬¡æƒ…å ±ï¼ˆå…¬å¼ãƒ–ãƒ­ã‚°/ãƒ—ãƒ¬ã‚¹/è«–æ–‡ï¼‰ã‚’å„ªå…ˆã€‚ä¸€èˆ¬ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ AI ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æŠ½å‡ºã€‚è¦ç´„ã¯æ—¥æœ¬èªåŒ–ã—ã€<strong>å‡ºå…¸ãƒªãƒ³ã‚¯ã¯åŸæ–‡</strong>ã®ã¾ã¾ã€‚</p>
    </section>
  </main>

  <footer class="site-footer">
    <div>Generated by <code>build.py</code> Â· Timezone: JST</div>
    <div><a href="https://github.com/">Hosted on GitHub Pages</a></div>
  </footer>

  <script>
    // ã‚¿ãƒ–åˆ‡ã‚Šæ›¿ãˆæ©Ÿèƒ½
    const tabs = document.querySelectorAll('.tab');
    tabs.forEach(btn => btn.addEventListener('click', () => {{
      tabs.forEach(b => {{ b.classList.remove('active'); b.setAttribute('aria-selected','false'); }});
      btn.classList.add('active'); btn.setAttribute('aria-selected','true');
      document.querySelectorAll('.tab-panel').forEach(p => p.classList.add('hidden'));
      const target = document.querySelector(btn.dataset.target);
      if (target) target.classList.remove('hidden');
    }}));

    // ãƒ•ã‚£ãƒ«ã‚¿ãƒ»ã‚½ãƒ¼ãƒˆæ©Ÿèƒ½
    const searchBox = document.getElementById('searchBox');
    const importanceFilter = document.getElementById('importanceFilter');
    const sortSelect = document.getElementById('sortSelect');

    function getVisibleCards() {{
      const activeTab = document.querySelector('.tab.active');
      if (!activeTab) return [];
      const targetPanel = document.querySelector(activeTab.dataset.target);
      return targetPanel ? Array.from(targetPanel.querySelectorAll('.card')) : [];
    }}

    function filterAndSortCards() {{
      const query = searchBox ? searchBox.value.toLowerCase() : '';
      const importanceThreshold = importanceFilter ? parseInt(importanceFilter.value) : 0;
      const sortBy = sortSelect ? sortSelect.value : 'importance';

      const cards = getVisibleCards();

      cards.forEach(card => {{
        // ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
        const titleEl = card.querySelector('.card-title');
        const summaryEl = card.querySelector('.card-summary');
        const sourceEl = card.querySelector('.source-badge');
        const title = titleEl ? titleEl.textContent.toLowerCase() : '';
        const summary = summaryEl ? summaryEl.textContent.toLowerCase() : '';
        const source = sourceEl ? sourceEl.textContent.toLowerCase() : '';

        const textMatch = !query || title.includes(query) || summary.includes(query) || source.includes(query);

        // é‡è¦åº¦ãƒ•ã‚£ãƒ«ã‚¿
        const importance = parseInt(card.dataset.importance || '50');
        const importanceMatch = importance >= importanceThreshold;

        // è¡¨ç¤ºãƒ»éè¡¨ç¤º
        if (textMatch && importanceMatch) {{
          card.style.display = '';
        }} else {{
          card.style.display = 'none';
        }}
      }});

      // ã‚½ãƒ¼ãƒˆ
      if (sortBy !== 'none') {{
        const container = cards[0]?.parentElement;
        if (container) {{
          const visibleCards = cards.filter(card => card.style.display !== 'none');
          visibleCards.sort((a, b) => {{
            switch (sortBy) {{
              case 'importance':
                return parseInt(b.dataset.importance || '0') - parseInt(a.dataset.importance || '0');
              case 'freshness':
                return parseInt(b.dataset.freshness || '0') - parseInt(a.dataset.freshness || '0');
              case 'time':
                const timeA = new Date(a.querySelector('.time-ago')?.dataset.timestamp || '2023-01-01');
                const timeB = new Date(b.querySelector('.time-ago')?.dataset.timestamp || '2023-01-01');
                return timeB - timeA;
              default:
                return 0;
            }}
          }});

          visibleCards.forEach(card => container.appendChild(card));
        }}
      }}
    }}

    // ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼è¨­å®š
    if (searchBox) {{
      searchBox.addEventListener('input', filterAndSortCards);
    }}

    // ãƒ•ã‚£ãƒ«ã‚¿ãƒ»ã‚½ãƒ¼ãƒˆUIã‚’å‹•çš„ã«è¿½åŠ 
    document.addEventListener('DOMContentLoaded', function() {{
      const searchContainer = document.querySelector('.search-container');
      if (searchContainer) {{
        searchContainer.innerHTML += `
          <div class="filter-controls">
            <select id="importanceFilter" title="é‡è¦åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼">
              <option value="0">å…¨é‡è¦åº¦</option>
              <option value="90">ğŸ”¥ æœ€é«˜é‡è¦åº¦ã®ã¿</option>
              <option value="75">â­ é«˜é‡è¦åº¦ä»¥ä¸Š</option>
              <option value="50">ğŸ“Š ä¸­é‡è¦åº¦ä»¥ä¸Š</option>
            </select>
            <select id="sortSelect" title="ã‚½ãƒ¼ãƒˆé †">
              <option value="importance">é‡è¦åº¦é †</option>
              <option value="freshness">é®®åº¦é †</option>
              <option value="time">æ™‚é–“é †</option>
              <option value="none">ã‚½ãƒ¼ãƒˆãªã—</option>
            </select>
          </div>
        `;

        // æ–°ã—ã„ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼
        const importanceFilter = document.getElementById('importanceFilter');
        const sortSelect = document.getElementById('sortSelect');

        if (importanceFilter) importanceFilter.addEventListener('change', filterAndSortCards);
        if (sortSelect) sortSelect.addEventListener('change', filterAndSortCards);
      }}

      // åˆæœŸã‚½ãƒ¼ãƒˆå®Ÿè¡Œ
      setTimeout(filterAndSortCards, 100);
    }});
  </script>
</body>
</html>
"""

SECTION_TMPL = """
<section id="{sec_id}" class="tab-panel {extra_class}">
{cards}
</section>
"""

CARD_TMPL = """
<article class="card {priority_class}" data-importance="{importance_score}" data-source-trust="{source_trust}">
  <div class="card-priority">
    <div class="priority-badge {priority_class}">
      <span class="priority-score">{importance_score}</span>
      <span class="priority-label">{priority_label}</span>
    </div>
    <div class="trust-indicator" title="ã‚½ãƒ¼ã‚¹ä¿¡é ¼åº¦: {source_trust}%">
      <div class="trust-bar" style="width: {source_trust}%"></div>
    </div>
  </div>

  <div class="card-header">
    <div class="card-meta">
      <span class="source-badge {source_type_class}">{source_name}</span>
      <span class="time-ago">{ago}</span>
    </div>
    <a class="card-title" href="{link}" target="_blank" rel="noopener">{title}</a>
  </div>

  <div class="card-body">
    <p class="card-summary">{summary}</p>
    <div class="card-indicators">
      <span class="indicator {translation_class}" title="{translation_title}">
        {summary_lang}
      </span>
      <span class="indicator {freshness_class}" title="é®®åº¦: {freshness_score}/100">
        ğŸ• {freshness_indicator}
      </span>
    </div>
  </div>

  <div class="card-footer">
    <div class="card-actions">
      <a href="{link}" target="_blank" rel="noopener" class="action-link">è©³ç´°ã‚’èª­ã‚€</a>
      <span class="reading-time">ğŸ“– ç´„{min_read_time}åˆ†</span>
    </div>
    <a href="{link}" target="_blank" rel="noopener" class="source-link">å‡ºå…¸å…ƒ</a>
  </div>
</article>
"""

EMPTY_TMPL = '<div class="empty">æ–°ç€ãªã—ï¼ˆæœŸé–“ã‚’åºƒã’ã‚‹ã‹ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼‰</div>'

def ago_str(dt: datetime) -> str:
    delta = NOW - dt
    secs = int(delta.total_seconds())
    if secs < 60: return f"{secs}ç§’å‰"
    mins = secs // 60
    if mins < 60: return f"{mins}åˆ†å‰"
    hrs = mins // 60
    if hrs < 24: return f"{hrs}æ™‚é–“å‰"
    days = hrs // 24
    return f"{days}æ—¥å‰"

def clean_html(s: str) -> str:
    if not s: return ""
    # strip tags very lightly
    s = re.sub(r"<.*?>", "", s)
    s = s.replace("&nbsp;", " ").strip()
    return html.escape(s, quote=False)

def pick_summary(entry) -> str:
    for key in ("summary", "subtitle", "description"):
        if key in entry and entry[key]:
            return clean_html(entry[key])
    return clean_html(entry.get("title", ""))

def parse_feeds():
    raw = yaml.safe_load(Path("feeds.yml").read_text(encoding="utf-8"))
    return raw or {}

def get_category(conf, category_name):
    """Case-insensitive category lookup"""
    # Try exact match first
    if category_name in conf:
        return conf[category_name]
    # Try case-insensitive match
    for key, value in conf.items():
        if key.lower() == category_name.lower():
            return value
    return []

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆï¼ˆå‡¦ç†åŠ¹ç‡åŒ–ï¼‰
INVESTMENT_KEYWORDS = frozenset([
    'funding', 'investment', 'ipo', 'venture', 'capital', 'm&a', 'acquisition',
    'æŠ•è³‡', 'è³‡é‡‘èª¿é”', 'ipo', 'ãƒ™ãƒ³ãƒãƒ£ãƒ¼', 'è³‡æœ¬', 'è²·å', 'åˆä½µ'
])

STRATEGY_KEYWORDS = frozenset([
    'strategy', 'executive', 'ceo', 'leadership', 'transformation', 'partnership',
    'æˆ¦ç•¥', 'çµŒå–¶', 'ceo', 'ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—', 'å¤‰é©', 'ææº'
])

GOVERNANCE_KEYWORDS = frozenset([
    'regulation', 'policy', 'compliance', 'ethics', 'governance', 'law',
    'è¦åˆ¶', 'æ”¿ç­–', 'ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹', 'å€«ç†', 'ã‚¬ãƒãƒŠãƒ³ã‚¹', 'æ³•å¾‹'
])

def categorize_business_news(item, feeds_info):
    """ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
    try:
        business_category = feeds_info.get('business_category', 'general')
        title = item.get('title', '').lower() if item.get('title') else ''
        summary = item.get('_summary', '').lower() if item.get('_summary') else ''
        content = f"{title} {summary}"

        # è¨­å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã‚’å„ªå…ˆ
        if business_category in ['strategy', 'investment', 'japan_business', 'governance']:
            return business_category

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿåˆ†é¡ï¼ˆset lookupï¼‰
        if any(kw in content for kw in INVESTMENT_KEYWORDS):
            return 'investment'
        if any(kw in content for kw in STRATEGY_KEYWORDS):
            return 'strategy'
        if any(kw in content for kw in GOVERNANCE_KEYWORDS):
            return 'governance'

        return 'general'

    except Exception as e:
        print(f"[WARN] Error in categorize_business_news: {e}")
        return 'general'

def within_window(published_parsed):
    """æ™‚é–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    try:
        if not published_parsed:
            return True, NOW  # å…¬é–‹æ—¥ä¸æ˜ã®å ´åˆã€ä¿æŒã—ã¦ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨

        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å¤‰æ›ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
        dt = datetime.fromtimestamp(time.mktime(published_parsed), tz=timezone.utc).astimezone(JST)

        # æœªæ¥ã®æ—¥ä»˜ãƒã‚§ãƒƒã‚¯
        if dt > NOW:
            print(f"[DEBUG] Future date detected: {dt}")
            return False, dt

        # æ™‚é–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒã‚§ãƒƒã‚¯
        time_diff = NOW - dt
        is_within = time_diff <= timedelta(hours=HOURS_LOOKBACK)

        if not is_within:
            hours_old = time_diff.total_seconds() / 3600
            print(f"[DEBUG] Item too old ({hours_old:.1f}h): {dt}")

        return is_within, dt

    except (OSError, ValueError, OverflowError) as e:
        print(f"[WARN] Time parsing error: {e}")
        return True, NOW  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¿æŒ

def is_ai_relevant(title: str, summary: str) -> bool:
    """
    AIã«é–¢é€£æ€§ã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    ã‚ˆã‚Šå³æ ¼ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§è³ªã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’é¸åˆ¥
    """
    content = f"{title} {summary}".lower()
    
    # é«˜é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã‚‰ãŒã‚ã‚Œã°å¿…ãšå«ã‚ã‚‹ï¼‰
    high_relevance = [
        'artificial intelligence', 'machine learning', 'deep learning', 'neural network',
        'gpt', 'llm', 'large language model', 'transformer', 'bert', 'claude',
        'chatgpt', 'gemini', 'copilot', 'anthropic', 'openai', 'deepmind',
        'computer vision', 'natural language processing', 'nlp', 'reinforcement learning',
        'generative ai', 'ai model', 'ai research', 'ai breakthrough',
        'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ',
                'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ',
        'ï¼¡ï¼©', 'AI', 'ML', 'DL', 'ç”ŸæˆAI', 'ã‚¸ã‚§ãƒãƒ¬ãƒ¼ãƒ†ã‚£ãƒ–AI',
        'ãƒãƒ£ãƒƒãƒˆGPT', 'ChatGPT', 'GPT', 'LLM', 'å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«',
        'Claude', 'Gemini', 'Copilot', 'Bard',
        'è‡ªç„¶è¨€èªå‡¦ç†', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³', 'ç”»åƒèªè­˜', 'éŸ³å£°èªè­˜',
        'ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹', 'è‡ªå‹•é‹è»¢', 'äºˆæ¸¬åˆ†æ', 'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹',
        'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ', 'æœ€é©åŒ–', 'ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³',
        'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—', 'è³‡é‡‘èª¿é”', 'æŠ•è³‡', 'ãƒ•ã‚¡ãƒ³ãƒ‰', 'IPO', 'M&A',
        'ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯', 'ãƒˆãƒ¨ã‚¿', 'NTT', 'ã‚½ãƒ‹ãƒ¼', 'æ—¥ç«‹', 'å¯Œå£«é€š', 'NEC',
        'ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯', 'æ¥½å¤©', 'ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ', 'ãƒ¡ãƒ«ã‚«ãƒª', 'LINE',
    ]
    
    # ä¸­é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ã‚ã‚Œã°å«ã‚ã‚‹ï¼‰
    medium_relevance = [
        'algorithm', 'automation', 'robot', 'autonomous', 'prediction',
        'data science', 'analytics', 'intelligent', 'smart system',
        'cognitive', 'inference', 'classification', 'recognition',
        'generation', 'synthesis', 'optimization', 'recommendation',
        'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ', 'è‡ªå‹•åŒ–', 'ãƒ­ãƒœãƒƒãƒˆ', 'è‡ªå¾‹', 'äºˆæ¸¬',
        'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹', 'åˆ†æ', 'ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ', 'ã‚¹ãƒãƒ¼ãƒˆ',
        'èªè­˜', 'ç”Ÿæˆ', 'æœ€é©åŒ–', 'ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰'
    ]
    
    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã‚‰ãŒã‚ã‚Œã°é™¤å¤–ï¼‰
    exclude_keywords = [
        'cryptocurrency', 'crypto', 'blockchain', 'bitcoin', 'nft',
        'gaming', 'game', 'sports', 'entertainment', 'music', 'movie',
        'politics', 'political', 'election', 'government policy',
        'weather', 'climate change', 'environmental',
        'æš—å·é€šè²¨', 'ã‚²ãƒ¼ãƒ ', 'ã‚¹ãƒãƒ¼ãƒ„', 'å¨¯æ¥½', 'éŸ³æ¥½', 'æ˜ ç”»',
                'æš—å·é€šè²¨', 'ã‚²ãƒ¼ãƒ ', 'ã‚¹ãƒãƒ¼ãƒ„', 'å¨¯æ¥½', 'éŸ³æ¥½', 'æ˜ ç”»',
        'æ”¿æ²»', 'é¸æŒ™', 'å¤©æ°—', 'æ°—å€™å¤‰å‹•', 'ç’°å¢ƒ',
        'ã‚¢ãƒ‹ãƒ¡', 'ãƒãƒ³ã‚¬', 'èŠ¸èƒ½', 'ã‚¿ãƒ¬ãƒ³ãƒˆ', 'ã‚¢ã‚¤ãƒ‰ãƒ«',
        'æ‹æ„›', 'çµå©š', 'ã‚°ãƒ«ãƒ¡', 'æ–™ç†', 'æ—…è¡Œ', 'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³'
    ]
    
    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    for keyword in exclude_keywords:
        if keyword in content:
            return False
    
    # é«˜é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    high_score = sum(1 for keyword in high_relevance if keyword in content)
    if high_score >= 1:
        return True
    
    # ä¸­é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆ2ã¤ä»¥ä¸Šã§æ¡ç”¨ï¼‰
    medium_score = sum(1 for keyword in medium_relevance if keyword in content)
    if medium_score >= 2:
        return True
    
    return False


def calculate_importance_score(item):
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    å¤§ããªãƒ‹ãƒ¥ãƒ¼ã‚¹ã»ã©é«˜ã„ã‚¹ã‚³ã‚¢ã‚’è¿”ã™
    """
    title = item.get("title", "").lower()
    summary = item.get("_summary", "").lower()
    source = item.get("_source", "").lower()
    content = f"{title} {summary}"
    
    score = 0
    
    # 1. ä¼æ¥­ãƒ»çµ„ç¹”ã®é‡è¦åº¦ï¼ˆå¤§æ‰‹ä¼æ¥­ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
    major_companies = {
        'openai': 100, 'anthropic': 100, 'google': 90, 'microsoft': 90,
        'meta': 85, 'nvidia': 85, 'apple': 80, 'amazon': 80,
        'tesla': 75, 'deepmind': 95, 'cohere': 70, 'hugging face': 70,
        'mistral': 65, 'stability ai': 65, 'midjourney': 60
    }
    
    for company, points in major_companies.items():
        if company in content:
            score += points
            break  # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã¿é©ç”¨
    
    # 2. é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç”»æœŸçš„ãªç™ºè¡¨ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
    high_impact_keywords = {
        'breakthrough': 80, 'launch': 70, 'release': 65, 'announce': 60,
        'unveil': 75, 'introduce': 60, 'partnership': 55, 'acquisition': 85,
        'funding': 70, 'investment': 65, 'ipo': 90, 'valuation': 60,
        'gpt-5': 100, 'gpt-4': 80, 'claude': 70, 'gemini': 70,
        'billion': 75, 'million': 50, 'record': 65, 'first': 60
    }
    
    for keyword, points in high_impact_keywords.items():
        if keyword in content:
            score += points * 0.5  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚0.5å€
    
    # 3. ã‚½ãƒ¼ã‚¹ã®ä¿¡é ¼æ€§ãƒ»å½±éŸ¿åŠ›
    source_credibility = {
        'techcrunch': 80, 'bloomberg': 90, 'reuters': 85, 'wsj': 85,
        'financial times': 80, 'the verge': 70, 'wired': 70,
        'mit technology review': 85, 'nature': 95, 'science': 95,
        'anthropic': 90, 'openai': 90, 'google': 85, 'meta': 80
    }
    
    for src, points in source_credibility.items():
        if src in source:
            score += points * 0.3  # ã‚½ãƒ¼ã‚¹ä¿¡é ¼æ€§ã¯30%ã®é‡ã¿
            break
    
    # 4. æŠ€è¡“çš„é‡è¦åº¦
    tech_importance = {
        'artificial general intelligence': 100, 'agi': 100,
        'multimodal': 70, 'reasoning': 60, 'safety': 65,
        'alignment': 70, 'robotics': 60, 'autonomous': 55,
        'quantum': 70, 'neural network': 50, 'transformer': 60
    }
    
    for tech, points in tech_importance.items():
        if tech in content:
            score += points * 0.4
    
    # 5. æ–°é®®åº¦ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ãƒœãƒ¼ãƒŠã‚¹ï¼‰
    dt = item.get("_dt")
    if dt:
        hours_old = (NOW - dt).total_seconds() / 3600
        if hours_old < 6:  # 6æ™‚é–“ä»¥å†…
            score += 30
        elif hours_old < 12:  # 12æ™‚é–“ä»¥å†…
            score += 20
        elif hours_old < 24:  # 24æ™‚é–“ä»¥å†…
            score += 10
    
    # 6. ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã•ï¼ˆè©³ç´°ãªã‚¿ã‚¤ãƒˆãƒ«ã»ã©ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¾¡å€¤é«˜ã„ï¼‰
    title_length = len(item.get("title", ""))
    if title_length > 80:
        score += 15
    elif title_length > 50:
        score += 10
    
    return max(score, 0)  # è² ã®ã‚¹ã‚³ã‚¢ã¯0ã«

def calculate_sns_importance_score(item):
    """
    SNSãƒã‚¹ãƒˆã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ8/14ä»¥é™ã®æ–°ã—ã„æƒ…å ±ç”¨ï¼‰
    ä¼æ¥­ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã€ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã€å†…å®¹ã®é‡è¦åº¦ã§åˆ¤å®š
    """
    title = item.get("title", "").lower()
    summary = item.get("_summary", "").lower()
    username = ""
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŠ½å‡º
    if "xãƒã‚¹ãƒˆ" in title:
        username = title.replace("xãƒã‚¹ãƒˆ", "").strip().lower()
    
    content = f"{title} {summary}"
    score = 0
    
    # 1. ä¼æ¥­ãƒ»çµ„ç¹”ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®é‡è¦åº¦ï¼ˆå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
    enterprise_accounts = {
        '@openai': 100, '@anthropic': 100, '@google': 90, '@microsoft': 90,
        '@meta': 85, '@nvidia': 85, '@apple': 80, '@amazon': 80,
        '@deepmind': 95, '@huggingface': 80, '@langchainai': 75,
        '@cohereai': 70, '@stabilityai': 70, '@midjourney': 65,
        # æ—¥æœ¬ä¼æ¥­ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ  
        '@softbank': 80, '@toyota': 75, '@nttcom': 70, '@sony': 70,
        '@hitachi_ltd': 65, '@fujitsu_global': 65, '@nec_corp': 65,
        '@rakuten': 60, '@recruit_jp': 55, '@mercari_jp': 50,
        # AIç ”ç©¶è€…ãƒ»ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼
        '@ylecun': 90, '@karpathy': 90, '@jeffdean': 85, '@goodfellow_ian': 85,
        '@elonmusk': 75, '@satyanadella': 80, '@sundarpichai': 80,
        '@sama': 95, '@darioacemoglu': 80, '@fchollet': 85,
        '@hardmaru': 75, '@adcock_brett': 70, '@minimaxir': 65,
        # æ—¥æœ¬ã®AIç ”ç©¶è€…ãƒ»ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼
        '@karaage0703': 70, '@shi3z': 65, '@yukihiko_n': 60,
        '@npaka': 65, '@ohtaman': 60, '@toukubo': 55,
        # ãã®ä»–ã®è‘—åäºº
        '@windsurf': 60, '@oikon48': 55, '@godofprompt': 50,
        '@newsfromgoogle': 70, '@suh_sunaneko': 50, '@pop_ikeda': 45
    }
    
    for account, points in enterprise_accounts.items():
        if account in username or account.replace('@', '') in username:
            score += points
            break  # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã¿é©ç”¨
    
    # 2. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é‡è¦åº¦ï¼ˆæŠ€è¡“çš„ãªå†…å®¹ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
    high_value_keywords = {
        'breakthrough': 50, 'release': 40, 'launch': 40, 'announce': 35,
        'gpt-5': 80, 'gpt-4': 60, 'claude': 50, 'gemini': 50,
        'research': 40, 'paper': 35, 'model': 30, 'ai': 20,
        'artificial intelligence': 40, 'machine learning': 35,
        'deep learning': 35, 'neural network': 30,
        # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        'äººå·¥çŸ¥èƒ½': 35, 'æ©Ÿæ¢°å­¦ç¿’': 30, 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°': 30,
        'ç”Ÿæˆai': 45, 'chatgpt': 40, 'å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«': 35,
        'ç ”ç©¶': 30, 'è«–æ–‡': 25, 'ãƒ¢ãƒ‡ãƒ«': 20, 'ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼': 45,
        'è³‡é‡‘èª¿é”': 40, 'æŠ•è³‡': 35, 'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—': 30
    }
    
    for keyword, points in high_value_keywords.items():
        if keyword in content:
            score += points * 0.3  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚0.3å€
    
    # 3. ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™
    engagement_indicators = {
        'thread': 15, 'important': 20, 'must read': 25, 'breaking': 30,
        'update': 10, 'new': 15, 'latest': 10, 'just': 10,
        'é‡è¦': 20, 'å¿…è¦‹': 25, 'æœ€æ–°': 10, 'é€Ÿå ±': 30, 'æ›´æ–°': 10,
        'è§£æ±º': 20, 'ã¤ã„ã«': 15, 'å•é¡Œ': 10
    }
    
    for indicator, points in engagement_indicators.items():
        if indicator in content:
            score += points * 0.2
    
    # 4. æŠ•ç¨¿ã®æ–°é®®åº¦ï¼ˆ8/14ä»¥é™ã®æ–°ã—ã•ã‚’é‡è¦–ï¼‰
    dt = item.get("_dt")
    if dt:
        aug14_jst = datetime(2025, 8, 14, 0, 0, 0, tzinfo=JST)
        hours_since_aug14 = (dt - aug14_jst).total_seconds() / 3600
        
        # 8/15ã®æŠ•ç¨¿ã«æœ€é«˜ãƒœãƒ¼ãƒŠã‚¹
        if hours_since_aug14 >= 24:  # 8/15ä»¥é™
            score += 30
        elif hours_since_aug14 >= 12:  # 8/14åˆå¾Œ
            score += 20
        elif hours_since_aug14 >= 0:  # 8/14æœ
            score += 10
    
    # 5. ãƒ†ã‚­ã‚¹ãƒˆé•·ãƒœãƒ¼ãƒŠã‚¹ï¼ˆè©³ç´°ãªæŠ•ç¨¿ã»ã©é«˜ä¾¡å€¤ï¼‰
    text_length = len(summary)
    if text_length > 100:
        score += 10
    elif text_length > 50:
        score += 5
    
    return max(score, 0)  # è² ã®ã‚¹ã‚³ã‚¢ã¯0ã«

def calculate_source_trust(source_name):
    """ã‚½ãƒ¼ã‚¹ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—ï¼ˆ0-100ï¼‰"""
    trust_scores = {
        'OpenAI': 95, 'Anthropic': 95, 'Google': 90, 'Microsoft': 90,
        'Meta': 85, 'Nvidia': 85, 'Apple': 80, 'Amazon': 80,
        'DeepMind': 90, 'Hugging Face': 85, 'MIT Technology Review': 90,
        'Nature': 95, 'Science': 95, 'Reuters': 90, 'Bloomberg': 90,
        'TechCrunch': 80, 'The Verge': 75, 'VentureBeat': 80,
        'X': 60, 'SNS': 60, 'Reddit': 70, 'arXiv': 90
    }
    return trust_scores.get(source_name, 50)

def calculate_freshness_score(dt):
    """é®®åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-100ï¼‰"""
    if not dt:
        return 50
    hours_old = (NOW - dt).total_seconds() / 3600
    if hours_old < 6:
        return 100  # 6æ™‚é–“ä»¥å†…
    elif hours_old < 12:
        return 90   # 12æ™‚é–“ä»¥å†…
    elif hours_old < 24:
        return 80   # 24æ™‚é–“ä»¥å†…
    elif hours_old < 48:
        return 60   # 2æ—¥ä»¥å†…
    elif hours_old < 168:  # 1é€±é–“
        return max(20, 60 - (hours_old - 48) / 2)
    else:
        return max(10, 20 - (hours_old - 168) / 100)

def get_freshness_indicator(score):
    """é®®åº¦ã‚¹ã‚³ã‚¢ã«åŸºã¥ãè¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆ"""
    if score >= 90: return "ğŸ”¥ æ–°ç€"
    elif score >= 70: return "ğŸ• æ–°é®®"
    elif score >= 50: return "ğŸ“„ æ™®é€š"
    else: return "ğŸ—‚ï¸ å¤ã„"

def estimate_reading_time(text):
    """æ¨å®šèª­æ›¸æ™‚é–“ã‚’è¨ˆç®—ï¼ˆåˆ†ï¼‰"""
    if not text:
        return 1
    word_count = len(text.split())
    return max(1, round(word_count / 200))  # 200 words per minute

def build_cards(items, translator):
    """æ”¹å–„ç‰ˆã‚«ãƒ¼ãƒ‰ç”Ÿæˆé–¢æ•°"""
    cards = []
    for it in items[:MAX_ITEMS_PER_CATEGORY]:
        title = it.get("title") or "(no title)"
        link = it.get("link") or "#"
        src = it.get("_source") or ""
        dt = it.get("_dt") or NOW
        raw_summary = it.get("_summary") or ""
        importance_score = it.get("_importance_score", 50)

        # é‡è¦åº¦ã‚¯ãƒ©ã‚¹æ±ºå®š
        if importance_score >= 90:
            priority_class = "high-priority"
            priority_label = "æœ€é«˜"
        elif importance_score >= 75:
            priority_class = "medium-priority"
            priority_label = "é«˜"
        elif importance_score >= 50:
            priority_class = "normal-priority"
            priority_label = "ä¸­"
        else:
            priority_class = "low-priority"
            priority_label = "ä½"

        # ã‚½ãƒ¼ã‚¹ä¿¡é ¼åº¦è¨ˆç®—
        source_trust = calculate_source_trust(src)

        # é®®åº¦è¨ˆç®—
        freshness_score = calculate_freshness_score(dt)
        freshness_indicator = get_freshness_indicator(freshness_score)
        freshness_class = "fresh" if freshness_score >= 70 else "stale"

        # ç¿»è¨³å‡¦ç†
        ja_summary = raw_summary
        did_translate = False
        translation_class = "original"
        translation_title = "åŸæ–‡"

        if TRANSLATE_TO_JA and translator and raw_summary and not looks_japanese(raw_summary):
            cache_key = f"{link}::{hash(raw_summary)}"
            cached = TRANSLATION_CACHE.get(cache_key)
            if cached:
                ja_summary = cached
                did_translate = True
            else:
                try:
                    ja = translator.translate(raw_summary)
                    if ja and ja != raw_summary:
                        ja_summary = ja
                        TRANSLATION_CACHE[cache_key] = ja_summary
                        did_translate = True
                except Exception as e:
                    print(f"[WARN] Translation failed for {link[:50]}: {e}")

        # ç¿»è¨³çŠ¶æ…‹è¨­å®š
        if did_translate:
            translation_class = "translated"
            translation_title = "Gemini AI ç¿»è¨³"
        else:
            translation_class = "original"
            translation_title = "è‹±èªåŸæ–‡"

        # è¦ç´„ã®æ–‡å­—æ•°åˆ¶é™
        final_summary = ja_summary if did_translate else raw_summary
        if len(final_summary) > 300:
            final_summary = final_summary[:300] + '...'

        # èª­æ›¸æ™‚é–“æ¨å®š
        min_read_time = estimate_reading_time(final_summary)

        # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—åˆ¤å®š
        source_type_class = "official" if any(word in src.lower() for word in ['openai', 'anthropic', 'google', 'microsoft', 'meta']) else "news"

        cards.append(CARD_TMPL.format(
            link=html.escape(link, quote=True),
            title=html.escape(title, quote=False),
            summary=final_summary,
            source_name=html.escape(src, quote=False),
            summary_lang=("æ—¥æœ¬èª" if did_translate else "è‹±èª"),
            ago=ago_str(dt),
            importance_score=importance_score,
            priority_class=priority_class,
            priority_label=priority_label,
            source_trust=source_trust,
            freshness_score=freshness_score,
            freshness_indicator=freshness_indicator,
            freshness_class=freshness_class,
            translation_class=translation_class,
            translation_title=translation_title,
            min_read_time=min_read_time,
            source_type_class=source_type_class
        ))

    return "\n".join(cards) if cards else EMPTY_TMPL

def gather_items(feeds, category_name):
    items = []
    print(f"[INFO] Processing {len(feeds)} feeds for {category_name}")
    for f in feeds:
        url = f.get("url")
        name = f.get("name", url)
        if not url: 
            print(f"[WARN] No URL for feed: {name}")
            continue
        try:
            print(f"[INFO] Fetching: {name}")
            # User-Agentã‚’è¨­å®šã—ã¦ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦ã‚’å›é¿
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            # 403ã‚¨ãƒ©ãƒ¼å¯¾ç­–: ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
            retry_count = 0
            max_retries = 2
            d = None
            
            while retry_count <= max_retries:
                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆåˆ¶é™ä»˜ãã§ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
                    import socket
                    original_timeout = socket.getdefaulttimeout()
                    socket.setdefaulttimeout(8)  # 8ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    
                    d = feedparser.parse(url, agent=headers['User-Agent'])
                    
                    socket.setdefaulttimeout(original_timeout)
                    # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                    if hasattr(d, 'status') and d.status == 403:
                        print(f"[WARN] 403 Forbidden for {name}, trying advanced fetch...")
                        # é«˜åº¦ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å†è©¦è¡Œ
                        d = advanced_feed_fetch(url, name)
                        if d is None:
                            print(f"[ERROR] Advanced fetch also failed for {name}")
                            break
                    break
                except Exception as retry_e:
                    retry_count += 1
                    if retry_count <= max_retries:
                        print(f"[WARN] Retry {retry_count}/{max_retries} for {name}: {retry_e}")
                        # é«˜åº¦ãªå–å¾—ã‚’è©¦è¡Œ
                        if 'google.com' in url:
                            print(f"[INFO] Trying advanced fetch for Google service: {name}")
                            d = advanced_feed_fetch(url, name)
                            if d is not None:
                                break
                        import time
                        time.sleep(2)  # 2ç§’å¾…æ©Ÿ
                    else:
                        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦é«˜åº¦ãªå–å¾—ã‚’è©¦è¡Œ
                        print(f"[INFO] Final attempt with advanced fetch for {name}")
                        d = advanced_feed_fetch(url, name)
                        if d is None:
                            raise retry_e
            
            if d and d.bozo:
                print(f"[WARN] Feed parse warning for {name}: {getattr(d, 'bozo_exception', 'unknown')}")
        except Exception as e:
            print(f"[ERROR] feed parse error: {name}: {e}")
            continue
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ãŒå¤±æ•—ã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not d or not hasattr(d, 'entries'):
            print(f"[WARN] No valid feed data for {name}, skipping...")
            continue
            
        entry_count = 0
        filtered_count = 0
        for e in d.entries:
            ok, dt = within_window(e.get("published_parsed") or e.get("updated_parsed"))
            if not ok:
                continue
            
            title = e.get("title", "")
            summary = pick_summary(e)
            
            # generalãƒ•ã‚£ãƒ¼ãƒ‰ã®å ´åˆã¯AIé–¢é€£åº¦ã‚’ãƒã‚§ãƒƒã‚¯
            is_general_feed = f.get("general", False)
            if is_general_feed:
                if not is_ai_relevant(title, summary):
                    filtered_count += 1
                    continue
            
            # 403ã‚¨ãƒ©ãƒ¼URLã‚’ãƒã‚§ãƒƒã‚¯
            link_url = e.get("link", "")
            if is_403_url(link_url):
                print(f"ğŸš« 403 URLé™¤å¤–: {title[:50]}...")
                continue
                
            entry_count += 1
            # é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            importance_score = 50  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            if category_name == "Business":
                importance_score = calculate_importance_score({
                    "title": title,
                    "_summary": summary,
                    "_source": name,
                    "_dt": dt
                })
            elif category_name == "Posts":
                importance_score = calculate_sns_importance_score({
                    "title": title,
                    "_summary": summary,
                    "_dt": dt
                })

            items.append({
                "title": title,
                "link": link_url,
                "_summary": summary,
                "_source": name,
                "_dt": dt,
                "_importance_score": importance_score
            })
        if entry_count > 0:
            if filtered_count > 0:
                print(f"[INFO] Found {entry_count} recent items from {name} (filtered out {filtered_count} non-AI items)")
            else:
                print(f"[INFO] Found {entry_count} recent items from {name}")
    # ã‚¹ãƒãƒ¼ãƒˆã‚½ãƒ¼ãƒˆ: é‡è¦åº¦ã¨æ™‚åˆ»ã‚’çµ„ã¿åˆã‚ã›ã¦ä¸¦ã³æ›¿ãˆ
    if category_name == "Business":
        # ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯é‡è¦åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        items.sort(key=lambda x: (calculate_importance_score(x), x["_dt"]), reverse=True)
        print(f"[INFO] {category_name}: Sorted by importance score")
    elif category_name == "Posts":
        # SNS/è«–æ–‡ãƒã‚¹ãƒˆã¯é‡è¦åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        items.sort(key=lambda x: (calculate_sns_importance_score(x), x["_dt"]), reverse=True)
        print(f"[INFO] {category_name}: Sorted by SNS importance score")
    else:
        # ãƒ„ãƒ¼ãƒ«ã‚«ãƒ†ã‚´ãƒªã¯æ™‚åˆ»é †
        items.sort(key=lambda x: x["_dt"], reverse=True)
    
    # æœ€çµ‚ãƒã‚§ãƒƒã‚¯: 403 URLãŒãªã„ã“ã¨ã‚’ç¢ºèª
    items_before_filter = len(items)
    items = filter_403_urls(items)
    items_after_filter = len(items)
    
    if items_before_filter != items_after_filter:
        print(f"âœ… {category_name}: æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§{items_before_filter - items_after_filter}ä»¶ã®403 URLã‚’é™¤å¤–")
    
    print(f"[INFO] {category_name}: Total {len(items)} items found")
    return items

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    start_time = time.time()

    print(f"\n{'='*60}")
    print(f"ğŸš€ Daily AI News Build Started")
    print(f"ğŸ“… {NOW.strftime('%Y-%m-%d %H:%M:%S JST')}")
    print(f"âš™ï¸  Config: LOOKBACK={HOURS_LOOKBACK}h, MAX_ITEMS={MAX_ITEMS_PER_CATEGORY}")
    print(f"ğŸ”¤ Translate: {TRANSLATE_TO_JA} (engine: {TRANSLATE_ENGINE})")
    print(f"{'='*60}\n")
    
    global TRANSLATION_CACHE
    TRANSLATION_CACHE = load_cache()
    print(f"[INFO] Loaded {len(TRANSLATION_CACHE)} cached translations")

    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
    if CONFIG['debug_mode']:
        print(f"[DEBUG] Feed sources: {len(parse_feeds())} categories")
        print(f"[DEBUG] Translation engine: {CONFIG['translate_engine']}")
        print(f"[DEBUG] Gemini API: {'enabled' if CONFIG['gemini_api_key'] else 'disabled'}")

    try:
        feeds_conf = parse_feeds()
        print(f"[INFO] Loaded {sum(len(v) for v in feeds_conf.values())} feeds from feeds.yml")
    except Exception as e:
        print(f"[ERROR] Failed to parse feeds.yml: {e}")
        feeds_conf = {}
    
    # Gather items with error handling
    try:
        business = gather_items(get_category(feeds_conf, "Business"), "Business")
        print(f"[INFO] Gathered {len(business)} Business items")
    except Exception as e:
        print(f"[ERROR] Failed to gather Business items: {e}")
        business = []
    
    try:
        tools = gather_items(get_category(feeds_conf, "Tools"), "Tools")
        print(f"[INFO] Gathered {len(tools)} Tools items")
    except Exception as e:
        print(f"[ERROR] Failed to gather Tools items: {e}")
        tools = []
    
    try:
        posts = gather_items(get_category(feeds_conf, "Posts"), "Posts")
        print(f"[INFO] Gathered {len(posts)} Posts items")
    except Exception as e:
        print(f"[ERROR] Failed to gather Posts items: {e}")
        posts = []
    
    # Remove global duplicates across all categories first
    print(f"[INFO] Removing duplicates across all categories...")
    all_items = business + tools + posts
    print(f"[INFO] Before deduplication: {len(all_items)} total items")
    
    seen_links = set()
    seen_titles = set()
    unique_business = []
    unique_tools = []
    unique_posts = []
    
    # Process each category and remove duplicates
    for item in business:
        link = item.get('link', '')
        title = item.get('title', '').lower().strip()
        if link not in seen_links and title not in seen_titles:
            unique_business.append(item)
            seen_links.add(link)
            seen_titles.add(title)
    
    for item in tools:
        link = item.get('link', '')
        title = item.get('title', '').lower().strip()
        if link not in seen_links and title not in seen_titles:
            unique_tools.append(item)
            seen_links.add(link)
            seen_titles.add(title)
    
    for item in posts:
        link = item.get('link', '')
        title = item.get('title', '').lower().strip()
        if link not in seen_links and title not in seen_titles:
            unique_posts.append(item)
            seen_links.add(link)
            seen_titles.add(title)
    
    # Update with deduplicated items
    business = unique_business
    tools = unique_tools  
    posts = unique_posts
    
    print(f"[INFO] After deduplication: Business={len(business)}, Tools={len(tools)}, Posts={len(posts)}")
    
    # Inject X posts
    if X_POSTS_CSV:
        try:
            x_posts = gather_x_posts(X_POSTS_CSV)
            if x_posts:
                print(f"[INFO] Adding {len(x_posts)} X posts")
                # Only add X posts that aren't already in posts
                for x_post in x_posts:
                    x_link = x_post.get('link', '')
                    x_title = x_post.get('title', '').lower().strip()
                    if x_link not in seen_links and x_title not in seen_titles:
                        posts.append(x_post)
                        seen_links.add(x_link)
                        seen_titles.add(x_title)
            else:
                print(f"[INFO] No X posts to add")
            posts = sorted(posts, key=lambda x: x.get('_dt', NOW), reverse=True)
        except Exception as e:
            print(f"[WARN] Failed to process X posts: {e}")


    try:
        translator = JaTranslator(engine=TRANSLATE_ENGINE)
    except Exception as e:
        print(f"[WARN] Failed to initialize translator: {e}")
        translator = None

    sections_html = []
    try:
        sections_html.append(SECTION_TMPL.format(
            sec_id="business",
            extra_class="",
            cards=build_cards(business[:MAX_ITEMS_PER_CATEGORY], translator)
        ))
    except Exception as e:
        print(f"[ERROR] Failed to build Business section: {e}")
        sections_html.append(SECTION_TMPL.format(sec_id="business", extra_class="", cards=EMPTY_TMPL))
    
    try:
        sections_html.append(SECTION_TMPL.format(
            sec_id="tools",
            extra_class="hidden",
            cards=build_cards(tools[:MAX_ITEMS_PER_CATEGORY], translator)
        ))
    except Exception as e:
        print(f"[ERROR] Failed to build Tools section: {e}")
        sections_html.append(SECTION_TMPL.format(sec_id="tools", extra_class="hidden", cards=EMPTY_TMPL))
    
    try:
        sections_html.append(SECTION_TMPL.format(
            sec_id="posts",
            extra_class="hidden",
            cards=build_cards(posts[:MAX_ITEMS_PER_CATEGORY], translator)
        ))
    except Exception as e:
        print(f"[ERROR] Failed to build Posts section: {e}")
        sections_html.append(SECTION_TMPL.format(sec_id="posts", extra_class="hidden", cards=EMPTY_TMPL))

    # çµ±è¨ˆæƒ…å ±è¡¨ç¤ºï¼ˆæ”¹å–„ç‰ˆï¼‰
    final_business = len(business[:MAX_ITEMS_PER_CATEGORY])
    final_tools = len(tools[:MAX_ITEMS_PER_CATEGORY])
    final_posts = len(posts[:MAX_ITEMS_PER_CATEGORY])
    total_final = final_business + final_tools + final_posts
    total_original = len(business) + len(tools) + len(posts)

    print(f"\n{'='*60}")
    print(f"ğŸ“Š Processing Summary")
    print(f"{'='*60}")
    print(f"ğŸ“ˆ Original counts:")
    print(f"   Business: {len(business):3d} â†’ {final_business:3d} items")
    print(f"   Tools:    {len(tools):3d} â†’ {final_tools:3d} items")
    print(f"   Posts:    {len(posts):3d} â†’ {final_posts:3d} items")
    print(f"   Total:    {total_original:3d} â†’ {total_final:3d} items")
    print(f"   Filtered: {total_original - total_final:3d} items")

    # å‡¦ç†æ™‚é–“è¨ˆç®—
    end_time = time.time()
    processing_time = end_time - start_time
    print(f"\nâ±ï¸  Processing Time: {processing_time:.2f} seconds")

    if total_final > 0:
        avg_time_per_item = processing_time / total_final
        print(f"   Average per item: {avg_time_per_item:.3f} seconds")
    print(f"{'='*60}\n")
    html_out = PAGE_TMPL.format(
        updated_title=NOW.strftime("%Y-%m-%d %H:%M JST"),
        updated_full=NOW.strftime("%Y-%m-%d %H:%M JST"),
        lookback=HOURS_LOOKBACK,
        cnt_business=len(business[:MAX_ITEMS_PER_CATEGORY]),
        cnt_tools=len(tools[:MAX_ITEMS_PER_CATEGORY]),
        cnt_posts=len(posts[:MAX_ITEMS_PER_CATEGORY]),
        sections="".join(sections_html)
    )

    try:
        Path("news_detail.html").write_text(html_out, encoding="utf-8")
        print(f"[SUCCESS] Wrote news_detail.html ({len(html_out)} bytes)")
    except Exception as e:
        print(f"[ERROR] Failed to write news_detail.html: {e}")
        raise
    
    try:
        save_cache(TRANSLATION_CACHE)
        print(f"[SUCCESS] Saved {len(TRANSLATION_CACHE)} translations to cache")
    except Exception as e:
        print(f"[WARN] Failed to save cache: {e}")

if __name__ == "__main__":
    main()
