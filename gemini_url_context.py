#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gemini URL Context Client - GAç‰ˆURL contextã‚’ä½¿ã£ãŸçµ±ä¸€çš„æƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ 
"""
import os
import json
import logging
from typing import List, Optional, Dict, Any
from datetime import datetime

# Gemini API imports
try:
    from google import genai
    from google.genai.types import GenerateContentConfig, HttpOptions
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    print("âŒ google-genai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦ã§ã™: pip install -U google-genai")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GeminiURLContextClient:
    """Gemini URL Context APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–"""
        if not GENAI_AVAILABLE:
            raise ImportError("google-genai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        
        self.client = self._make_client()
        self.default_model = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
        
    def _make_client(self) -> 'genai.Client':
        """Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆï¼ˆVertex AIå¯¾å¿œï¼‰"""
        use_vertex = os.getenv("GOOGLE_GENAI_USE_VERTEXAI", "").lower() in ("1", "true", "yes")
        
        if use_vertex:
            logger.info("ğŸ”§ Vertex AIçµŒè·¯ã§åˆæœŸåŒ–ä¸­...")
            return genai.Client(http_options=HttpOptions(api_version="v1"))
        else:
            logger.info("ğŸ”§ Developer APIçµŒè·¯ã§åˆæœŸåŒ–ä¸­...")
            return genai.Client()
    
    def generate_from_urls(
        self,
        prompt: str,
        urls: List[str],
        model: Optional[str] = None,
        enable_search: bool = False,
        max_retries: int = 3
    ) -> Dict[str, Any]:
        """
        URLã‚’ç›´æ¥æŒ‡å®šã—ã¦Geminiã§å†…å®¹ã‚’è§£æ
        
        Args:
            prompt: è§£ææŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            urls: å¯¾è±¡URLãƒªã‚¹ãƒˆï¼ˆæœ€å¤§20URLï¼‰
            model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
            enable_search: Google Searchä½µç”¨
            max_retries: ãƒªãƒˆãƒ©ã‚¤å›æ•°
            
        Returns:
            è§£æçµæœè¾æ›¸ï¼ˆtext, url_context_metadata, usage_metadataå«ã‚€ï¼‰
        """
        if not urls:
            raise ValueError("URLãƒªã‚¹ãƒˆãŒç©ºã§ã™")
        
        if len(urls) > 20:
            logger.warning(f"âš ï¸ URLæ•°ãŒåˆ¶é™ã‚’è¶…é: {len(urls)} > 20")
            urls = urls[:20]
        
        model_id = model or self.default_model
        logger.info(f"ğŸ¤– Geminiè§£æé–‹å§‹: model={model_id}, urls={len(urls)}ä»¶")
        
        # ãƒ„ãƒ¼ãƒ«è¨­å®š
        tools = [{"url_context": {}}]
        
        # Google Searchæœ‰åŠ¹åŒ–
        if enable_search or os.getenv("ENABLE_GOOGLE_SEARCH", "").lower() in ("1", "true", "yes"):
            tools.append({"google_search": {}})
            logger.info("ğŸ” Google Search groundingæœ‰åŠ¹")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        content_text = f"{prompt}\\n\\nå¯¾è±¡URL:\\n" + "\\n".join(urls)
        
        # APIå®Ÿè¡Œï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        for attempt in range(max_retries + 1):
            try:
                logger.info(f"ğŸ“¡ APIå‘¼ã³å‡ºã—ä¸­... (è©¦è¡Œ {attempt + 1}/{max_retries + 1})")
                
                resp = self.client.models.generate_content(
                    model=model_id,
                    contents=content_text,
                    config=GenerateContentConfig(tools=tools),
                )
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æ
                result = self._parse_response(resp, urls)
                
                # æˆåŠŸãƒ­ã‚°
                self._log_success(result, model_id, len(urls))
                
                return result
                
            except Exception as e:
                logger.error(f"âŒ APIå‘¼ã³å‡ºã—å¤±æ•— (è©¦è¡Œ {attempt + 1}): {e}")
                
                if attempt == max_retries:
                    # æœ€çµ‚è©¦è¡Œã§ã‚‚å¤±æ•—
                    return {
                        "text": f"ã‚¨ãƒ©ãƒ¼: URLè§£æã«å¤±æ•—ã—ã¾ã—ãŸ - {str(e)}",
                        "url_context_metadata": None,
                        "usage_metadata": None,
                        "error": str(e),
                        "raw": None
                    }
                
                # ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿ
                import time
                time.sleep(2 ** attempt)
        
        return {}
    
    def _parse_response(self, resp: Any, urls: List[str]) -> Dict[str, Any]:
        """Geminiãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ"""
        
        # ãƒ†ã‚­ã‚¹ãƒˆæœ¬æ–‡
        text = getattr(resp, "text", None) or "ãƒ†ã‚­ã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"
        
        # URL context metadataï¼ˆæ ¹æ‹ æƒ…å ±ï¼‰
        url_meta = None
        if hasattr(resp, 'candidates') and resp.candidates:
            url_meta = getattr(resp.candidates[0], 'url_context_metadata', None)
        
        # ä½¿ç”¨é‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚³ã‚¹ãƒˆè¨ˆç®—ç”¨ï¼‰
        usage = getattr(resp, "usage_metadata", None)
        
        return {
            "text": text,
            "url_context_metadata": url_meta,
            "usage_metadata": usage,
            "raw": resp,
            "timestamp": datetime.now().isoformat(),
            "input_urls": urls
        }
    
    def _log_success(self, result: Dict[str, Any], model: str, url_count: int):
        """æˆåŠŸãƒ­ã‚°ã®å‡ºåŠ›"""
        usage = result.get("usage_metadata")
        
        if usage:
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆã‚³ã‚¹ãƒˆè¦‹ç©ã®æ ¹æ‹ ï¼‰
            prompt_tokens = getattr(usage, 'prompt_token_count', 0)
            completion_tokens = getattr(usage, 'candidates_token_count', 0)
            total_tokens = getattr(usage, 'total_token_count', 0)
            
            logger.info(f"âœ… è§£æå®Œäº†: model={model}")
            logger.info(f"ğŸ“Š ä½¿ç”¨é‡: prompt={prompt_tokens}, completion={completion_tokens}, total={total_tokens} tokens")
            logger.info(f"ğŸ”— å¯¾è±¡URLæ•°: {url_count}")
        else:
            logger.info(f"âœ… è§£æå®Œäº†: model={model}, URLs={url_count}")
        
        # URL context metadataã®æ¦‚è¦
        url_meta = result.get("url_context_metadata")
        if url_meta:
            logger.info(f"ğŸŒ URL context metadataå–å¾—æ¸ˆã¿")
    
    def summarize_news_articles(
        self,
        article_urls: List[str],
        focus_topics: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è¦ç´„ï¼ˆAIæ¥­ç•Œç‰¹åŒ–ï¼‰"""
        
        focus_text = ""
        if focus_topics:
            focus_text = f"\\n\\nç‰¹ã«ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«æ³¨ç›®ã—ã¦è§£æã—ã¦ãã ã•ã„:\\n- " + "\\n- ".join(focus_topics)
        
        prompt = f"""
ä»¥ä¸‹ã®AIæ¥­ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’åˆ†æã—ã€åŒ…æ‹¬çš„ãªè¦ç´„ã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚

## åˆ†æé …ç›®:
1. **ä¸»è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹**: å„è¨˜äº‹ã®æ ¸å¿ƒçš„ãªå†…å®¹
2. **æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰**: è¨€åŠã•ã‚Œã¦ã„ã‚‹æŠ€è¡“ã‚„è£½å“
3. **ä¼æ¥­å‹•å‘**: é–¢é€£ä¼æ¥­ã¨ãã®æ´»å‹•
4. **æ¥­ç•Œã¸ã®å½±éŸ¿**: AIæ¥­ç•Œå…¨ä½“ã¸ã®æ„å‘³åˆã„
5. **æ—¥æœ¬å¸‚å ´ã¸ã®é–¢é€£æ€§**: æ—¥æœ¬ã®AIæ¥­ç•Œã¸ã®å½±éŸ¿

## å‡ºåŠ›å½¢å¼:
- å„è¨˜äº‹ã”ã¨ã«è¦ç´„ï¼ˆ2-3æ–‡ï¼‰
- å…¨ä½“ã®å‚¾å‘åˆ†æ
- é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½3è¨˜äº‹ï¼‰

{focus_text}

è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã€ç™ºä¿¡å…ƒã€ç™ºè¡¨æ—¥æ™‚ã‚‚å«ã‚ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚
        """.strip()
        
        return self.generate_from_urls(
            prompt=prompt,
            urls=article_urls,
            enable_search=False  # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã¯ç›´æ¥èª­ã¿å–ã‚Š
        )
    
    def analyze_product_documentation(
        self,
        doc_urls: List[str],
        analysis_type: str = "features"
    ) -> Dict[str, Any]:
        """è£½å“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åˆ†æ"""
        
        if analysis_type == "features":
            prompt = """
ä»¥ä¸‹ã®è£½å“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚„APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã€ä¸»è¦ãªæ©Ÿèƒ½ã¨ä»•æ§˜ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

## æŠ½å‡ºé …ç›®:
1. **ä¸»è¦æ©Ÿèƒ½**: è£½å“/APIã®æ ¸å¿ƒæ©Ÿèƒ½
2. **æŠ€è¡“ä»•æ§˜**: ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã€åˆ¶é™ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
3. **æ–™é‡‘ãƒ»åˆ¶é™**: åˆ©ç”¨æ–™é‡‘ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€ä½¿ç”¨åˆ¶é™
4. **çµ±åˆæ–¹æ³•**: APIã®ä½¿ç”¨æ–¹æ³•ã€SDKã‚µãƒãƒ¼ãƒˆ
5. **ç«¶åˆå„ªä½æ€§**: ä»–ç¤¾è£½å“ã¨ã®å·®åˆ¥åŒ–è¦ç´ 

å‡ºåŠ›ã¯æ§‹é€ åŒ–ã•ã‚ŒãŸå½¢å¼ã§ã€é–‹ç™ºè€…ãŒç†è§£ã—ã‚„ã™ã„ã‚ˆã†ã«æ•´ç†ã—ã¦ãã ã•ã„ã€‚
            """.strip()
        
        elif analysis_type == "comparison":
            prompt = """
ä»¥ä¸‹ã®è¤‡æ•°ã®è£½å“/ã‚µãƒ¼ãƒ“ã‚¹ã‚’æ¯”è¼ƒåˆ†æã—ã¦ãã ã•ã„ã€‚

## æ¯”è¼ƒé …ç›®:
1. **æ©Ÿèƒ½æ¯”è¼ƒè¡¨**: ä¸»è¦æ©Ÿèƒ½ã®æœ‰ç„¡
2. **ä¾¡æ ¼æ¯”è¼ƒ**: æ–™é‡‘ä½“ç³»ã®é•ã„
3. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: é€Ÿåº¦ã€ç²¾åº¦ã€å¯ç”¨æ€§
4. **ä½¿ã„ã‚„ã™ã•**: å°å…¥é›£æ˜“åº¦ã€å­¦ç¿’ã‚³ã‚¹ãƒˆ
5. **æ¨å¥¨ç”¨é€”**: ã©ã®ã‚ˆã†ãªç”¨é€”ã«æœ€é©ã‹

çµè«–ã¨ã—ã¦ã€ç”¨é€”åˆ¥ã®æ¨å¥¨è£½å“ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚
            """.strip()
        
        else:
            prompt = f"""
ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’{analysis_type}ã®è¦³ç‚¹ã‹ã‚‰åˆ†æã—ã¦ãã ã•ã„ã€‚
æŠ€è¡“çš„ãªè©³ç´°ã¨å®Ÿç”¨çš„ãªè¦³ç‚¹ã®ä¸¡æ–¹ã‚’å«ã‚ã¦ã€æ—¥æœ¬èªã§åŒ…æ‹¬çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
            """.strip()
        
        return self.generate_from_urls(
            prompt=prompt,
            urls=doc_urls,
            enable_search=True  # æŠ€è¡“æ–‡æ›¸ã¯é–¢é€£æƒ…å ±ã‚‚æ¤œç´¢
        )
    
    def extract_research_insights(
        self,
        paper_urls: List[str],
        research_focus: Optional[str] = None
    ) -> Dict[str, Any]:
        """ç ”ç©¶è«–æ–‡ã‹ã‚‰ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆæŠ½å‡º"""
        
        focus_text = f"\\n\\nç‰¹ã«{research_focus}ã«é–¢é€£ã™ã‚‹å†…å®¹ã«æ³¨ç›®ã—ã¦ãã ã•ã„ã€‚" if research_focus else ""
        
        prompt = f"""
ä»¥ä¸‹ã®ç ”ç©¶è«–æ–‡ã‚„ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ã€é‡è¦ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

## æŠ½å‡ºé …ç›®:
1. **ç ”ç©¶ã®èƒŒæ™¯**: ãªãœã“ã®ç ”ç©¶ãŒé‡è¦ãªã®ã‹
2. **ä¸»è¦ãªç™ºè¦‹**: æ–°ã—ã„çŸ¥è¦‹ã‚„æŠ€è¡“çš„ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼
3. **æ‰‹æ³•ãƒ»ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: ä½¿ç”¨ã•ã‚ŒãŸæŠ€è¡“ã‚„æ–¹æ³•è«–
4. **å®Ÿé¨“çµæœ**: å®šé‡çš„ãªæˆæœã‚„æ€§èƒ½æ”¹å–„
5. **å®Ÿç”¨åŒ–ã®å¯èƒ½æ€§**: ç”£æ¥­å¿œç”¨ã®æ½œåœ¨æ€§
6. **ä»Šå¾Œã®ç ”ç©¶æ–¹å‘**: æ®‹ã•ã‚ŒãŸèª²é¡Œã¨ç™ºå±•æ€§

{focus_text}

å­¦è¡“çš„å†…å®¹ã‚’å®Ÿå‹™è€…ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã€AIæ¥­ç•Œã¸ã®å½±éŸ¿ã‚’è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚
        """.strip()
        
        return self.generate_from_urls(
            prompt=prompt,
            urls=paper_urls,
            enable_search=True
        )

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_client = None

def get_client() -> GeminiURLContextClient:
    """ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå–å¾—"""
    global _client
    if _client is None:
        _client = GeminiURLContextClient()
    return _client

# ä¾¿åˆ©é–¢æ•°ç¾¤
def summarize_urls(prompt: str, urls: List[str], **kwargs) -> Dict[str, Any]:
    """URLã‚’ç›´æ¥è¦ç´„"""
    return get_client().generate_from_urls(prompt, urls, **kwargs)

def analyze_news_batch(article_urls: List[str], focus_topics: List[str] = None) -> Dict[str, Any]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒãƒƒãƒè§£æ"""
    return get_client().summarize_news_articles(article_urls, focus_topics)

def compare_products(product_urls: List[str]) -> Dict[str, Any]:
    """è£½å“æ¯”è¼ƒåˆ†æ"""
    return get_client().analyze_product_documentation(product_urls, "comparison")

def extract_research(paper_urls: List[str], focus: str = None) -> Dict[str, Any]:
    """ç ”ç©¶è«–æ–‡ã‚¤ãƒ³ã‚µã‚¤ãƒˆæŠ½å‡º"""
    return get_client().extract_research_insights(paper_urls, focus)

if __name__ == "__main__":
    print("ğŸ§ª Gemini URL Context ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
    
    # ãƒ†ã‚¹ãƒˆç”¨URL
    test_urls = [
        "https://ai.google.dev/gemini-api/docs/url-context",
        "https://developers.googleblog.com/en/gemini-api-url-context-ga/"
    ]
    
    try:
        client = GeminiURLContextClient()
        
        result = client.generate_from_urls(
            prompt="ã“ã‚Œã‚‰ã®ãƒšãƒ¼ã‚¸ã®è¦ç‚¹ã‚’3ã¤ã®ãƒã‚¤ãƒ³ãƒˆã§æ—¥æœ¬èªè¦ç´„ã—ã¦ãã ã•ã„ã€‚",
            urls=test_urls,
            enable_search=False
        )
        
        print("âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
        print(f"ğŸ“ è¦ç´„: {result['text'][:200]}...")
        print(f"ğŸ“Š ä½¿ç”¨é‡: {result['usage_metadata']}")
        print(f"ğŸ”— URL metadata: {'å–å¾—æ¸ˆã¿' if result['url_context_metadata'] else 'ãªã—'}")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()