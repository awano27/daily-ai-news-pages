# -*- coding: utf-8 -*-
"""
Daily AI News - static site generator (JST)
- Summaries are translated to Japanese (no API key) using deep-translator.
- Primary engine: GoogleTranslator (unofficial). Fallback: MyMemory (ja-JP).
- Caches translations to _cache/translations.json to avoid repeated calls.
- Reads RSS list from feeds.yml with categories: Business, Tools, Posts.
- Injects X posts from a CSV file into the 'Posts' category.

Env (optional):
  HOURS_LOOKBACK=24        # Fetch window in hours
  MAX_ITEMS_PER_CATEGORY=8 # Max cards per tab
  TRANSLATE_TO_JA=1        # 1=enable JA summaries, 0=disable
  TRANSLATE_ENGINE=google  # google|mymemory
  X_POSTS_CSV=_sources/x_favorites.csv # Path to X posts CSV
  TZ=Asia/Tokyo            # for timestamps
"""
import os, re, sys, json, time, html, csv, io
from datetime import datetime, timezone, timedelta
from pathlib import Path
from urllib.parse import urlparse
from urllib.request import urlopen

import yaml
import feedparser
import requests
import random
import time
from urllib.parse import urljoin

# URL ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from url_filter import filter_403_urls, is_403_url
    print("âœ… URL ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½: æœ‰åŠ¹")
except ImportError:
    print("âš ï¸ URL ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½: ç„¡åŠ¹")
    def filter_403_urls(items):
        return items
    def is_403_url(url):
        return False

# ---------- config ----------
HOURS_LOOKBACK = int(os.getenv("HOURS_LOOKBACK", "24"))
MAX_ITEMS_PER_CATEGORY = int(os.getenv("MAX_ITEMS_PER_CATEGORY", "8"))
TRANSLATE_TO_JA = os.getenv("TRANSLATE_TO_JA", "1") == "1"
TRANSLATE_ENGINE = os.getenv("TRANSLATE_ENGINE", "google").lower()
# Google Sheets CSV URL for live X posts data
GOOGLE_SHEETS_URL = "https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0"
X_POSTS_CSV = os.getenv("X_POSTS_CSV", GOOGLE_SHEETS_URL)

JST = timezone(timedelta(hours=9))
NOW = datetime.now(JST)

CACHE_DIR = Path("_cache")
CACHE_DIR.mkdir(exist_ok=True)
CACHE_FILE = CACHE_DIR / "translations.json"

def load_cache():
    try:
        if CACHE_FILE.exists():
            return json.loads(CACHE_FILE.read_text(encoding="utf-8"))
    except Exception:
        pass
    return {}

def save_cache(cache):
    try:
        CACHE_DIR.mkdir(exist_ok=True)
        CACHE_FILE.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception:
        pass

def advanced_feed_fetch(url, name):
    """é«˜åº¦ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾— - Google News 403ã‚¨ãƒ©ãƒ¼å¯¾ç­–"""
    
    # è¤‡æ•°ã®User-Agentã‚’ç”¨æ„
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0'
    ]
    
    for i, user_agent in enumerate(user_agents):
        try:
            print(f"[INFO] Advanced fetch attempt {i+1}/{len(user_agents)} for {name}")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦è©³ç´°ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
            session = requests.Session()
            session.headers.update({
                'User-Agent': user_agent,
                'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            })
            
            # Google Newsã¾ãŸã¯Googleç³»ã®URLã®å ´åˆã€è¿½åŠ ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
            if 'google.com' in url or 'news.google.com' in url:
                session.headers.update({
                    'Referer': 'https://news.google.com/',
                    'Origin': 'https://news.google.com',
                    'Sec-Fetch-User': '?1'
                })
            
            # Googleç³»ã‚µãƒ¼ãƒ“ã‚¹ã®å ´åˆã€è¿½åŠ ã®é…å»¶
            if 'google.com' in url:
                delay = random.uniform(2, 5)  # 2-5ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ é…å»¶
                print(f"[INFO] Google service detected, applying {delay:.1f}s delay")
                time.sleep(delay)
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            response = session.get(url, timeout=30, allow_redirects=True)
            
            if response.status_code == 200:
                print(f"[SUCCESS] {name} fetched successfully with User-Agent {i+1}")
                # feedparserã«æ¸¡ã™ãŸã‚ã«BytesIOã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                import io
                feed_data = io.BytesIO(response.content)
                d = feedparser.parse(feed_data)
                return d
            elif response.status_code == 403:
                print(f"[WARN] 403 Forbidden with User-Agent {i+1} for {name}")
                continue
            else:
                print(f"[WARN] HTTP {response.status_code} with User-Agent {i+1} for {name}")
                continue
                
        except Exception as e:
            print(f"[WARN] Exception with User-Agent {i+1} for {name}: {e}")
            continue
    
    print(f"[ERROR] All advanced fetch attempts failed for {name}")
    
    # æœ€å¾Œã®æ‰‹æ®µ: Gemini Web Fetcherã‚’ä½¿ç”¨
    try:
        from gemini_web_fetcher import GeminiWebFetcher
        fetcher = GeminiWebFetcher()
        if fetcher.analyzer.enabled:
            print(f"[INFO] Trying Gemini Web Fetcher for {name}...")
            news_items = fetcher.fetch_from_problematic_source(url, name)
            if news_items:
                # feedparserãƒ©ã‚¤ã‚¯ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                fake_feed = type('FakeFeed', (), {})()
                fake_feed.entries = []
                
                for item in news_items:
                    entry = type('FakeEntry', (), {})()
                    entry.title = item.get('title', '')
                    entry.summary = item.get('summary', '')
                    entry.link = item.get('url', '#')
                    entry.published_parsed = item.get('_dt', datetime.now()).timetuple()
                    fake_feed.entries.append(entry)
                
                print(f"[SUCCESS] Gemini fetched {len(news_items)} items for {name}")
                return fake_feed
    except ImportError:
        print(f"[WARN] Gemini Web Fetcher not available")
    except Exception as e:
        print(f"[WARN] Gemini Web Fetcher failed: {e}")
    
    return None

# ---------- translation ----------
def looks_japanese(s: str) -> bool:
    if not s:
        return False
    # Hiragana, Katakana, CJK
    return re.search(r"[\u3040-\u30ff\u3400-\u9fff]", s) is not None

class JaTranslator:
    def __init__(self, engine="google"):
        self.engine = engine
        self._gt = None
        self._mm = None
        self.warned = False

    def _google(self, text: str) -> str:
        if self._gt is None:
            from deep_translator import GoogleTranslator
            self._gt = GoogleTranslator(source="auto", target="ja")
        return self._gt.translate(text)

    def _mymemory(self, text: str) -> str:
        # deep-translator >=1.11.0 expects region code 'ja-JP' for MyMemory
        if self._mm is None:
            from deep_translator import MyMemoryTranslator
            self._mm = MyMemoryTranslator(source="en-GB", target="ja-JP")
        return self._mm.translate(text)

    def translate(self, text: str) -> str:
        if not text or looks_japanese(text):
            return text
        try:
            if self.engine == "google":
                try:
                    return self._google(text)
                except Exception:
                    return self._mymemory(text)
            elif self.engine == "mymemory":
                try:
                    return self._mymemory(text)
                except Exception:
                    return self._google(text)
            else:
                # unknown -> google then mymemory
                try:
                    return self._google(text)
                except Exception:
                    return self._mymemory(text)
        except Exception as e:
            if not self.warned:
                print(f"[WARN] translation disabled due to error: {e.__class__.__name__}: {e}")
                self.warned = True
            return text

# ---------- X (Twitter) post injection ----------
def _read_csv_bytes(path_or_url: str) -> bytes:
    if re.match(r'^https?://', path_or_url, re.I):
        with urlopen(path_or_url) as r:
            return r.read()
    with open(path_or_url, 'rb') as f:
        return f.read()

def _extract_x_data_from_csv(raw: bytes) -> list[dict]:
    # æ–‡å­—ã‚³ãƒ¼ãƒ‰è‡ªå‹•åˆ¤å®š(UTF-8 BOMå„ªå…ˆâ†’UTF-8â†’CP932)
    for enc in ('utf-8-sig','utf-8','cp932'):
        try:
            txt = raw.decode(enc)
            break
        except Exception:
            continue
    else:
        txt = raw.decode('utf-8', errors='ignore')

    # CSVå½¢å¼: "æ—¥ä»˜", "@ãƒ¦ãƒ¼ã‚¶ãƒ¼", "ãƒ†ã‚­ã‚¹ãƒˆ", "ç”»åƒURL", "ãƒ„ã‚¤ãƒ¼ãƒˆURL"
    data = []
    try:
        rdr = csv.reader(io.StringIO(txt))
        row_count = 0
        for row in rdr:
            row_count += 1
            if row_count == 1:  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                continue
                
            if len(row) >= 3:  # æœ€ä½3åˆ—ã‚ã‚Œã°å‡¦ç†
                date_str = row[0].strip('"').strip() if len(row) > 0 else ""
                username = row[1].strip('"').strip() if len(row) > 1 else ""
                text = row[2].strip('"').strip() if len(row) > 2 else ""
                tweet_url = row[4].strip('"').strip() if len(row) > 4 else ""
                
                # URLãŒãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼URLã‚’ç”Ÿæˆ
                if not tweet_url and username:
                    username_clean = username.replace('@', '').replace('"', '')
                    tweet_url = f"https://x.com/{username_clean}/status/example"
                elif not tweet_url:
                    tweet_url = "https://x.com/unknown/status/example"
                
                # æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Œã°å‡¦ç†ï¼ˆæ¡ä»¶ã‚’å¤§å¹…ã«ç·©å’Œï¼‰
                if text and len(text.strip()) > 5:  # 5æ–‡å­—ä»¥ä¸Šã‚ã‚Œã°å‡¦ç†
                    # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆè¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œï¼‰
                    dt = None
                    # è¤‡æ•°ã®æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è©¦ã™
                    date_formats = [
                        "%B %d, %Y at %I:%M%p",  # "August 10, 2025 at 02:41AM"
                        "%B %d, %Y",             # "August 13, 2025"
                        "%Y-%m-%d %H:%M:%S",     # "2025-08-18 14:30:00"
                        "%Y-%m-%d",              # "2025-08-18"
                        "%Yå¹´%mæœˆ%dæ—¥",           # "2025å¹´8æœˆ18æ—¥"
                        "%m/%d/%Y",              # "8/18/2025"
                    ]
                    for fmt in date_formats:
                        try:
                            dt = datetime.strptime(date_str, fmt)
                            dt = dt.replace(tzinfo=JST)  # JSTã¨ã—ã¦æ‰±ã†
                            break
                        except:
                            continue
                    
                    # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨ï¼ˆæŠ•ç¨¿ã‚’è¡¨ç¤ºã•ã›ã‚‹ãŸã‚ï¼‰
                    if dt is None:
                        print(f"[WARN] Date parse failed for '{date_str}', using current time")
                        dt = datetime.now(JST)
                    
                    # å¸¸ã«æŠ•ç¨¿ã‚’è¿½åŠ 
                    data.append({
                        'url': tweet_url,
                        'username': username,
                        'text': text,
                        'datetime': dt
                    })
    except Exception as e:
        print(f"[WARN] CSV parsing error: {e}")
        pass

    # å¤ã„å½¢å¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆURLæŠ½å‡ºã®ã¿ï¼‰
    if not data:
        urls = re.findall(r'https?://(?:x|twitter)\.com/[^\s,"]+', txt)
        for url in urls:
            data.append({
                'url': url,
                'username': '',
                'text': '',
                'datetime': NOW
            })
    
    return data

def _extract_x_urls_from_csv(raw: bytes) -> list[str]:
    # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
    data = _extract_x_data_from_csv(raw)
    
    # æ­£è¦åŒ– & é‡è¤‡é™¤å»ï¼ˆé †åºç¶­æŒï¼‰
    seen, out = set(), []
    for item in data:
        u = item['url'].replace('x.com/','twitter.com/')  # æ­£è¦åŒ–
        if u not in seen:
            seen.add(u); out.append(u)
    return out

def _author_from_url(u: str) -> str:
    try:
        p = urlparse(u)
        parts = p.path.strip('/').split('/')
        return '@'+parts[0] if parts and parts[0] else 'X'
    except Exception:
        return 'X'

def gather_x_posts(csv_path: str) -> list[dict]:
    # Check if it's a URL or local file
    is_url = csv_path.startswith('http')
    
    if not is_url and not Path(csv_path).exists():
        print(f"[INFO] X posts CSV not found at: {csv_path}")
        return []
    
    if is_url:
        print(f"[INFO] Loading X posts from Google Sheets: {csv_path}")
    else:
        print(f"[INFO] Loading X posts from local file: {csv_path}")
    items = []
    try:
        raw = _read_csv_bytes(csv_path)
        x_data = _extract_x_data_from_csv(raw)
        print(f"[INFO] Extracted {len(x_data)} X posts from CSV.")
        
        for data in x_data:
            url = data['url']
            username = data['username'] or _author_from_url(url)
            post_date = data['datetime']
            # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚‚ä¿æŒï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã¯åˆ¥ã«ï¼‰
            full_text = data['text']
            text_preview = data['text'][:150] + '...' if len(data['text']) > 150 else data['text']
            
            # XæŠ•ç¨¿ã¯æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç„¡åŠ¹åŒ–ï¼ˆã™ã¹ã¦ã®æŠ•ç¨¿ã‚’å«ã‚ã‚‹ï¼‰
            # if (NOW - post_date) <= timedelta(days=7):  # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç„¡åŠ¹åŒ–
            if True:  # ã™ã¹ã¦ã®æŠ•ç¨¿ã‚’å‡¦ç†
                items.append({
                    "title": f"Xãƒã‚¹ãƒˆ {username}",
                    "link": url,
                    "_summary": text_preview or "XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿",
                    "_full_text": full_text,  # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
                    "_source": "X / SNS", 
                    "_dt": post_date,  # å®Ÿéš›ã®æŠ•ç¨¿æ—¥æ™‚ã‚’ä½¿ç”¨
                })
        
        print(f"[INFO] Created {len(items)} X post items (time filter disabled).")
    except Exception as e:
        print(f"[WARN] Failed to process X posts CSV: {e}")
    return items

# ---------- HTML template ----------
PAGE_TMPL = """<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Daily AI News â€” {updated_title}</title>
  <link rel="stylesheet" href="style.css"/>
</head>
<body>
  <header class="site-header">
    <div class="brand">ğŸ“° Daily AI News</div>
    <nav class="nav-links">
      <a href="ai_news_dashboard.html" class="nav-link">ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</a>
    </nav>
    <div class="updated">æœ€çµ‚æ›´æ–°ï¼š{updated_full}</div>
  </header>

  <main class="container">
    <h1 class="page-title">ä»Šæ—¥ã®æœ€æ–°AIæƒ…å ±</h1>
    <p class="lead">
        ä¸–ç•Œã®AIæ¥­ç•Œã®æœ€æ–°å‹•å‘ã‚’24æ™‚é–“365æ—¥ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã€‚OpenAIã€Googleã€Metaã€Anthropicãªã©ä¸»è¦ä¼æ¥­ã®å…¬å¼ç™ºè¡¨ã‹ã‚‰ã€
        arXivè«–æ–‡ã€é–‹ç™ºè€…ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æŠ€è¡“è¨è«–ã¾ã§å¹…åºƒãåé›†ã€‚ãƒ“ã‚¸ãƒã‚¹ï¼ˆè³‡é‡‘èª¿é”ãƒ»M&Aãƒ»æˆ¦ç•¥ææºï¼‰ã€
        ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ï¼ˆæ–°ãƒ¢ãƒ‡ãƒ«ãƒ»APIãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰ã€ç ”ç©¶ï¼ˆè«–æ–‡ãƒ»ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ï¼‰ã®3ã‚«ãƒ†ã‚´ãƒªã§æ•´ç†ã—ã€
        é‡è¦åº¦é †ã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‚å„è¨˜äº‹ã®è¦ç´„ã¯æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã€åŸæ–‡ãƒªãƒ³ã‚¯ã§è©³ç´°ç¢ºèªå¯èƒ½ã€‚
        ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ç ”ç©¶è€…ã€æŠ•è³‡å®¶ã€çµŒå–¶è€…ãªã©ã€AIæ¥­ç•Œã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«å‘ã‘ã®åŒ…æ‹¬çš„æƒ…å ±æºã¨ã—ã¦ã€
        ç›´è¿‘{lookback}æ™‚é–“ã®é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å³é¸é…ä¿¡ã€‚ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã¯æ¥­ç•Œå…¨ä½“åƒã®ä¿¯ç°åˆ†æã‚‚æä¾›ã€‚
    </p>

    <section class="kpi-grid">
      <div class="kpi-card">
        <div class="kpi-value">{cnt_business}ä»¶</div>
        <div class="kpi-label">ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹</div>
        <div class="kpi-note">é‡è¦åº¦é«˜ã‚</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{cnt_tools}ä»¶</div>
        <div class="kpi-label">ãƒ„ãƒ¼ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹</div>
        <div class="kpi-note">é–‹ç™ºè€…å‘ã‘</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{cnt_posts}ä»¶</div>
        <div class="kpi-label">SNS/è«–æ–‡ãƒã‚¹ãƒˆ</div>
        <div class="kpi-note">æ¤œè¨¼ç³»</div>
      </div>
      <div class="kpi-card">
        <div class="kpi-value">{updated_full}</div>
        <div class="kpi-label">æœ€çµ‚æ›´æ–°</div>
        <div class="kpi-note">JST</div>
      </div>
    </section>

    <nav class="tabs" role="tablist">
      <button class="tab active" data-target="#business" aria-selected="true">ğŸ¢ ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹</button>
      <button class="tab" data-target="#tools" aria-selected="false">âš¡ ãƒ„ãƒ¼ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹</button>
      <button class="tab" data-target="#posts" aria-selected="false">ğŸ§ª SNS/è«–æ–‡ãƒã‚¹ãƒˆ</button>
    </nav>

    <!-- æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹: ã‚¿ã‚¤ãƒˆãƒ«ã‚„è¦ç´„ã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™ -->
    <div class="search-container">
      <input id="searchBox" type="text" placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è¨˜äº‹ã‚’æ¤œç´¢..." aria-label="æ¤œç´¢" />
    </div>

    {sections}
    <section class="note">
      <p>æ–¹é‡ï¼šä¸€æ¬¡æƒ…å ±ï¼ˆå…¬å¼ãƒ–ãƒ­ã‚°/ãƒ—ãƒ¬ã‚¹/è«–æ–‡ï¼‰ã‚’å„ªå…ˆã€‚ä¸€èˆ¬ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ AI ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æŠ½å‡ºã€‚è¦ç´„ã¯æ—¥æœ¬èªåŒ–ã—ã€<strong>å‡ºå…¸ãƒªãƒ³ã‚¯ã¯åŸæ–‡</strong>ã®ã¾ã¾ã€‚</p>
    </section>
  </main>

  <footer class="site-footer">
    <div>Generated by <code>build.py</code> Â· Timezone: JST</div>
    <div><a href="https://github.com/">Hosted on GitHub Pages</a></div>
  </footer>

  <script>
    const tabs = document.querySelectorAll('.tab');
    tabs.forEach(btn => btn.addEventListener('click', () => {{
      tabs.forEach(b => {{ b.classList.remove('active'); b.setAttribute('aria-selected','false'); }});
      btn.classList.add('active'); btn.setAttribute('aria-selected','true');
      document.querySelectorAll('.tab-panel').forEach(p => p.classList.add('hidden'));
      const target = document.querySelector(btn.dataset.target);
      if (target) target.classList.remove('hidden');
    }}));

    // æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã®å…¥åŠ›ã«å¿œã˜ã¦ã‚«ãƒ¼ãƒ‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
    const searchBox = document.getElementById('searchBox');
    if (searchBox) {{
      searchBox.addEventListener('input', () => {{
        const query = searchBox.value.toLowerCase();
        // ã™ã¹ã¦ã®ã‚«ãƒ¼ãƒ‰ã‚’å¯¾è±¡ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
        document.querySelectorAll('.card').forEach(card => {{
          const titleEl = card.querySelector('.card-title');
          const summaryEl = card.querySelector('.card-summary');
          const title = titleEl ? titleEl.textContent.toLowerCase() : '';
          const summary = summaryEl ? summaryEl.textContent.toLowerCase() : '';
          if (!query || title.includes(query) || summary.includes(query)) {{
            card.style.display = '';
          }} else {{
            card.style.display = 'none';
          }}
        }});
      }});
    }}
  </script>
</body>
</html>
"""

SECTION_TMPL = """
<section id="{sec_id}" class="tab-panel {extra_class}">
{cards}
</section>
"""

CARD_TMPL = """
<article class="card">
  <div class="card-header">
    <a class="card-title" href="{link}" target="_blank" rel="noopener">{title}</a>
  </div>
  <div class="card-body">
    <p class="card-summary">{summary}</p>
    <div class="chips">
      <span class="chip">{source_name}</span>
      <span class="chip ghost">{summary_lang}</span>
      <span class="chip ghost">{ago}</span>
    </div>
  </div>
  <div class="card-footer">
    å‡ºå…¸: <a href="{link}" target="_blank" rel="noopener">{link}</a>
  </div>
</article>
"""

EMPTY_TMPL = '<div class="empty">æ–°ç€ãªã—ï¼ˆæœŸé–“ã‚’åºƒã’ã‚‹ã‹ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼‰</div>'

def ago_str(dt: datetime) -> str:
    delta = NOW - dt
    secs = int(delta.total_seconds())
    if secs < 60: return f"{secs}ç§’å‰"
    mins = secs // 60
    if mins < 60: return f"{mins}åˆ†å‰"
    hrs = mins // 60
    if hrs < 24: return f"{hrs}æ™‚é–“å‰"
    days = hrs // 24
    return f"{days}æ—¥å‰"

def clean_html(s: str) -> str:
    if not s: return ""
    # strip tags very lightly
    s = re.sub(r"<.*?>", "", s)
    s = s.replace("&nbsp;", " ").strip()
    return html.escape(s, quote=False)

def pick_summary(entry) -> str:
    for key in ("summary", "subtitle", "description"):
        if key in entry and entry[key]:
            return clean_html(entry[key])
    return clean_html(entry.get("title", ""))

def parse_feeds():
    raw = yaml.safe_load(Path("feeds.yml").read_text(encoding="utf-8"))
    return raw or {}

def get_category(conf, category_name):
    """Case-insensitive category lookup"""
    # Try exact match first
    if category_name in conf:
        return conf[category_name]
    # Try case-insensitive match
    for key, value in conf.items():
        if key.lower() == category_name.lower():
            return value
    return []

def categorize_business_news(item, feeds_info):
    """ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
    business_category = feeds_info.get('business_category', 'general')
    title = item.get('title', '').lower()
    summary = item.get('_summary', '').lower()
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
    if business_category == 'strategy':
        return 'strategy'  # æˆ¦ç•¥ãƒ»çµŒå–¶
    elif business_category == 'investment':
        return 'investment'  # æŠ•è³‡ãƒ»M&A
    elif business_category == 'japan_business':
        return 'japan_business'  # æ—¥æœ¬ä¼æ¥­
    elif business_category == 'governance':
        return 'governance'  # è¦åˆ¶ãƒ»ã‚¬ãƒãƒŠãƒ³ã‚¹
    else:
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è‡ªå‹•åˆ†é¡
        investment_keywords = ['funding', 'investment', 'ipo', 'venture', 'capital', 'm&a', 'acquisition', 'æŠ•è³‡', 'è³‡é‡‘èª¿é”', 'IPO']
        strategy_keywords = ['strategy', 'executive', 'ceo', 'leadership', 'transformation', 'æˆ¦ç•¥', 'çµŒå–¶', 'CEO']
        governance_keywords = ['regulation', 'policy', 'compliance', 'ethics', 'governance', 'è¦åˆ¶', 'æ”¿ç­–', 'ã‚¬ãƒãƒŠãƒ³ã‚¹']
        
        for keyword in investment_keywords:
            if keyword in title or keyword in summary:
                return 'investment'
        for keyword in strategy_keywords:
            if keyword in title or keyword in summary:
                return 'strategy'
        for keyword in governance_keywords:
            if keyword in title or keyword in summary:
                return 'governance'
                
        return 'general'

def within_window(published_parsed):
    if not published_parsed: 
        return True, NOW  # keep if unknown, use current time
    dt = datetime.fromtimestamp(time.mktime(published_parsed), tz=timezone.utc).astimezone(JST)
    return (NOW - dt) <= timedelta(hours=HOURS_LOOKBACK), dt

def is_ai_relevant(title: str, summary: str) -> bool:
    """
    AIã«é–¢é€£æ€§ã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    ã‚ˆã‚Šå³æ ¼ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§è³ªã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’é¸åˆ¥
    """
    content = f"{title} {summary}".lower()
    
    # é«˜é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã‚‰ãŒã‚ã‚Œã°å¿…ãšå«ã‚ã‚‹ï¼‰
    high_relevance = [
        'artificial intelligence', 'machine learning', 'deep learning', 'neural network',
        'gpt', 'llm', 'large language model', 'transformer', 'bert', 'claude',
        'chatgpt', 'gemini', 'copilot', 'anthropic', 'openai', 'deepmind',
        'computer vision', 'natural language processing', 'nlp', 'reinforcement learning',
        'generative ai', 'ai model', 'ai research', 'ai breakthrough',
        'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ',
                'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ',
        'ï¼¡ï¼©', 'AI', 'ML', 'DL', 'ç”ŸæˆAI', 'ã‚¸ã‚§ãƒãƒ¬ãƒ¼ãƒ†ã‚£ãƒ–AI',
        'ãƒãƒ£ãƒƒãƒˆGPT', 'ChatGPT', 'GPT', 'LLM', 'å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«',
        'Claude', 'Gemini', 'Copilot', 'Bard',
        'è‡ªç„¶è¨€èªå‡¦ç†', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³', 'ç”»åƒèªè­˜', 'éŸ³å£°èªè­˜',
        'ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹', 'è‡ªå‹•é‹è»¢', 'äºˆæ¸¬åˆ†æ', 'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹',
        'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ', 'æœ€é©åŒ–', 'ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³',
        'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—', 'è³‡é‡‘èª¿é”', 'æŠ•è³‡', 'ãƒ•ã‚¡ãƒ³ãƒ‰', 'IPO', 'M&A',
        'ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯', 'ãƒˆãƒ¨ã‚¿', 'NTT', 'ã‚½ãƒ‹ãƒ¼', 'æ—¥ç«‹', 'å¯Œå£«é€š', 'NEC',
        'ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯', 'æ¥½å¤©', 'ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ', 'ãƒ¡ãƒ«ã‚«ãƒª', 'LINE',
    ]
    
    # ä¸­é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ã‚ã‚Œã°å«ã‚ã‚‹ï¼‰
    medium_relevance = [
        'algorithm', 'automation', 'robot', 'autonomous', 'prediction',
        'data science', 'analytics', 'intelligent', 'smart system',
        'cognitive', 'inference', 'classification', 'recognition',
        'generation', 'synthesis', 'optimization', 'recommendation',
        'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ', 'è‡ªå‹•åŒ–', 'ãƒ­ãƒœãƒƒãƒˆ', 'è‡ªå¾‹', 'äºˆæ¸¬',
        'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹', 'åˆ†æ', 'ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ', 'ã‚¹ãƒãƒ¼ãƒˆ',
        'èªè­˜', 'ç”Ÿæˆ', 'æœ€é©åŒ–', 'ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰'
    ]
    
    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã‚‰ãŒã‚ã‚Œã°é™¤å¤–ï¼‰
    exclude_keywords = [
        'cryptocurrency', 'crypto', 'blockchain', 'bitcoin', 'nft',
        'gaming', 'game', 'sports', 'entertainment', 'music', 'movie',
        'politics', 'political', 'election', 'government policy',
        'weather', 'climate change', 'environmental',
        'æš—å·é€šè²¨', 'ã‚²ãƒ¼ãƒ ', 'ã‚¹ãƒãƒ¼ãƒ„', 'å¨¯æ¥½', 'éŸ³æ¥½', 'æ˜ ç”»',
                'æš—å·é€šè²¨', 'ã‚²ãƒ¼ãƒ ', 'ã‚¹ãƒãƒ¼ãƒ„', 'å¨¯æ¥½', 'éŸ³æ¥½', 'æ˜ ç”»',
        'æ”¿æ²»', 'é¸æŒ™', 'å¤©æ°—', 'æ°—å€™å¤‰å‹•', 'ç’°å¢ƒ',
        'ã‚¢ãƒ‹ãƒ¡', 'ãƒãƒ³ã‚¬', 'èŠ¸èƒ½', 'ã‚¿ãƒ¬ãƒ³ãƒˆ', 'ã‚¢ã‚¤ãƒ‰ãƒ«',
        'æ‹æ„›', 'çµå©š', 'ã‚°ãƒ«ãƒ¡', 'æ–™ç†', 'æ—…è¡Œ', 'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³'
    ]
    
    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    for keyword in exclude_keywords:
        if keyword in content:
            return False
    
    # é«˜é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    high_score = sum(1 for keyword in high_relevance if keyword in content)
    if high_score >= 1:
        return True
    
    # ä¸­é–¢é€£åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆ2ã¤ä»¥ä¸Šã§æ¡ç”¨ï¼‰
    medium_score = sum(1 for keyword in medium_relevance if keyword in content)
    if medium_score >= 2:
        return True
    
    return False


def calculate_importance_score(item):
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    å¤§ããªãƒ‹ãƒ¥ãƒ¼ã‚¹ã»ã©é«˜ã„ã‚¹ã‚³ã‚¢ã‚’è¿”ã™
    """
    title = item.get("title", "").lower()
    summary = item.get("_summary", "").lower()
    source = item.get("_source", "").lower()
    content = f"{title} {summary}"
    
    score = 0
    
    # 1. ä¼æ¥­ãƒ»çµ„ç¹”ã®é‡è¦åº¦ï¼ˆå¤§æ‰‹ä¼æ¥­ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
    major_companies = {
        'openai': 100, 'anthropic': 100, 'google': 90, 'microsoft': 90,
        'meta': 85, 'nvidia': 85, 'apple': 80, 'amazon': 80,
        'tesla': 75, 'deepmind': 95, 'cohere': 70, 'hugging face': 70,
        'mistral': 65, 'stability ai': 65, 'midjourney': 60
    }
    
    for company, points in major_companies.items():
        if company in content:
            score += points
            break  # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã¿é©ç”¨
    
    # 2. é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç”»æœŸçš„ãªç™ºè¡¨ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
    high_impact_keywords = {
        'breakthrough': 80, 'launch': 70, 'release': 65, 'announce': 60,
        'unveil': 75, 'introduce': 60, 'partnership': 55, 'acquisition': 85,
        'funding': 70, 'investment': 65, 'ipo': 90, 'valuation': 60,
        'gpt-5': 100, 'gpt-4': 80, 'claude': 70, 'gemini': 70,
        'billion': 75, 'million': 50, 'record': 65, 'first': 60
    }
    
    for keyword, points in high_impact_keywords.items():
        if keyword in content:
            score += points * 0.5  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚0.5å€
    
    # 3. ã‚½ãƒ¼ã‚¹ã®ä¿¡é ¼æ€§ãƒ»å½±éŸ¿åŠ›
    source_credibility = {
        'techcrunch': 80, 'bloomberg': 90, 'reuters': 85, 'wsj': 85,
        'financial times': 80, 'the verge': 70, 'wired': 70,
        'mit technology review': 85, 'nature': 95, 'science': 95,
        'anthropic': 90, 'openai': 90, 'google': 85, 'meta': 80
    }
    
    for src, points in source_credibility.items():
        if src in source:
            score += points * 0.3  # ã‚½ãƒ¼ã‚¹ä¿¡é ¼æ€§ã¯30%ã®é‡ã¿
            break
    
    # 4. æŠ€è¡“çš„é‡è¦åº¦
    tech_importance = {
        'artificial general intelligence': 100, 'agi': 100,
        'multimodal': 70, 'reasoning': 60, 'safety': 65,
        'alignment': 70, 'robotics': 60, 'autonomous': 55,
        'quantum': 70, 'neural network': 50, 'transformer': 60
    }
    
    for tech, points in tech_importance.items():
        if tech in content:
            score += points * 0.4
    
    # 5. æ–°é®®åº¦ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ãƒœãƒ¼ãƒŠã‚¹ï¼‰
    dt = item.get("_dt")
    if dt:
        hours_old = (NOW - dt).total_seconds() / 3600
        if hours_old < 6:  # 6æ™‚é–“ä»¥å†…
            score += 30
        elif hours_old < 12:  # 12æ™‚é–“ä»¥å†…
            score += 20
        elif hours_old < 24:  # 24æ™‚é–“ä»¥å†…
            score += 10
    
    # 6. ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã•ï¼ˆè©³ç´°ãªã‚¿ã‚¤ãƒˆãƒ«ã»ã©ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¾¡å€¤é«˜ã„ï¼‰
    title_length = len(item.get("title", ""))
    if title_length > 80:
        score += 15
    elif title_length > 50:
        score += 10
    
    return max(score, 0)  # è² ã®ã‚¹ã‚³ã‚¢ã¯0ã«

def calculate_sns_importance_score(item):
    """
    SNSãƒã‚¹ãƒˆã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ8/14ä»¥é™ã®æ–°ã—ã„æƒ…å ±ç”¨ï¼‰
    ä¼æ¥­ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã€ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã€å†…å®¹ã®é‡è¦åº¦ã§åˆ¤å®š
    """
    title = item.get("title", "").lower()
    summary = item.get("_summary", "").lower()
    username = ""
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŠ½å‡º
    if "xãƒã‚¹ãƒˆ" in title:
        username = title.replace("xãƒã‚¹ãƒˆ", "").strip().lower()
    
    content = f"{title} {summary}"
    score = 0
    
    # 1. ä¼æ¥­ãƒ»çµ„ç¹”ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®é‡è¦åº¦ï¼ˆå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
    enterprise_accounts = {
        '@openai': 100, '@anthropic': 100, '@google': 90, '@microsoft': 90,
        '@meta': 85, '@nvidia': 85, '@apple': 80, '@amazon': 80,
        '@deepmind': 95, '@huggingface': 80, '@langchainai': 75,
        '@cohereai': 70, '@stabilityai': 70, '@midjourney': 65,
        # æ—¥æœ¬ä¼æ¥­ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ  
        '@softbank': 80, '@toyota': 75, '@nttcom': 70, '@sony': 70,
        '@hitachi_ltd': 65, '@fujitsu_global': 65, '@nec_corp': 65,
        '@rakuten': 60, '@recruit_jp': 55, '@mercari_jp': 50,
        # AIç ”ç©¶è€…ãƒ»ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼
        '@ylecun': 90, '@karpathy': 90, '@jeffdean': 85, '@goodfellow_ian': 85,
        '@elonmusk': 75, '@satyanadella': 80, '@sundarpichai': 80,
        '@sama': 95, '@darioacemoglu': 80, '@fchollet': 85,
        '@hardmaru': 75, '@adcock_brett': 70, '@minimaxir': 65,
        # æ—¥æœ¬ã®AIç ”ç©¶è€…ãƒ»ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼
        '@karaage0703': 70, '@shi3z': 65, '@yukihiko_n': 60,
        '@npaka': 65, '@ohtaman': 60, '@toukubo': 55,
        # ãã®ä»–ã®è‘—åäºº
        '@windsurf': 60, '@oikon48': 55, '@godofprompt': 50,
        '@newsfromgoogle': 70, '@suh_sunaneko': 50, '@pop_ikeda': 45
    }
    
    for account, points in enterprise_accounts.items():
        if account in username or account.replace('@', '') in username:
            score += points
            break  # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã¿é©ç”¨
    
    # 2. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é‡è¦åº¦ï¼ˆæŠ€è¡“çš„ãªå†…å®¹ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
    high_value_keywords = {
        'breakthrough': 50, 'release': 40, 'launch': 40, 'announce': 35,
        'gpt-5': 80, 'gpt-4': 60, 'claude': 50, 'gemini': 50,
        'research': 40, 'paper': 35, 'model': 30, 'ai': 20,
        'artificial intelligence': 40, 'machine learning': 35,
        'deep learning': 35, 'neural network': 30,
        # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        'äººå·¥çŸ¥èƒ½': 35, 'æ©Ÿæ¢°å­¦ç¿’': 30, 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°': 30,
        'ç”Ÿæˆai': 45, 'chatgpt': 40, 'å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«': 35,
        'ç ”ç©¶': 30, 'è«–æ–‡': 25, 'ãƒ¢ãƒ‡ãƒ«': 20, 'ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼': 45,
        'è³‡é‡‘èª¿é”': 40, 'æŠ•è³‡': 35, 'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—': 30
    }
    
    for keyword, points in high_value_keywords.items():
        if keyword in content:
            score += points * 0.3  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚0.3å€
    
    # 3. ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™
    engagement_indicators = {
        'thread': 15, 'important': 20, 'must read': 25, 'breaking': 30,
        'update': 10, 'new': 15, 'latest': 10, 'just': 10,
        'é‡è¦': 20, 'å¿…è¦‹': 25, 'æœ€æ–°': 10, 'é€Ÿå ±': 30, 'æ›´æ–°': 10,
        'è§£æ±º': 20, 'ã¤ã„ã«': 15, 'å•é¡Œ': 10
    }
    
    for indicator, points in engagement_indicators.items():
        if indicator in content:
            score += points * 0.2
    
    # 4. æŠ•ç¨¿ã®æ–°é®®åº¦ï¼ˆ8/14ä»¥é™ã®æ–°ã—ã•ã‚’é‡è¦–ï¼‰
    dt = item.get("_dt")
    if dt:
        aug14_jst = datetime(2025, 8, 14, 0, 0, 0, tzinfo=JST)
        hours_since_aug14 = (dt - aug14_jst).total_seconds() / 3600
        
        # 8/15ã®æŠ•ç¨¿ã«æœ€é«˜ãƒœãƒ¼ãƒŠã‚¹
        if hours_since_aug14 >= 24:  # 8/15ä»¥é™
            score += 30
        elif hours_since_aug14 >= 12:  # 8/14åˆå¾Œ
            score += 20
        elif hours_since_aug14 >= 0:  # 8/14æœ
            score += 10
    
    # 5. ãƒ†ã‚­ã‚¹ãƒˆé•·ãƒœãƒ¼ãƒŠã‚¹ï¼ˆè©³ç´°ãªæŠ•ç¨¿ã»ã©é«˜ä¾¡å€¤ï¼‰
    text_length = len(summary)
    if text_length > 100:
        score += 10
    elif text_length > 50:
        score += 5
    
    return max(score, 0)  # è² ã®ã‚¹ã‚³ã‚¢ã¯0ã«

def build_cards(items, translator):
    cards = []
    for it in items[:MAX_ITEMS_PER_CATEGORY]:
        title = it.get("title") or "(no title)"
        link = it.get("link") or "#"
        src  = it.get("_source") or ""
        dt   = it.get("_dt") or NOW
        raw_summary = it.get("_summary") or ""
        ja_summary = raw_summary
        did_translate = False

        if TRANSLATE_TO_JA and translator and raw_summary and not looks_japanese(raw_summary):
            # cache key: stable on link+hash(summary)
            cache_key = f"{link}::{hash(raw_summary)}"
            cached = TRANSLATION_CACHE.get(cache_key)
            if cached:
                ja_summary = cached
                did_translate = True
            else:
                try:
                    ja = translator.translate(raw_summary)
                    if ja and ja != raw_summary:
                        ja_summary = ja
                        TRANSLATION_CACHE[cache_key] = ja_summary
                        did_translate = True
                except Exception as e:
                    print(f"[WARN] Translation failed for {link[:50]}: {e}")

        cards.append(CARD_TMPL.format(
            link=html.escape(link, quote=True),
            title=html.escape(title, quote=False),
            summary=(ja_summary if did_translate else raw_summary),
            source_name=html.escape(src, quote=False),
            summary_lang=("è¦ç´„: æ—¥æœ¬èª" if did_translate else "è¦ç´„: è‹±èª"),
            ago=ago_str(dt),
        ))
    return "\n".join(cards) if cards else EMPTY_TMPL

def gather_items(feeds, category_name):
    items = []
    print(f"[INFO] Processing {len(feeds)} feeds for {category_name}")
    for f in feeds:
        url = f.get("url")
        name = f.get("name", url)
        if not url: 
            print(f"[WARN] No URL for feed: {name}")
            continue
        try:
            print(f"[INFO] Fetching: {name}")
            # User-Agentã‚’è¨­å®šã—ã¦ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦ã‚’å›é¿
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            # 403ã‚¨ãƒ©ãƒ¼å¯¾ç­–: ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
            retry_count = 0
            max_retries = 2
            d = None
            
            while retry_count <= max_retries:
                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆåˆ¶é™ä»˜ãã§ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
                    import socket
                    original_timeout = socket.getdefaulttimeout()
                    socket.setdefaulttimeout(8)  # 8ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    
                    d = feedparser.parse(url, agent=headers['User-Agent'])
                    
                    socket.setdefaulttimeout(original_timeout)
                    # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                    if hasattr(d, 'status') and d.status == 403:
                        print(f"[WARN] 403 Forbidden for {name}, trying advanced fetch...")
                        # é«˜åº¦ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å†è©¦è¡Œ
                        d = advanced_feed_fetch(url, name)
                        if d is None:
                            print(f"[ERROR] Advanced fetch also failed for {name}")
                            break
                    break
                except Exception as retry_e:
                    retry_count += 1
                    if retry_count <= max_retries:
                        print(f"[WARN] Retry {retry_count}/{max_retries} for {name}: {retry_e}")
                        # é«˜åº¦ãªå–å¾—ã‚’è©¦è¡Œ
                        if 'google.com' in url:
                            print(f"[INFO] Trying advanced fetch for Google service: {name}")
                            d = advanced_feed_fetch(url, name)
                            if d is not None:
                                break
                        import time
                        time.sleep(2)  # 2ç§’å¾…æ©Ÿ
                    else:
                        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦é«˜åº¦ãªå–å¾—ã‚’è©¦è¡Œ
                        print(f"[INFO] Final attempt with advanced fetch for {name}")
                        d = advanced_feed_fetch(url, name)
                        if d is None:
                            raise retry_e
            
            if d and d.bozo:
                print(f"[WARN] Feed parse warning for {name}: {getattr(d, 'bozo_exception', 'unknown')}")
        except Exception as e:
            print(f"[ERROR] feed parse error: {name}: {e}")
            continue
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ãŒå¤±æ•—ã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not d or not hasattr(d, 'entries'):
            print(f"[WARN] No valid feed data for {name}, skipping...")
            continue
            
        entry_count = 0
        filtered_count = 0
        for e in d.entries:
            ok, dt = within_window(e.get("published_parsed") or e.get("updated_parsed"))
            if not ok:
                continue
            
            title = e.get("title", "")
            summary = pick_summary(e)
            
            # generalãƒ•ã‚£ãƒ¼ãƒ‰ã®å ´åˆã¯AIé–¢é€£åº¦ã‚’ãƒã‚§ãƒƒã‚¯
            is_general_feed = f.get("general", False)
            if is_general_feed:
                if not is_ai_relevant(title, summary):
                    filtered_count += 1
                    continue
            
            # 403ã‚¨ãƒ©ãƒ¼URLã‚’ãƒã‚§ãƒƒã‚¯
            link_url = e.get("link", "")
            if is_403_url(link_url):
                print(f"ğŸš« 403 URLé™¤å¤–: {title[:50]}...")
                continue
                
            entry_count += 1
            items.append({
                "title": title,
                "link": link_url,
                "_summary": summary,
                "_source": name,
                "_dt": dt,
            })
        if entry_count > 0:
            if filtered_count > 0:
                print(f"[INFO] Found {entry_count} recent items from {name} (filtered out {filtered_count} non-AI items)")
            else:
                print(f"[INFO] Found {entry_count} recent items from {name}")
    # ã‚¹ãƒãƒ¼ãƒˆã‚½ãƒ¼ãƒˆ: é‡è¦åº¦ã¨æ™‚åˆ»ã‚’çµ„ã¿åˆã‚ã›ã¦ä¸¦ã³æ›¿ãˆ
    if category_name == "Business":
        # ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯é‡è¦åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        items.sort(key=lambda x: (calculate_importance_score(x), x["_dt"]), reverse=True)
        print(f"[INFO] {category_name}: Sorted by importance score")
    elif category_name == "Posts":
        # SNS/è«–æ–‡ãƒã‚¹ãƒˆã¯é‡è¦åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        items.sort(key=lambda x: (calculate_sns_importance_score(x), x["_dt"]), reverse=True)
        print(f"[INFO] {category_name}: Sorted by SNS importance score")
    else:
        # ãƒ„ãƒ¼ãƒ«ã‚«ãƒ†ã‚´ãƒªã¯æ™‚åˆ»é †
        items.sort(key=lambda x: x["_dt"], reverse=True)
    
    # æœ€çµ‚ãƒã‚§ãƒƒã‚¯: 403 URLãŒãªã„ã“ã¨ã‚’ç¢ºèª
    items_before_filter = len(items)
    items = filter_403_urls(items)
    items_after_filter = len(items)
    
    if items_before_filter != items_after_filter:
        print(f"âœ… {category_name}: æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§{items_before_filter - items_after_filter}ä»¶ã®403 URLã‚’é™¤å¤–")
    
    print(f"[INFO] {category_name}: Total {len(items)} items found")
    return items

def main():
    print(f"[INFO] Starting build at {NOW.strftime('%Y-%m-%d %H:%M JST')}")
    print(f"[INFO] HOURS_LOOKBACK={HOURS_LOOKBACK}, MAX_ITEMS_PER_CATEGORY={MAX_ITEMS_PER_CATEGORY}")
    print(f"[INFO] TRANSLATE_TO_JA={TRANSLATE_TO_JA}, TRANSLATE_ENGINE={TRANSLATE_ENGINE}")
    
    global TRANSLATION_CACHE
    TRANSLATION_CACHE = load_cache()
    print(f"[INFO] Loaded {len(TRANSLATION_CACHE)} cached translations")

    try:
        feeds_conf = parse_feeds()
        print(f"[INFO] Loaded {sum(len(v) for v in feeds_conf.values())} feeds from feeds.yml")
    except Exception as e:
        print(f"[ERROR] Failed to parse feeds.yml: {e}")
        feeds_conf = {}
    
    # Gather items with error handling
    try:
        business = gather_items(get_category(feeds_conf, "Business"), "Business")
        print(f"[INFO] Gathered {len(business)} Business items")
    except Exception as e:
        print(f"[ERROR] Failed to gather Business items: {e}")
        business = []
    
    try:
        tools = gather_items(get_category(feeds_conf, "Tools"), "Tools")
        print(f"[INFO] Gathered {len(tools)} Tools items")
    except Exception as e:
        print(f"[ERROR] Failed to gather Tools items: {e}")
        tools = []
    
    try:
        posts = gather_items(get_category(feeds_conf, "Posts"), "Posts")
        print(f"[INFO] Gathered {len(posts)} Posts items")
    except Exception as e:
        print(f"[ERROR] Failed to gather Posts items: {e}")
        posts = []
    
    # Remove global duplicates across all categories first
    print(f"[INFO] Removing duplicates across all categories...")
    all_items = business + tools + posts
    print(f"[INFO] Before deduplication: {len(all_items)} total items")
    
    seen_links = set()
    seen_titles = set()
    unique_business = []
    unique_tools = []
    unique_posts = []
    
    # Process each category and remove duplicates
    for item in business:
        link = item.get('link', '')
        title = item.get('title', '').lower().strip()
        if link not in seen_links and title not in seen_titles:
            unique_business.append(item)
            seen_links.add(link)
            seen_titles.add(title)
    
    for item in tools:
        link = item.get('link', '')
        title = item.get('title', '').lower().strip()
        if link not in seen_links and title not in seen_titles:
            unique_tools.append(item)
            seen_links.add(link)
            seen_titles.add(title)
    
    for item in posts:
        link = item.get('link', '')
        title = item.get('title', '').lower().strip()
        if link not in seen_links and title not in seen_titles:
            unique_posts.append(item)
            seen_links.add(link)
            seen_titles.add(title)
    
    # Update with deduplicated items
    business = unique_business
    tools = unique_tools  
    posts = unique_posts
    
    print(f"[INFO] After deduplication: Business={len(business)}, Tools={len(tools)}, Posts={len(posts)}")
    
    # Inject X posts
    if X_POSTS_CSV:
        try:
            x_posts = gather_x_posts(X_POSTS_CSV)
            if x_posts:
                print(f"[INFO] Adding {len(x_posts)} X posts")
                # Only add X posts that aren't already in posts
                for x_post in x_posts:
                    x_link = x_post.get('link', '')
                    x_title = x_post.get('title', '').lower().strip()
                    if x_link not in seen_links and x_title not in seen_titles:
                        posts.append(x_post)
                        seen_links.add(x_link)
                        seen_titles.add(x_title)
            else:
                print(f"[INFO] No X posts to add")
            posts = sorted(posts, key=lambda x: x.get('_dt', NOW), reverse=True)
        except Exception as e:
            print(f"[WARN] Failed to process X posts: {e}")


    try:
        translator = JaTranslator(engine=TRANSLATE_ENGINE)
    except Exception as e:
        print(f"[WARN] Failed to initialize translator: {e}")
        translator = None

    sections_html = []
    try:
        sections_html.append(SECTION_TMPL.format(
            sec_id="business",
            extra_class="",
            cards=build_cards(business[:MAX_ITEMS_PER_CATEGORY], translator)
        ))
    except Exception as e:
        print(f"[ERROR] Failed to build Business section: {e}")
        sections_html.append(SECTION_TMPL.format(sec_id="business", extra_class="", cards=EMPTY_TMPL))
    
    try:
        sections_html.append(SECTION_TMPL.format(
            sec_id="tools",
            extra_class="hidden",
            cards=build_cards(tools[:MAX_ITEMS_PER_CATEGORY], translator)
        ))
    except Exception as e:
        print(f"[ERROR] Failed to build Tools section: {e}")
        sections_html.append(SECTION_TMPL.format(sec_id="tools", extra_class="hidden", cards=EMPTY_TMPL))
    
    try:
        sections_html.append(SECTION_TMPL.format(
            sec_id="posts",
            extra_class="hidden",
            cards=build_cards(posts[:MAX_ITEMS_PER_CATEGORY], translator)
        ))
    except Exception as e:
        print(f"[ERROR] Failed to build Posts section: {e}")
        sections_html.append(SECTION_TMPL.format(sec_id="posts", extra_class="hidden", cards=EMPTY_TMPL))

    print(f"[INFO] Final counts after limiting to {MAX_ITEMS_PER_CATEGORY} per category:")
    print(f"  Business: {len(business[:MAX_ITEMS_PER_CATEGORY])} items")
    print(f"  Tools: {len(tools[:MAX_ITEMS_PER_CATEGORY])} items")
    print(f"  Posts: {len(posts[:MAX_ITEMS_PER_CATEGORY])} items")
    print(f"[INFO] Total items to display: {len(business[:MAX_ITEMS_PER_CATEGORY]) + len(tools[:MAX_ITEMS_PER_CATEGORY]) + len(posts[:MAX_ITEMS_PER_CATEGORY])}")
    html_out = PAGE_TMPL.format(
        updated_title=NOW.strftime("%Y-%m-%d %H:%M JST"),
        updated_full=NOW.strftime("%Y-%m-%d %H:%M JST"),
        lookback=HOURS_LOOKBACK,
        cnt_business=len(business[:MAX_ITEMS_PER_CATEGORY]),
        cnt_tools=len(tools[:MAX_ITEMS_PER_CATEGORY]),
        cnt_posts=len(posts[:MAX_ITEMS_PER_CATEGORY]),
        sections="".join(sections_html)
    )

    try:
        Path("news_detail.html").write_text(html_out, encoding="utf-8")
        print(f"[SUCCESS] Wrote news_detail.html ({len(html_out)} bytes)")
    except Exception as e:
        print(f"[ERROR] Failed to write news_detail.html: {e}")
        raise
    
    try:
        save_cache(TRANSLATION_CACHE)
        print(f"[SUCCESS] Saved {len(TRANSLATION_CACHE)} translations to cache")
    except Exception as e:
        print(f"[WARN] Failed to save cache: {e}")

if __name__ == "__main__":
    main()
